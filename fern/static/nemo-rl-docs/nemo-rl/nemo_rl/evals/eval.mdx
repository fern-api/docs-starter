---
layout: overview
slug: nemo-rl/nemo_rl/evals/eval
title: nemo_rl.evals.eval
---

## Module Contents

### Classes

| Name | Description |
|------|-------------|
| [`EvalConfig`](#nemo_rl-evals-eval-EvalConfig) | - |
| [`MasterConfig`](#nemo_rl-evals-eval-MasterConfig) | - |
| [`_PassThroughMathConfig`](#nemo_rl-evals-eval-_PassThroughMathConfig) | - |

### Functions

| Name | Description |
|------|-------------|
| [`_generate_texts`](#nemo_rl-evals-eval-_generate_texts) | Generate texts using either sync or async method. |
| [`_print_results`](#nemo_rl-evals-eval-_print_results) | Print evaluation results. |
| [`_run_env_eval_impl`](#nemo_rl-evals-eval-_run_env_eval_impl) | Unified implementation for both sync and async evaluation. |
| [`_save_evaluation_data_to_json`](#nemo_rl-evals-eval-_save_evaluation_data_to_json) | Save evaluation data to a JSON file. |
| [`eval_cons_k`](#nemo_rl-evals-eval-eval_cons_k) | Evaluate cons@k score using an unbiased estimator. |
| [`eval_pass_k`](#nemo_rl-evals-eval-eval_pass_k) | Evaluate pass@k score using an unbiased estimator. |
| [`run_env_eval`](#nemo_rl-evals-eval-run_env_eval) | Main entry point for running evaluation using environment. |
| [`setup`](#nemo_rl-evals-eval-setup) | Set up components for model evaluation. |

### API

<Anchor id="nemo_rl-evals-eval-EvalConfig">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
class nemo_rl.evals.eval.EvalConfig
```

</CodeBlock>
</Anchor>

<Indent>

**Bases:** `typing.TypedDict`

<ParamField path="k_value" type="int">

</ParamField>

<ParamField path="metric" type="str">

</ParamField>

<ParamField path="num_tests_per_prompt" type="int">

</ParamField>

<ParamField path="save_path" type="str | None">

</ParamField>

<ParamField path="seed" type="int">

</ParamField>

</Indent>

<Anchor id="nemo_rl-evals-eval-MasterConfig">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
class nemo_rl.evals.eval.MasterConfig
```

</CodeBlock>
</Anchor>

<Indent>

**Bases:** `typing.TypedDict`

<ParamField path="cluster" type="ClusterConfig">

</ParamField>

<ParamField path="data" type="EvalDataConfigType">

</ParamField>

<ParamField path="env" type="_PassThroughMathConfig">

</ParamField>

<ParamField path="eval" type="EvalConfig">

</ParamField>

<ParamField path="generation" type="GenerationConfig">

</ParamField>

<ParamField path="tokenizer" type="TokenizerConfig">

</ParamField>

</Indent>

<Anchor id="nemo_rl-evals-eval-_PassThroughMathConfig">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
class nemo_rl.evals.eval._PassThroughMathConfig
```

</CodeBlock>
</Anchor>

<Indent>

**Bases:** `typing.TypedDict`

<ParamField path="math" type="MathEnvConfig">

</ParamField>

</Indent>

<Anchor id="nemo_rl-evals-eval-_generate_texts">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.evals.eval._generate_texts(
    vllm_generation,
    inputs,
    use_async
)
```

</CodeBlock>
</Anchor>

<Indent>

<Badge>async</Badge>

Generate texts using either sync or async method.


</Indent>

<Anchor id="nemo_rl-evals-eval-_print_results">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.evals.eval._print_results(
    master_config,
    generation_config,
    score,
    dataset_size,
    metric,
    k_value,
    num_tests_per_prompt
)
```

</CodeBlock>
</Anchor>

<Indent>

Print evaluation results.


</Indent>

<Anchor id="nemo_rl-evals-eval-_run_env_eval_impl">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.evals.eval._run_env_eval_impl(
    vllm_generation,
    dataloader,
    env,
    master_config,
    use_async = False
)
```

</CodeBlock>
</Anchor>

<Indent>

<Badge>async</Badge>

Unified implementation for both sync and async evaluation.


</Indent>

<Anchor id="nemo_rl-evals-eval-_save_evaluation_data_to_json">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.evals.eval._save_evaluation_data_to_json(
    evaluation_data,
    master_config,
    save_path
)
```

</CodeBlock>
</Anchor>

<Indent>

Save evaluation data to a JSON file.

**Parameters:**

<ParamField path="evaluation_data">
List of evaluation samples
</ParamField>

<ParamField path="master_config">
Configuration dictionary
</ParamField>

<ParamField path="save_path">
Path to save evaluation results. Set to null to disable saving.
      Example: "results/eval_output" or "/path/to/evaluation_results"
</ParamField>


</Indent>

<Anchor id="nemo_rl-evals-eval-eval_cons_k">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.evals.eval.eval_cons_k(
    rewards: torch.Tensor,
    num_tests_per_prompt: int,
    k: int,
    extracted_answers: list[str | None]
) -> float
```

</CodeBlock>
</Anchor>

<Indent>

Evaluate cons@k score using an unbiased estimator.

**Parameters:**

<ParamField path="rewards" type="torch.Tensor">
Tensor of shape (batch_size * num_tests_per_prompt)
</ParamField>

<ParamField path="num_tests_per_prompt" type="int">
int
</ParamField>

<ParamField path="k" type="int">
int
</ParamField>

<ParamField path="extracted_answers" type="list[str | None]">
list[str| None]
</ParamField>

**Returns:** `float`

float


</Indent>

<Anchor id="nemo_rl-evals-eval-eval_pass_k">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.evals.eval.eval_pass_k(
    rewards: torch.Tensor,
    num_tests_per_prompt: int,
    k: int
) -> float
```

</CodeBlock>
</Anchor>

<Indent>

Evaluate pass@k score using an unbiased estimator.

Reference: https://github.com/huggingface/evaluate/blob/32546aafec25cdc2a5d7dd9f941fc5be56ba122f/metrics/code_eval/code_eval.py#L198-L213
Args:
    rewards: Tensor of shape (batch_size * num_tests_per_prompt)
    k: int (pass@k value)

**Returns:** `float`

float


</Indent>

<Anchor id="nemo_rl-evals-eval-run_env_eval">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.evals.eval.run_env_eval(
    vllm_generation,
    dataloader,
    env,
    master_config
)
```

</CodeBlock>
</Anchor>

<Indent>

Main entry point for running evaluation using environment.

Generates model responses and evaluates them by env.

**Parameters:**

<ParamField path="vllm_generation">
Model for generating responses.
</ParamField>

<ParamField path="dataloader">
Data loader with evaluation samples.
</ParamField>

<ParamField path="env">
Environment that scores responses.
</ParamField>

<ParamField path="master_config">
Configuration settings.
</ParamField>


</Indent>

<Anchor id="nemo_rl-evals-eval-setup">

<CodeBlock links={{"nemo_rl.evals.eval.MasterConfig":"#nemo_rl-evals-eval-MasterConfig","nemo_rl.data.datasets.AllTaskProcessedDataset":"/nemo-rl/nemo_rl/data/datasets/processed_dataset#nemo_rl-data-datasets-processed_dataset-AllTaskProcessedDataset"}} showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.evals.eval.setup(
    master_config: nemo_rl.evals.eval.MasterConfig,
    tokenizer: transformers.AutoTokenizer,
    dataset: nemo_rl.data.datasets.AllTaskProcessedDataset
) -> tuple[nemo_rl.models.generation.vllm.VllmGeneration, torch.utils.data.DataLoader, nemo_rl.evals.eval.MasterConfig]
```

</CodeBlock>
</Anchor>

<Indent>

Set up components for model evaluation.

Initializes the VLLM model and data loader.

**Parameters:**

<ParamField path="master_config" type="MasterConfig">
Configuration settings.
</ParamField>

<ParamField path="dataset" type="AllTaskProcessedDataset">
Dataset to evaluate on.
</ParamField>

**Returns:** `tuple[VllmGeneration, DataLoader, MasterConfig]`

VLLM model, data loader, and config.


</Indent>
