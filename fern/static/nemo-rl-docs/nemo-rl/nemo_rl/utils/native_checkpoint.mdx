---
layout: overview
slug: nemo-rl/nemo_rl/utils/native_checkpoint
title: nemo_rl.utils.native_checkpoint
---

Checkpoint management utilities for HF models.

## Module Contents

### Classes

| Name | Description |
|------|-------------|
| [`ModelState`](#nemo_rl-utils-native_checkpoint-ModelState) | Helper class for tracking model state in distributed checkpointing. |
| [`OptimizerState`](#nemo_rl-utils-native_checkpoint-OptimizerState) | Helper class for tracking optimizer state in distributed checkpointing. |

### Functions

| Name | Description |
|------|-------------|
| [`convert_dcp_to_hf`](#nemo_rl-utils-native_checkpoint-convert_dcp_to_hf) | Convert a Torch DCP checkpoint to a Hugging Face checkpoint. |
| [`load_checkpoint`](#nemo_rl-utils-native_checkpoint-load_checkpoint) | Load a model weights and optionally optimizer state. |
| [`save_checkpoint`](#nemo_rl-utils-native_checkpoint-save_checkpoint) | Save a checkpoint of the model and optionally optimizer state. |

### API

<Anchor id="nemo_rl-utils-native_checkpoint-ModelState">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
class nemo_rl.utils.native_checkpoint.ModelState(
    model: torch.nn.Module
)
```

</CodeBlock>
</Anchor>

<Indent>

**Bases:** `Stateful`

Helper class for tracking model state in distributed checkpointing.

This class is compliant with the Stateful protocol, allowing DCP to automatically
call state_dict/load_state_dict as needed in the dcp.save/load APIs.

**Parameters:**

<ParamField path="model" type="torch.nn.Module">
The PyTorch model to track.
</ParamField>


<Anchor id="nemo_rl-utils-native_checkpoint-ModelState-load_state_dict">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.utils.native_checkpoint.ModelState.load_state_dict(
    state_dict: dict[str, typing.Any]
) -> None
```

</CodeBlock>
</Anchor>

<Indent>

Load the state dictionary into the model.

**Parameters:**

<ParamField path="state_dict" type="dict">
State dictionary to load.
</ParamField>


</Indent>
<Anchor id="nemo_rl-utils-native_checkpoint-ModelState-state_dict">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.utils.native_checkpoint.ModelState.state_dict() -> dict[str, typing.Any]
```

</CodeBlock>
</Anchor>

<Indent>

Get the model's state dictionary.

**Returns:** `dict[str, Any]`

Dictionary containing the model's state dict with CPU offloading enabled.


</Indent>
</Indent>

<Anchor id="nemo_rl-utils-native_checkpoint-OptimizerState">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
class nemo_rl.utils.native_checkpoint.OptimizerState(
    model: torch.nn.Module,
    optimizer: torch.optim.Optimizer,
    scheduler: typing.Optional[typing.Any] = None
)
```

</CodeBlock>
</Anchor>

<Indent>

**Bases:** `Stateful`

Helper class for tracking optimizer state in distributed checkpointing.

This class is compliant with the Stateful protocol, allowing DCP to automatically
call state_dict/load_state_dict as needed in the dcp.save/load APIs.

**Parameters:**

<ParamField path="model" type="torch.nn.Module">
The PyTorch model associated with the optimizer.
</ParamField>

<ParamField path="optimizer" type="torch.optim.Optimizer">
The optimizer to track.
</ParamField>

<ParamField path="scheduler" type="Optional[Any]" default="None">
Optional learning rate scheduler.
</ParamField>


<Anchor id="nemo_rl-utils-native_checkpoint-OptimizerState-load_state_dict">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.utils.native_checkpoint.OptimizerState.load_state_dict(
    state_dict: dict[str, typing.Any]
) -> None
```

</CodeBlock>
</Anchor>

<Indent>

Load the state dictionaries into the optimizer and scheduler.

**Parameters:**

<ParamField path="state_dict" type="dict">
State dictionary containing optimizer and scheduler states to load.
</ParamField>


</Indent>
<Anchor id="nemo_rl-utils-native_checkpoint-OptimizerState-state_dict">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.utils.native_checkpoint.OptimizerState.state_dict() -> dict[str, typing.Any]
```

</CodeBlock>
</Anchor>

<Indent>

Get the optimizer and scheduler state dictionaries.

**Returns:** `dict[str, Any]`

Dictionary containing the optimizer and scheduler state dicts with CPU offloading enabled.


</Indent>
</Indent>

<Anchor id="nemo_rl-utils-native_checkpoint-convert_dcp_to_hf">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.utils.native_checkpoint.convert_dcp_to_hf(
    dcp_ckpt_path: str,
    hf_ckpt_path: str,
    model_name_or_path: str,
    tokenizer_name_or_path: str,
    overwrite: bool = False,
    hf_overrides: typing.Optional[dict[str, typing.Any]] = {}
) -> str
```

</CodeBlock>
</Anchor>

<Indent>

Convert a Torch DCP checkpoint to a Hugging Face checkpoint.

This is not an optimized utility. If checkpoint is too large, consider saving DCP during training
and using this utility to convert to HF format.

**Parameters:**

<ParamField path="dcp_ckpt_path" type="str">
Path to DCP checkpoint
</ParamField>

<ParamField path="hf_ckpt_path" type="str">
Path to save HF checkpoint
</ParamField>

<ParamField path="model_name_or_path" type="str">
Model name or path for config
</ParamField>

<ParamField path="tokenizer_name_or_path" type="str">
Tokenizer name or path.
                                   Defaults to model_name_or_path if None.
</ParamField>

<ParamField path="overwrite" type="bool" default="False">
Whether to overwrite existing checkpoint. Defaults to False.
</ParamField>

**Returns:** `str`

Path to the saved HF checkpoint

**Raises:**

- `FileExistsError`: If HF checkpoint already exists and overwrite is False


</Indent>

<Anchor id="nemo_rl-utils-native_checkpoint-load_checkpoint">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.utils.native_checkpoint.load_checkpoint(
    model: torch.nn.Module,
    weights_path: str,
    optimizer: typing.Optional[torch.optim.Optimizer] = None,
    scheduler: typing.Optional[typing.Any] = None,
    optimizer_path: typing.Optional[str] = None
) -> None
```

</CodeBlock>
</Anchor>

<Indent>

Load a model weights and optionally optimizer state.

**Parameters:**

<ParamField path="model" type="torch.nn.Module">
The PyTorch model whose weights to update
</ParamField>

<ParamField path="weights_path" type="str">
Path to load model weights from
</ParamField>

<ParamField path="optimizer" type="Optional[torch.optim.Optimizer]" default="None">
Optional optimizer to load state into
</ParamField>

<ParamField path="scheduler" type="Optional[Any]" default="None">
Optional scheduler to load state into
</ParamField>

<ParamField path="optimizer_path" type="Optional[str]" default="None">
Path to load optimizer state from (required if optimizer provided)
</ParamField>


</Indent>

<Anchor id="nemo_rl-utils-native_checkpoint-save_checkpoint">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.utils.native_checkpoint.save_checkpoint(
    model: torch.nn.Module,
    weights_path: str,
    optimizer: typing.Optional[torch.optim.Optimizer] = None,
    scheduler: typing.Optional[typing.Any] = None,
    optimizer_path: typing.Optional[str] = None,
    tokenizer: typing.Optional[typing.Any] = None,
    tokenizer_path: typing.Optional[str] = None
) -> None
```

</CodeBlock>
</Anchor>

<Indent>

Save a checkpoint of the model and optionally optimizer state.

**Parameters:**

<ParamField path="model" type="torch.nn.Module">
The PyTorch model to save
</ParamField>

<ParamField path="weights_path" type="str">
Path to save model weights
</ParamField>

<ParamField path="optimizer" type="Optional[torch.optim.Optimizer]" default="None">
Optional optimizer to save
</ParamField>

<ParamField path="scheduler" type="Optional[Any]" default="None">
Optional scheduler to save
</ParamField>

<ParamField path="optimizer_path" type="Optional[str]" default="None">
Path to save optimizer state (required if optimizer provided)
</ParamField>

<ParamField path="tokenizer" type="Optional[Any]" default="None">
Optional tokenizer to save
</ParamField>

<ParamField path="tokenizer_path" type="Optional[str]" default="None">
Path to save tokenizer state (required if tokenizer provided)
</ParamField>


</Indent>
