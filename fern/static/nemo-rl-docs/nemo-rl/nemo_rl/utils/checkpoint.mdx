---
layout: overview
slug: nemo-rl/nemo_rl/utils/checkpoint
title: nemo_rl.utils.checkpoint
---

Checkpoint management utilities for the rl algorithm loop.

It handles logic at the algorithm level. Each RL Actor is expected to have its
own checkpoint saving function (called by the algorithm loop).

## Module Contents

### Classes

| Name | Description |
|------|-------------|
| [`CheckpointManager`](#nemo_rl-utils-checkpoint-CheckpointManager) | Manages model checkpoints during training. |
| [`CheckpointingConfig`](#nemo_rl-utils-checkpoint-CheckpointingConfig) | Configuration for checkpoint management. |

### Functions

| Name | Description |
|------|-------------|
| [`_load_checkpoint_history`](#nemo_rl-utils-checkpoint-_load_checkpoint_history) | Load the history of checkpoints and their metrics. |

### Data

[`PathLike`](#nemo_rl-utils-checkpoint-PathLike)

### API

<Anchor id="nemo_rl-utils-checkpoint-CheckpointManager">

<CodeBlock links={{"nemo_rl.utils.checkpoint.CheckpointingConfig":"#nemo_rl-utils-checkpoint-CheckpointingConfig"}} showLineNumbers={false} wordWrap={true}>

```python
class nemo_rl.utils.checkpoint.CheckpointManager(
    config: nemo_rl.utils.checkpoint.CheckpointingConfig
)
```

</CodeBlock>
</Anchor>

<Indent>

Manages model checkpoints during training.

This class handles creating checkpoint dirs, saving training info, and
configurations. It also provides utilities for keeping just the top-k checkpoints.
The checkpointing structure looks like this:
<CodeBlock showLineNumbers={false}>

```python
checkpoint_dir/
    step_0/
        training_info.json
        config.yaml
        policy.py (up to the algorithm loop to save here)
        policy_optimizer.py (up to the algorithm loop to save here)
        ...
    step_1/
        ...
```

</CodeBlock>

Attributes: Derived from the CheckpointingConfig.


<ParamField path="checkpoint_dir" type="= Path(config['checkpoint_dir'])">
</ParamField>

<ParamField path="higher_is_better" type="= config['higher_is_better']">
</ParamField>

<ParamField path="is_peft" type="= config.get('is_peft', False)">
</ParamField>

<ParamField path="keep_top_k" type="= config['keep_top_k']">
</ParamField>

<ParamField path="metric_name" type="str | None = config['metric_name']">
</ParamField>

<ParamField path="model_cache_dir" type="= config.get('model_cache_dir', '')">
</ParamField>

<ParamField path="model_repo_id" type="= config.get('model_repo_id', '')">
</ParamField>

<ParamField path="model_save_format" type="= config.get('model_save_format', 'safetensors')">
</ParamField>

<ParamField path="save_consolidated" type="= config.get('save_consolidated', False)">
</ParamField>
<Anchor id="nemo_rl-utils-checkpoint-CheckpointManager-finalize_checkpoint">

<CodeBlock links={{"nemo_rl.utils.checkpoint.PathLike":"#nemo_rl-utils-checkpoint-PathLike"}} showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.utils.checkpoint.CheckpointManager.finalize_checkpoint(
    checkpoint_path: nemo_rl.utils.checkpoint.PathLike
) -> None
```

</CodeBlock>
</Anchor>

<Indent>

Complete a checkpoint by moving it from temporary to permanent location.

If a checkpoint at the target location already exists (i.e when resuming training),
we override the old one.
Also triggers cleanup of old checkpoints based on the keep_top_k setting.

**Parameters:**

<ParamField path="checkpoint_path" type="PathLike">
Path to the temporary checkpoint directory.
</ParamField>


</Indent>
<Anchor id="nemo_rl-utils-checkpoint-CheckpointManager-get_best_checkpoint_path">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.utils.checkpoint.CheckpointManager.get_best_checkpoint_path() -> typing.Optional[str]
```

</CodeBlock>
</Anchor>

<Indent>

Get the path to the best checkpoint based on the metric.

Returns the path to the checkpoint with the best metric value. If no checkpoints
exist, returns None. If some checkpoints are missing the metric, they are filtered
out with a warning. If no checkpoints have the metric, returns the latest checkpoint.

**Returns:** `Optional[str]`

Optional[str]: Path to the best checkpoint, or None if no checkpoints exist.


</Indent>
<Anchor id="nemo_rl-utils-checkpoint-CheckpointManager-get_latest_checkpoint_path">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.utils.checkpoint.CheckpointManager.get_latest_checkpoint_path() -> typing.Optional[str]
```

</CodeBlock>
</Anchor>

<Indent>

Get the path to the latest checkpoint.

Returns the path to the checkpoint with the highest step number.

**Returns:** `Optional[str]`

Optional[str]: Path to the latest checkpoint, or None if no checkpoints exist.


</Indent>
<Anchor id="nemo_rl-utils-checkpoint-CheckpointManager-init_tmp_checkpoint">

<CodeBlock links={{"nemo_rl.utils.checkpoint.PathLike":"#nemo_rl-utils-checkpoint-PathLike"}} showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.utils.checkpoint.CheckpointManager.init_tmp_checkpoint(
    step: int,
    training_info: typing.Mapping[str, typing.Any],
    run_config: typing.Optional[typing.Mapping[str, typing.Any]] = None
) -> nemo_rl.utils.checkpoint.PathLike
```

</CodeBlock>
</Anchor>

<Indent>

Initialize a temporary checkpoint directory.

Creates a temporary directory for a new checkpoint and saves training info
and configuration. The directory is named 'tmp_step_&#123;step&#125;' and will be renamed
to 'step_&#123;step&#125;' when the checkpoint is completed.
We do it this way to allow the algorithm loop to save any files it wants to save
in a safe, temporary directory.

**Parameters:**

<ParamField path="step" type="int">
The training step number.
</ParamField>

<ParamField path="training_info" type="dict[str, Any]">
Dictionary containing training metrics and info.
</ParamField>

<ParamField path="run_config" type="Optional[dict[str, Any]]" default="None">
Optional configuration for the training run.
</ParamField>

**Returns:** `PathLike`

Path to the temporary checkpoint directory.


</Indent>
<Anchor id="nemo_rl-utils-checkpoint-CheckpointManager-load_training_info">

<CodeBlock links={{"nemo_rl.utils.checkpoint.PathLike":"#nemo_rl-utils-checkpoint-PathLike"}} showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.utils.checkpoint.CheckpointManager.load_training_info(
    checkpoint_path: typing.Optional[nemo_rl.utils.checkpoint.PathLike] = None
) -> typing.Optional[dict[str, typing.Any]]
```

</CodeBlock>
</Anchor>

<Indent>

Load the training info from a checkpoint.

**Parameters:**

<ParamField path="checkpoint_path" type="Optional[PathLike]" default="None">
Path to the checkpoint. If None,
returns None.
</ParamField>

**Returns:** `Optional[dict[str, Any]]`

Optional[dict[str, Any]]: Dictionary containing the training info, or None if
checkpoint_path is None.


</Indent>
<Anchor id="nemo_rl-utils-checkpoint-CheckpointManager-remove_old_checkpoints">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.utils.checkpoint.CheckpointManager.remove_old_checkpoints(
    exclude_latest: bool = True
) -> None
```

</CodeBlock>
</Anchor>

<Indent>

Remove checkpoints that are not in the top-k or latest based on the (optional) metric.

If keep_top_k is set, this method removes all checkpoints except the top-k
best ones. The "best" checkpoints are determined by:
- If a metric is provided: the given metric value and the higher_is_better setting.
  When multiple checkpoints have the same metric value, more recent checkpoints
  (higher step numbers) are preferred.
- If no metric is provided: the step number. The most recent k checkpoints are kept.

**Parameters:**

<ParamField path="exclude_latest" type="bool" default="True">
Whether to exclude the latest checkpoint from deletion. (may result in K+1 checkpoints)
</ParamField>


</Indent>
</Indent>

<Anchor id="nemo_rl-utils-checkpoint-CheckpointingConfig">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
class nemo_rl.utils.checkpoint.CheckpointingConfig
```

</CodeBlock>
</Anchor>

<Indent>

**Bases:** `typing.TypedDict`

Configuration for checkpoint management.

Attributes:
enabled (bool): Whether checkpointing is enabled.
checkpoint_dir (PathLike): Directory where checkpoints will be saved.
metric_name (str | None): Name of the metric to use for determining best checkpoints.
    Must be of the form "val:&lt;metric_name&gt;" or "train:&lt;metric_name&gt;" to indicate whether
    the metric should be taken from the validation or training metrics.
higher_is_better (bool): Whether higher values of the metric indicate better performance.
keep_top_k (Optional[int]): Number of best checkpoints to keep. If None, all checkpoints are kept.
model_save_format (str | None): Format for saving model (v2 allowed values: "torch_save" or "safetensors", v1 allowed values: None).
save_consolidated (bool): Whether to save consolidated checkpoints (for HF compatibility).
model_cache_dir (str): Directory for model cache (for safetensors format).
model_repo_id (str): Repository ID for the model (for safetensors format).
is_peft (bool): Whether the model uses PEFT.

<ParamField path="checkpoint_dir" type="PathLike">

</ParamField>

<ParamField path="checkpoint_must_save_by" type="NotRequired[str | None]">

</ParamField>

<ParamField path="enabled" type="bool">

</ParamField>

<ParamField path="higher_is_better" type="bool">

</ParamField>

<ParamField path="is_peft" type="NotRequired[bool]">

</ParamField>

<ParamField path="keep_top_k" type="NotRequired[int]">

</ParamField>

<ParamField path="metric_name" type="str | None">

</ParamField>

<ParamField path="model_cache_dir" type="NotRequired[str]">

</ParamField>

<ParamField path="model_repo_id" type="NotRequired[str]">

</ParamField>

<ParamField path="model_save_format" type="NotRequired[str | None]">

</ParamField>

<ParamField path="peft_config" type="NotRequired[Any]">

</ParamField>

<ParamField path="save_consolidated" type="NotRequired[bool]">

</ParamField>

<ParamField path="save_period" type="int">

</ParamField>

</Indent>

<Anchor id="nemo_rl-utils-checkpoint-_load_checkpoint_history">

<CodeBlock links={{"nemo_rl.utils.checkpoint.PathLike":"#nemo_rl-utils-checkpoint-PathLike"}} showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.utils.checkpoint._load_checkpoint_history(
    checkpoint_dir: pathlib.Path
) -> list[tuple[int, nemo_rl.utils.checkpoint.PathLike, dict[str, typing.Any]]]
```

</CodeBlock>
</Anchor>

<Indent>

Load the history of checkpoints and their metrics.

**Parameters:**

<ParamField path="checkpoint_dir" type="Path">
Directory containing the checkpoints.
</ParamField>

**Returns:** `list[tuple[int, PathLike, dict[str, Any]]]`

list[tuple[int, PathLike, dict[str, Any]]]: List of tuples containing
(step_number, checkpoint_path, info) for each checkpoint.


</Indent>

<Anchor id="nemo_rl-utils-checkpoint-PathLike">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.utils.checkpoint.PathLike = Union[str, 'os.PathLike[Any]']
```

</CodeBlock>
</Anchor>

