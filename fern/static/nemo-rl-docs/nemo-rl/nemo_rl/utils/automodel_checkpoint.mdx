---
layout: overview
slug: nemo-rl/nemo_rl/utils/automodel_checkpoint
title: nemo_rl.utils.automodel_checkpoint
---

Automodel checkpoint utilities for DTensor policy workers.

This module provides a wrapper class around the nemo_automodel Checkpointer
for saving and loading model checkpoints in DTensor-based policy workers.

## Module Contents

### Classes

| Name | Description |
|------|-------------|
| [`AutomodelCheckpointManager`](#nemo_rl-utils-automodel_checkpoint-AutomodelCheckpointManager) | Manages checkpointing for DTensor-based models using nemo_automodel's Checkpointer. |

### Functions

| Name | Description |
|------|-------------|
| [`_infer_checkpoint_root`](#nemo_rl-utils-automodel_checkpoint-_infer_checkpoint_root) | Infer checkpoint root directory from weights path. |
| [`detect_checkpoint_format`](#nemo_rl-utils-automodel_checkpoint-detect_checkpoint_format) | Detect model save format and PEFT status from checkpoint directory. |

### API

<Anchor id="nemo_rl-utils-automodel_checkpoint-AutomodelCheckpointManager">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
class nemo_rl.utils.automodel_checkpoint.AutomodelCheckpointManager(
    dp_mesh: torch.distributed.device_mesh.DeviceMesh,
    tp_mesh: torch.distributed.device_mesh.DeviceMesh,
    model_state_dict_keys: typing.Optional[list[str]] = None,
    moe_mesh: typing.Optional[torch.distributed.device_mesh.DeviceMesh] = None
)
```

</CodeBlock>
</Anchor>

<Indent>

Manages checkpointing for DTensor-based models using nemo_automodel's Checkpointer.

This class provides a clean interface for saving and loading model checkpoints,
wrapping the nemo_automodel Checkpointer with configuration management.


<ParamField path="checkpoint_config" type="Optional[AutomodelCheckpointingConfig] = None">
</ParamField>

<ParamField path="checkpointer" type="Optional[Checkpointer] = None">
</ParamField>
<Anchor id="nemo_rl-utils-automodel_checkpoint-AutomodelCheckpointManager-_get_dp_rank">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.utils.automodel_checkpoint.AutomodelCheckpointManager._get_dp_rank() -> int
```

</CodeBlock>
</Anchor>

<Indent>

Get the data parallel rank.


</Indent>
<Anchor id="nemo_rl-utils-automodel_checkpoint-AutomodelCheckpointManager-_get_tp_rank">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.utils.automodel_checkpoint.AutomodelCheckpointManager._get_tp_rank() -> int
```

</CodeBlock>
</Anchor>

<Indent>

Get the tensor parallel rank.


</Indent>
<Anchor id="nemo_rl-utils-automodel_checkpoint-AutomodelCheckpointManager-_rebuild_checkpointer_addons">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.utils.automodel_checkpoint.AutomodelCheckpointManager._rebuild_checkpointer_addons() -> None
```

</CodeBlock>
</Anchor>

<Indent>

Rebuild the checkpointer's _addons list based on current config.

The Checkpointer's _addons list is populated during __init__ based on config.
When config changes (e.g., model_save_format or is_peft), we need to rebuild
the addons list to match the new config.


</Indent>
<Anchor id="nemo_rl-utils-automodel_checkpoint-AutomodelCheckpointManager-init_checkpointer">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.utils.automodel_checkpoint.AutomodelCheckpointManager.init_checkpointer(
    config_updates: typing.Optional[dict[str, typing.Any]] = None,
    checkpoint_root: typing.Optional[str] = None
) -> None
```

</CodeBlock>
</Anchor>

<Indent>

Initialize the Automodel Checkpointer if not already created.

This method creates a new Checkpointer instance with the provided configuration.
If a checkpointer already exists, this method does nothing.

**Parameters:**

<ParamField path="config_updates" type="Optional[dict[str, Any]]" default="None">
Dict of CheckpointingConfig fields to set during initialization.
</ParamField>

<ParamField path="checkpoint_root" type="Optional[str]" default="None">
Optional root directory for checkpoints.
</ParamField>


</Indent>
<Anchor id="nemo_rl-utils-automodel_checkpoint-AutomodelCheckpointManager-load_base_model">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.utils.automodel_checkpoint.AutomodelCheckpointManager.load_base_model(
    model: torch.nn.Module,
    model_name: str,
    hf_cache_dir: typing.Optional[str] = None,
    dequantize_base_checkpoint: bool = False,
    peft_init_method: typing.Optional[str] = None
) -> None
```

</CodeBlock>
</Anchor>

<Indent>

Load base model weights using the Automodel Checkpointer.

This method loads the initial HuggingFace model weights into the parallelized model.

**Parameters:**

<ParamField path="model" type="nn.Module">
The model to load weights into.
</ParamField>

<ParamField path="model_name" type="str">
Name or path of the model.
</ParamField>

<ParamField path="hf_cache_dir" type="Optional[str]" default="None">
Optional HuggingFace cache directory.
</ParamField>

<ParamField path="dequantize_base_checkpoint" type="bool" default="False">
Whether to dequantize the base checkpoint.
</ParamField>

**Raises:**

- `AssertionError`: If checkpointer has not been initialized.


</Indent>
<Anchor id="nemo_rl-utils-automodel_checkpoint-AutomodelCheckpointManager-load_checkpoint">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.utils.automodel_checkpoint.AutomodelCheckpointManager.load_checkpoint(
    model: torch.nn.Module,
    weights_path: str,
    optimizer: typing.Optional[torch.optim.Optimizer] = None,
    optimizer_path: typing.Optional[str] = None,
    scheduler: typing.Optional[torch.optim.lr_scheduler.LRScheduler] = None
) -> None
```

</CodeBlock>
</Anchor>

<Indent>

Load a checkpoint into the model using Automodel Checkpointer.

**Parameters:**

<ParamField path="model" type="nn.Module">
The model to load weights into.
</ParamField>

<ParamField path="weights_path" type="str">
Path to the checkpoint weights.
</ParamField>

<ParamField path="optimizer" type="Optional[torch.optim.Optimizer]" default="None">
Optional optimizer to load state into.
</ParamField>

<ParamField path="optimizer_path" type="Optional[str]" default="None">
Optional path to optimizer checkpoint.
</ParamField>

<ParamField path="scheduler" type="Optional[torch.optim.lr_scheduler.LRScheduler]" default="None">
Optional learning rate scheduler.
</ParamField>


</Indent>
<Anchor id="nemo_rl-utils-automodel_checkpoint-AutomodelCheckpointManager-save_checkpoint">

<CodeBlock links={{"nemo_rl.utils.checkpoint.CheckpointingConfig":"/nemo-rl/nemo_rl/utils/checkpoint#nemo_rl-utils-checkpoint-CheckpointingConfig"}} showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.utils.automodel_checkpoint.AutomodelCheckpointManager.save_checkpoint(
    model: torch.nn.Module,
    weights_path: str,
    optimizer: typing.Optional[torch.optim.Optimizer] = None,
    optimizer_path: typing.Optional[str] = None,
    scheduler: typing.Optional[torch.optim.lr_scheduler.LRScheduler] = None,
    tokenizer: typing.Optional[transformers.AutoTokenizer] = None,
    tokenizer_path: typing.Optional[str] = None,
    checkpointing_cfg: typing.Optional[nemo_rl.utils.checkpoint.CheckpointingConfig] = None,
    lora_enabled: bool = False,
    peft_config: typing.Optional[nemo_automodel.components._peft.lora.PeftConfig] = None
) -> None
```

</CodeBlock>
</Anchor>

<Indent>

Save a checkpoint of the model.

The optimizer states are saved only if `optimizer` and `optimizer_path` are provided.

**Parameters:**

<ParamField path="model" type="nn.Module">
The model to save.
</ParamField>

<ParamField path="weights_path" type="str">
Path to save model weights.
</ParamField>

<ParamField path="optimizer" type="Optional[torch.optim.Optimizer]" default="None">
Optional optimizer to save.
</ParamField>

<ParamField path="optimizer_path" type="Optional[str]" default="None">
Optional path to save optimizer state.
</ParamField>

<ParamField path="scheduler" type="Optional[torch.optim.lr_scheduler.LRScheduler]" default="None">
Optional learning rate scheduler.
</ParamField>

<ParamField path="tokenizer" type="Optional[AutoTokenizer]" default="None">
Optional tokenizer to save with the checkpoint.
</ParamField>

<ParamField path="tokenizer_path" type="Optional[str]" default="None">
Optional path to save tokenizer separately.
</ParamField>

<ParamField path="checkpointing_cfg" type="Optional[CheckpointingConfig]" default="None">
Checkpointing configuration.
</ParamField>

<ParamField path="lora_enabled" type="bool" default="False">
Whether LoRA is enabled.
</ParamField>

<ParamField path="peft_config" type="Optional[PeftConfig]" default="None">
Optional PEFT configuration.
</ParamField>


</Indent>
<Anchor id="nemo_rl-utils-automodel_checkpoint-AutomodelCheckpointManager-set_model_state_dict_keys">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.utils.automodel_checkpoint.AutomodelCheckpointManager.set_model_state_dict_keys(
    keys: list[str]
) -> None
```

</CodeBlock>
</Anchor>

<Indent>

Set the model state dict keys for checkpoint validation.

**Parameters:**

<ParamField path="keys" type="list[str]">
List of model state dict keys.
</ParamField>


</Indent>
<Anchor id="nemo_rl-utils-automodel_checkpoint-AutomodelCheckpointManager-update_checkpointer_config">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.utils.automodel_checkpoint.AutomodelCheckpointManager.update_checkpointer_config(
    config_updates: typing.Optional[dict[str, typing.Any]] = None,
    checkpoint_root: typing.Optional[str] = None
) -> None
```

</CodeBlock>
</Anchor>

<Indent>

Update the configuration of an existing Checkpointer.

This method updates the mutable config fields on the existing Checkpointer instance.
If no checkpointer exists, this method does nothing.

Note: Some config changes (like model_save_format) require rebuilding the
checkpointer's internal addons list. This method handles that automatically.

**Parameters:**

<ParamField path="config_updates" type="Optional[dict[str, Any]]" default="None">
Dict of CheckpointingConfig fields to update.
</ParamField>

<ParamField path="checkpoint_root" type="Optional[str]" default="None">
Optional root directory for checkpoints.
</ParamField>


</Indent>
</Indent>

<Anchor id="nemo_rl-utils-automodel_checkpoint-_infer_checkpoint_root">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.utils.automodel_checkpoint._infer_checkpoint_root(
    weights_path: str
) -> str
```

</CodeBlock>
</Anchor>

<Indent>

Infer checkpoint root directory from weights path.

When weights_path ends with "â€¦/weights/model", we need the parent of
the weights directory (the checkpoint root), not the weights directory itself.

**Parameters:**

<ParamField path="weights_path" type="str">
Path to model weights (e.g., "/path/to/policy/weights/model")
</ParamField>

**Returns:** `str`

Checkpoint root directory (e.g., "/path/to/policy")


</Indent>

<Anchor id="nemo_rl-utils-automodel_checkpoint-detect_checkpoint_format">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.utils.automodel_checkpoint.detect_checkpoint_format(
    weights_path: str
) -> tuple[str, bool]
```

</CodeBlock>
</Anchor>

<Indent>

Detect model save format and PEFT status from checkpoint directory.

**Parameters:**

<ParamField path="weights_path" type="str">
Path to the checkpoint directory (e.g., weights/model)
</ParamField>

**Returns:** `tuple[str, bool]`

(model_save_format, is_peft) where:
   model_save_format is "torch_save" for DCP or "safetensors" for safetensors
   is_peft is True if PEFT/adapter patterns are detected


</Indent>
