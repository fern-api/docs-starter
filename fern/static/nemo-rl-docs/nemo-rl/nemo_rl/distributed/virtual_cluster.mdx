---
layout: overview
slug: nemo-rl/nemo_rl/distributed/virtual_cluster
title: nemo_rl.distributed.virtual_cluster
---

## Module Contents

### Classes

| Name | Description |
|------|-------------|
| [`ClusterConfig`](#nemo_rl-distributed-virtual_cluster-ClusterConfig) | - |
| [`GetGPUIDActor`](#nemo_rl-distributed-virtual_cluster-GetGPUIDActor) | Util actor class to return GPU id of the current worker. |
| [`PY_EXECUTABLES`](#nemo_rl-distributed-virtual_cluster-PY_EXECUTABLES) | - |
| [`RayVirtualCluster`](#nemo_rl-distributed-virtual_cluster-RayVirtualCluster) | Creates a virtual distributed cluster using Ray placement groups. |
| [`ResourceInsufficientError`](#nemo_rl-distributed-virtual_cluster-ResourceInsufficientError) | Exception raised when the cluster does not have enough resources to satisfy the requested configuration. |

### Functions

| Name | Description |
|------|-------------|
| [`_get_free_port_local`](#nemo_rl-distributed-virtual_cluster-_get_free_port_local) | - |
| [`_get_node_ip_and_free_port`](#nemo_rl-distributed-virtual_cluster-_get_node_ip_and_free_port) | - |
| [`_get_node_ip_local`](#nemo_rl-distributed-virtual_cluster-_get_node_ip_local) | - |
| [`init_ray`](#nemo_rl-distributed-virtual_cluster-init_ray) | Initialise Ray. |

### Data

[`dir_path`](#nemo_rl-distributed-virtual_cluster-dir_path)

[`git_root`](#nemo_rl-distributed-virtual_cluster-git_root)

[`logger`](#nemo_rl-distributed-virtual_cluster-logger)

### API

<Anchor id="nemo_rl-distributed-virtual_cluster-ClusterConfig">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
class nemo_rl.distributed.virtual_cluster.ClusterConfig
```

</CodeBlock>
</Anchor>

<Indent>

**Bases:** `typing.TypedDict`

<ParamField path="gpus_per_node" type="int">

</ParamField>

<ParamField path="num_nodes" type="int">

</ParamField>

</Indent>

<Anchor id="nemo_rl-distributed-virtual_cluster-GetGPUIDActor">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
class nemo_rl.distributed.virtual_cluster.GetGPUIDActor()
```

</CodeBlock>
</Anchor>

<Indent>

Util actor class to return GPU id of the current worker.


<Anchor id="nemo_rl-distributed-virtual_cluster-GetGPUIDActor-get_gpu_id">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.distributed.virtual_cluster.GetGPUIDActor.get_gpu_id()
```

</CodeBlock>
</Anchor>

<Indent>


</Indent>
</Indent>

<Anchor id="nemo_rl-distributed-virtual_cluster-PY_EXECUTABLES">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
class nemo_rl.distributed.virtual_cluster.PY_EXECUTABLES()
```

</CodeBlock>
</Anchor>

<Indent>

<ParamField path="AUTOMODEL">
</ParamField>

<ParamField path="BASE" type="= f'uv run --locked --directory &#123;git_root&#125;'">
</ParamField>

<ParamField path="FSDP">
</ParamField>

<ParamField path="MCORE">
</ParamField>

<ParamField path="NEMO_GYM">
</ParamField>

<ParamField path="SGLANG">
</ParamField>

<ParamField path="SYSTEM" type="= sys.executable">
</ParamField>

<ParamField path="VLLM">
</ParamField>
</Indent>

<Anchor id="nemo_rl-distributed-virtual_cluster-RayVirtualCluster">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
class nemo_rl.distributed.virtual_cluster.RayVirtualCluster(
    bundle_ct_per_node_list: list[int],
    use_gpus: bool = True,
    max_colocated_worker_groups: int = 1,
    num_gpus_per_node: int = 8,
    name: str = '',
    placement_group_strategy: str = 'SPREAD'
)
```

</CodeBlock>
</Anchor>

<Indent>

Creates a virtual distributed cluster using Ray placement groups.

This class simplifies distributed training setup by:
- Creating placement groups that represent logical compute nodes
- Allocating GPU and CPU resources for distributed workers
- Managing communication between distributed processes

- Bundle: A resource allocation unit (ex: 4 GPUs on a single node)
- Worker: A process that performs computation (model training/inference)
- Node: A physical or virtual machine containing multiple bundles


<ParamField path="_node_placement_groups" type="Optional[list[PlacementGroup]] = None">
</ParamField>

<ParamField path="_sorted_bundle_indices" type="Optional[list[int]] = None">
</ParamField>

<ParamField path="_world_size" type="= sum(self._bundle_ct_per_node_list)">
</ParamField>
<Anchor id="nemo_rl-distributed-virtual_cluster-RayVirtualCluster-__del__">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.distributed.virtual_cluster.RayVirtualCluster.__del__() -> None
```

</CodeBlock>
</Anchor>

<Indent>

Shutsdown the virtual cluster when the object is deleted or is garbage collected.

This is an extra safety net in case the user forgets to call shutdown and the pointer to
the cluster is lost due to leaving a function scope. It's always recommended that the
user calls shutdown().


</Indent>
<Anchor id="nemo_rl-distributed-virtual_cluster-RayVirtualCluster-_create_placement_groups_internal">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.distributed.virtual_cluster.RayVirtualCluster._create_placement_groups_internal(
    strategy: str,
    use_unified_pg: bool = False
) -> list[ray.util.placement_group.PlacementGroup]
```

</CodeBlock>
</Anchor>

<Indent>

Internal method to create placement groups without retry logic.


</Indent>
<Anchor id="nemo_rl-distributed-virtual_cluster-RayVirtualCluster-_get_sorted_bundle_indices">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.distributed.virtual_cluster.RayVirtualCluster._get_sorted_bundle_indices() -> typing.Optional[list[int]]
```

</CodeBlock>
</Anchor>

<Indent>

Gets the sorted bundle indices for the placement groups.


</Indent>
<Anchor id="nemo_rl-distributed-virtual_cluster-RayVirtualCluster-_init_placement_groups">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.distributed.virtual_cluster.RayVirtualCluster._init_placement_groups(
    strategy: str | None = None,
    use_unified_pg: bool = False
) -> list[ray.util.placement_group.PlacementGroup]
```

</CodeBlock>
</Anchor>

<Indent>

Creates placement groups based on whether cross-node model parallelism is needed.

**Parameters:**

<ParamField path="strategy" type="str | None" default="None">
Ray placement group strategy (defaults to self.placement_group_strategy)
</ParamField>

<ParamField path="use_unified_pg" type="bool" default="False">
If True, create a single unified placement group.
          If False, create per-node placement groups.
</ParamField>

**Returns:** `list[PlacementGroup]`

List of placement groups


</Indent>
<Anchor id="nemo_rl-distributed-virtual_cluster-RayVirtualCluster-get_available_address_and_port">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.distributed.virtual_cluster.RayVirtualCluster.get_available_address_and_port(
    pg_idx: int,
    bundle_idx: int
) -> tuple[str, int]
```

</CodeBlock>
</Anchor>

<Indent>

Gets an available address and port for the given placement group index and bundle index.

**Returns:** `tuple[str, int]`

Tuple of (address, port)


</Indent>
<Anchor id="nemo_rl-distributed-virtual_cluster-RayVirtualCluster-get_master_address_and_port">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.distributed.virtual_cluster.RayVirtualCluster.get_master_address_and_port() -> tuple[str, int]
```

</CodeBlock>
</Anchor>

<Indent>

Gets the master address and port for the distributed training setup.

**Returns:** `tuple[str, int]`

Tuple of (address, port)


</Indent>
<Anchor id="nemo_rl-distributed-virtual_cluster-RayVirtualCluster-get_placement_groups">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.distributed.virtual_cluster.RayVirtualCluster.get_placement_groups() -> list[ray.util.placement_group.PlacementGroup]
```

</CodeBlock>
</Anchor>

<Indent>


</Indent>
<Anchor id="nemo_rl-distributed-virtual_cluster-RayVirtualCluster-node_count">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.distributed.virtual_cluster.RayVirtualCluster.node_count() -> int
```

</CodeBlock>
</Anchor>

<Indent>


</Indent>
<Anchor id="nemo_rl-distributed-virtual_cluster-RayVirtualCluster-shutdown">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.distributed.virtual_cluster.RayVirtualCluster.shutdown() -> bool
```

</CodeBlock>
</Anchor>

<Indent>

Cleans up and releases all resources associated with this virtual cluster.

This includes removing all placement groups and resetting the internal state.

This method is idempotent and can be safely called multiple times.


</Indent>
<Anchor id="nemo_rl-distributed-virtual_cluster-RayVirtualCluster-world_size">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.distributed.virtual_cluster.RayVirtualCluster.world_size() -> int
```

</CodeBlock>
</Anchor>

<Indent>


</Indent>
</Indent>

<Anchor id="nemo_rl-distributed-virtual_cluster-ResourceInsufficientError">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
class nemo_rl.distributed.virtual_cluster.ResourceInsufficientError()
```

</CodeBlock>
</Anchor>

<Indent>

<Badge>Exception</Badge>

**Bases:** `Exception`

Exception raised when the cluster does not have enough resources to satisfy the requested configuration.


</Indent>

<Anchor id="nemo_rl-distributed-virtual_cluster-_get_free_port_local">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.distributed.virtual_cluster._get_free_port_local() -> int
```

</CodeBlock>
</Anchor>

<Indent>


</Indent>

<Anchor id="nemo_rl-distributed-virtual_cluster-_get_node_ip_and_free_port">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.distributed.virtual_cluster._get_node_ip_and_free_port() -> tuple[str, int]
```

</CodeBlock>
</Anchor>

<Indent>


</Indent>

<Anchor id="nemo_rl-distributed-virtual_cluster-_get_node_ip_local">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.distributed.virtual_cluster._get_node_ip_local() -> str
```

</CodeBlock>
</Anchor>

<Indent>


</Indent>

<Anchor id="nemo_rl-distributed-virtual_cluster-init_ray">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.distributed.virtual_cluster.init_ray(
    log_dir: typing.Optional[str] = None
) -> None
```

</CodeBlock>
</Anchor>

<Indent>

Initialise Ray.

Try to attach to an existing local cluster.
If that cluster uses the same CUDA_VISIBLE_DEVICES or Slurm managed tag we will reuse it.
Otherwise, we will detach and start a fresh local cluster.

**Parameters:**

<ParamField path="log_dir" type="Optional[str]" default="None">
Optional directory to store Ray logs and temp files.
</ParamField>


</Indent>

<Anchor id="nemo_rl-distributed-virtual_cluster-dir_path">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.distributed.virtual_cluster.dir_path = os.path.dirname(os.path.abspath(__file__))
```

</CodeBlock>
</Anchor>


<Anchor id="nemo_rl-distributed-virtual_cluster-git_root">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.distributed.virtual_cluster.git_root = os.path.abspath(os.path.join(dir_path, '../..'))
```

</CodeBlock>
</Anchor>


<Anchor id="nemo_rl-distributed-virtual_cluster-logger">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.distributed.virtual_cluster.logger = logging.getLogger(__name__)
```

</CodeBlock>
</Anchor>

