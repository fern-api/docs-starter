---
layout: overview
slug: nemo-rl/nemo_rl/distributed/batched_data_dict
title: nemo_rl.distributed.batched_data_dict
---

## Module Contents

### Classes

| Name | Description |
|------|-------------|
| [`BatchedDataDict`](#nemo_rl-distributed-batched_data_dict-BatchedDataDict) | - |
| [`DynamicBatchingArgs`](#nemo_rl-distributed-batched_data_dict-DynamicBatchingArgs) | Configuration settings for dynamic batching. |
| [`SequencePackingArgs`](#nemo_rl-distributed-batched_data_dict-SequencePackingArgs) | Configuration settings for sequence packing. |
| [`SlicedDataDict`](#nemo_rl-distributed-batched_data_dict-SlicedDataDict) | A specialized subclass of BatchedDataDict that represents a slice or shard of a larger batch. |

### Data

[`DictT`](#nemo_rl-distributed-batched_data_dict-DictT)

### API

<Anchor id="nemo_rl-distributed-batched_data_dict-BatchedDataDict">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
class nemo_rl.distributed.batched_data_dict.BatchedDataDict(
    args = (),
    kwargs = {}
)
```

</CodeBlock>
</Anchor>

<Indent>

**Bases:** `UserDict`, `Generic[DictT]`

<ParamField path="ADDITIONAL_OPTIONAL_KEY_TENSORS" type="= ['token_type_ids']">
</ParamField>

<ParamField path="size" type="int">
Get the batch size of the batch.
</ParamField>
<Anchor id="nemo_rl-distributed-batched_data_dict-BatchedDataDict-all_gather">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.distributed.batched_data_dict.BatchedDataDict.all_gather(
    group: torch.distributed.ProcessGroup
) -> typing_extensions.Self
```

</CodeBlock>
</Anchor>

<Indent>

Gathers batches with possibly jagged leading dimensions across the DP ranks.

If using reshard, it will treat PP as DP ranks.
Works with data that is either tensors or string lists.


</Indent>
<Anchor id="nemo_rl-distributed-batched_data_dict-BatchedDataDict-chunk">

<CodeBlock links={{"nemo_rl.distributed.batched_data_dict.SlicedDataDict":"#nemo_rl-distributed-batched_data_dict-SlicedDataDict"}} showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.distributed.batched_data_dict.BatchedDataDict.chunk(
    rank: int,
    chunks: int
) -> nemo_rl.distributed.batched_data_dict.SlicedDataDict
```

</CodeBlock>
</Anchor>

<Indent>

Chunks a global batch into 'chunks' splits and returns the 'rank'th split batch=[A A A B B B D D E], rank=2, chunks=3 -&gt; [D D E].

Requires all leading dimensions of tensors and lengths of lists to be the same over the batch
and the chunks must divide batch size.


</Indent>
<Anchor id="nemo_rl-distributed-batched_data_dict-BatchedDataDict-from_batches">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.distributed.batched_data_dict.BatchedDataDict.from_batches(
    batches: typing.Sequence[typing.Mapping[typing.Any, typing.Any]],
    pad_value_dict: typing.Optional[dict[str, int | float]] = None
) -> typing_extensions.Self
```

</CodeBlock>
</Anchor>

<Indent>

<Badge>classmethod</Badge>

Given a list of batches, stack the tensors/lists within and put them in a single dictionary.

Pad sequences to the max length in the batch using either 0(default) or a non-default value for a given key provided in pad_value_dict.

**Parameters:**

<ParamField path="batches" type="list[Dict]">
A list of dictionaries, each containing a batch of data.
</ParamField>

<ParamField path="pad_value_dict" type="Optional[dict[str, int]]" default="None">
An optional dict mapping keys to non-default(0) padding values.
</ParamField>

**Returns:** `Self`

A new BatchedDataDict containing the stacked data.


</Indent>
<Anchor id="nemo_rl-distributed-batched_data_dict-BatchedDataDict-get_batch">

<CodeBlock links={{"nemo_rl.distributed.batched_data_dict.SlicedDataDict":"#nemo_rl-distributed-batched_data_dict-SlicedDataDict"}} showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.distributed.batched_data_dict.BatchedDataDict.get_batch(
    batch_idx,
    batch_size = None
) -> nemo_rl.distributed.batched_data_dict.SlicedDataDict
```

</CodeBlock>
</Anchor>

<Indent>

Slices a subbatch from the batch.

**Parameters:**

<ParamField path="batch_idx">
the batch index to slice
</ParamField>

<ParamField path="batch_size" default="None">
the size of the batch to be sliced
</ParamField>

**Returns:** `SlicedDataDict`

A new BatchedDataDict containing the sliced data


</Indent>
<Anchor id="nemo_rl-distributed-batched_data_dict-BatchedDataDict-get_dict">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.distributed.batched_data_dict.BatchedDataDict.get_dict() -> dict[typing.Any, typing.Any]
```

</CodeBlock>
</Anchor>

<Indent>

Get the underlying data dictionary.


</Indent>
<Anchor id="nemo_rl-distributed-batched_data_dict-BatchedDataDict-get_microbatch_iterator_dynamic_shapes_len">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.distributed.batched_data_dict.BatchedDataDict.get_microbatch_iterator_dynamic_shapes_len() -> int
```

</CodeBlock>
</Anchor>

<Indent>

Get the length of the microbatch iterator for dynamic shapes.


</Indent>
<Anchor id="nemo_rl-distributed-batched_data_dict-BatchedDataDict-get_microbatch_iterator_for_packable_sequences_len">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.distributed.batched_data_dict.BatchedDataDict.get_microbatch_iterator_for_packable_sequences_len() -> tuple[int, int]
```

</CodeBlock>
</Anchor>

<Indent>

Get the length of the microbatch iterator for sequence packing and the max packed seqlen.


</Indent>
<Anchor id="nemo_rl-distributed-batched_data_dict-BatchedDataDict-get_multimodal_dict">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.distributed.batched_data_dict.BatchedDataDict.get_multimodal_dict(
    as_tensors: bool = False,
    device: typing.Optional[torch.device] = None
) -> dict[str, typing.Any]
```

</CodeBlock>
</Anchor>

<Indent>

Return a regular dict of tensors or packed multimodal data items.


</Indent>
<Anchor id="nemo_rl-distributed-batched_data_dict-BatchedDataDict-make_microbatch_iterator">

<CodeBlock links={{"nemo_rl.distributed.batched_data_dict.SlicedDataDict":"#nemo_rl-distributed-batched_data_dict-SlicedDataDict"}} showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.distributed.batched_data_dict.BatchedDataDict.make_microbatch_iterator(
    microbatch_size: int
) -> typing.Iterator[nemo_rl.distributed.batched_data_dict.SlicedDataDict]
```

</CodeBlock>
</Anchor>

<Indent>

Make an iterator over the batch that yields microbatches of size microbatch_size.


</Indent>
<Anchor id="nemo_rl-distributed-batched_data_dict-BatchedDataDict-make_microbatch_iterator_for_packable_sequences">

<CodeBlock links={{"nemo_rl.distributed.batched_data_dict.SlicedDataDict":"#nemo_rl-distributed-batched_data_dict-SlicedDataDict"}} showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.distributed.batched_data_dict.BatchedDataDict.make_microbatch_iterator_for_packable_sequences() -> typing.Iterator[nemo_rl.distributed.batched_data_dict.SlicedDataDict]
```

</CodeBlock>
</Anchor>

<Indent>

Make an iterator over the batch that yields microbatches that can be packed into a given max_tokens_per_microbatch.


</Indent>
<Anchor id="nemo_rl-distributed-batched_data_dict-BatchedDataDict-make_microbatch_iterator_with_dynamic_shapes">

<CodeBlock links={{"nemo_rl.distributed.batched_data_dict.SlicedDataDict":"#nemo_rl-distributed-batched_data_dict-SlicedDataDict"}} showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.distributed.batched_data_dict.BatchedDataDict.make_microbatch_iterator_with_dynamic_shapes(
    sequence_dim: int = 1
) -> typing.Iterator[nemo_rl.distributed.batched_data_dict.SlicedDataDict]
```

</CodeBlock>
</Anchor>

<Indent>

Makes an iterator that yields microbatchs of dynamic batch and sequence sizes.

**Parameters:**

<ParamField path="sequence_dim" type="int" default="1">
the index of the sequence dim for all tensors in the data dict
</ParamField>

**Returns:** `Iterator[SlicedDataDict]`

Iterator["SlicedDataDict"]: An iterator that yield dynamic microbatches


</Indent>
<Anchor id="nemo_rl-distributed-batched_data_dict-BatchedDataDict-reorder_data">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.distributed.batched_data_dict.BatchedDataDict.reorder_data(
    reorded_indices: list[int]
)
```

</CodeBlock>
</Anchor>

<Indent>

Reorders the data along the batch dimension by the given indices.


</Indent>
<Anchor id="nemo_rl-distributed-batched_data_dict-BatchedDataDict-repeat_interleave">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.distributed.batched_data_dict.BatchedDataDict.repeat_interleave(
    num_repeats: int
) -> typing_extensions.Self
```

</CodeBlock>
</Anchor>

<Indent>

Repeats the batch num_repeats times.

For each element in the batch, repeat each value num_repeats times.
i.e:
&#123;"key": torch.tensor([1, 2, 3]), "other_key": [1, 2, 3]&#125; -&gt; &#123;"key": torch.tensor([1, 1, 2, 2, 3, 3]), "other_key": [1, 1, 2, 2, 3, 3]&#125;


</Indent>
<Anchor id="nemo_rl-distributed-batched_data_dict-BatchedDataDict-select_indices">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.distributed.batched_data_dict.BatchedDataDict.select_indices(
    indices: typing.Union[list[int], torch.Tensor]
) -> typing_extensions.Self
```

</CodeBlock>
</Anchor>

<Indent>

Selects specific rows from the batch based on indices.

**Parameters:**

<ParamField path="indices" type="Union[list[int], torch.Tensor]">
A list or tensor of integer indices to select.
</ParamField>

**Returns:** `Self`

A new BatchedDataDict containing only the selected rows.


</Indent>
<Anchor id="nemo_rl-distributed-batched_data_dict-BatchedDataDict-shard_by_batch_size">

<CodeBlock links={{"nemo_rl.distributed.batched_data_dict.DynamicBatchingArgs":"#nemo_rl-distributed-batched_data_dict-DynamicBatchingArgs","nemo_rl.distributed.batched_data_dict.SequencePackingArgs":"#nemo_rl-distributed-batched_data_dict-SequencePackingArgs","nemo_rl.distributed.batched_data_dict.SlicedDataDict":"#nemo_rl-distributed-batched_data_dict-SlicedDataDict"}} showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.distributed.batched_data_dict.BatchedDataDict.shard_by_batch_size(
    shards: int,
    batch_size: typing.Optional[int] = None,
    allow_uneven_shards: bool = False,
    dynamic_batching_args: typing.Optional[nemo_rl.distributed.batched_data_dict.DynamicBatchingArgs] = None,
    sequence_packing_args: typing.Optional[nemo_rl.distributed.batched_data_dict.SequencePackingArgs] = None
) -> list[nemo_rl.distributed.batched_data_dict.SlicedDataDict] | tuple[list[nemo_rl.distributed.batched_data_dict.SlicedDataDict], list[int]]
```

</CodeBlock>
</Anchor>

<Indent>

Shards a batch by first dividing it into chunks of size batch_size, then further dividing each chunk into shards equal parts. Finally aggregates the sub-shards by their position.

If batch_size is None, there will be no chunking beforehand (will default to the total batch size).

For example, with data [A A B B C C D D], batch_size=2, shards=2:
- Element 0: [A B C D] (first elements from each chunk)
- Element 1: [A B C D] (second elements from each chunk)

Examples:
<CodeBlock showLineNumbers={false}>

```python
>>> from nemo_rl.distributed.batched_data_dict import BatchedDataDict
>>> # Create a batch of two message logs with different lengths
>>> batch = BatchedDataDict({
...     'problem_id': [0, 0, 1, 1, 2, 2, 3, 3],
...     'arbitrary_data': [1, 2, 3, 4, 5, 6, 7, 8]
... })
>>> shards = batch.shard_by_batch_size(shards=2)
>>> shards
[{'problem_id': [0, 0, 1, 1], 'arbitrary_data': [1, 2, 3, 4]}, {'problem_id': [2, 2, 3, 3], 'arbitrary_data': [5, 6, 7, 8]}]
>>> # Now say that I'm training with a GBS of 4 and I want to take gradients steps on problems 0 and 1 before 2 and 3 (problems are repeated because GRPO)
>>> # In the current case, problems 0 and 2 will be trained on first since they're the first elements in each DP rank's batch.
>>> # So, we'll use the batch_size argument to split the batch into chunks of size 4 first.
>>> shards = batch.shard_by_batch_size(shards=2, batch_size=4)
>>> shards
[{'problem_id': [0, 0, 2, 2], 'arbitrary_data': [1, 2, 5, 6]}, {'problem_id': [1, 1, 3, 3], 'arbitrary_data': [3, 4, 7, 8]}]
>>> # Now, the ranks have 0 and 1 first so when they split their batches into microbatches (of size 2 since GBS=4 and DP=2), they'll train on 0 and 1 first.
>>> # Another way to use this function is with the 'allow_uneven_shards' flag, which allows the last shard to be smaller than the others when necessary.
>>> # This is necessary in multi-turn rollouts when some sequences terminate early, leaving unclean batch sizes.
>>> batch = BatchedDataDict({
...     'problem_id': [0, 1, 2, 3, 4],
...     'arbitrary_data': [10, 11, 12, 13, 14]
... })
>>> shards = batch.shard_by_batch_size(shards=2, allow_uneven_shards=True)
>>> shards
[{'problem_id': [0, 1, 2], 'arbitrary_data': [10, 11, 12]}, {'problem_id': [3, 4], 'arbitrary_data': [13, 14]}]
>>> # This is incompatible with the batch_size argument
```

</CodeBlock>

**Parameters:**

<ParamField path="shards" type="int">
The number of shards to divide each batch_size chunk into.
</ParamField>

<ParamField path="batch_size" type="int" default="None">
The size of each initial chunk.
</ParamField>

<ParamField path="allow_uneven_shards" type="bool" default="False">
Whether to allow shards to be unevenly sized.
                        If True, the last shard may be smaller than the others.
</ParamField>

<ParamField path="dynamic_batching_args" type="dict" default="None">
If passed, preprocess batch for dynamic batching. This
                            dict requires four keys:
                            1. max_tokens_per_microbatch (int): the maximum
                                number of tokens in a microbatch
                            2. sequence_length_round (int): round each all
                                sequence lengths to this multiple
                            3. input_key (str): the key in the batch
                                which holds input ids.
                            4. input_lengths_key (str): the key in the batch
                                which holds the sequence length per value.
                                The sequence dim index is assumed to be 1.
                          Cannot be passed with sequence_packing_args.
</ParamField>

<ParamField path="sequence_packing_args" type="dict" default="None">
If passed, preprocess batch for sequence packing. This
                            dict requires five keys:
                            1. max_tokens_per_microbatch (int): the maximum
                                number of tokens in a microbatch
                            2. input_key (str): the key in the batch
                                which holds input ids.
                            3. input_lengths_key (str): the key in the batch
                                which holds the sequence length per value.
                                The sequence dim index is assumed to be 1.
                            4. algorithm (str): the algorithm to use for sequence packing.
                            5. sequence_length_pad_multiple (int): the multiple to pad each sequence to.
                               With CP enabled, this should be set to a multiple of 2*CP and SP.
                          Cannot be passed with dynamic_batching_args.
</ParamField>

**Returns:** `list[SlicedDataDict] | tuple[list[SlicedDataDict], list[int]]`

list[BatchedDataDict]: A list of BatchedDataDicts, length equal to shards.


</Indent>
<Anchor id="nemo_rl-distributed-batched_data_dict-BatchedDataDict-slice">

<CodeBlock links={{"nemo_rl.distributed.batched_data_dict.SlicedDataDict":"#nemo_rl-distributed-batched_data_dict-SlicedDataDict"}} showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.distributed.batched_data_dict.BatchedDataDict.slice(
    start: int,
    end: int
) -> nemo_rl.distributed.batched_data_dict.SlicedDataDict
```

</CodeBlock>
</Anchor>

<Indent>

Slices the batch from start to end.

**Parameters:**

<ParamField path="start" type="int">
Starting index (inclusive)
</ParamField>

<ParamField path="end" type="int">
Ending index (exclusive)
</ParamField>

**Returns:** `SlicedDataDict`

A new BatchedDataDict containing the sliced data


</Indent>
<Anchor id="nemo_rl-distributed-batched_data_dict-BatchedDataDict-to">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.distributed.batched_data_dict.BatchedDataDict.to(
    device: str | torch.device
) -> typing_extensions.Self
```

</CodeBlock>
</Anchor>

<Indent>

Move tensors in batched dict to device.


</Indent>
<Anchor id="nemo_rl-distributed-batched_data_dict-BatchedDataDict-truncate_tensors">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.distributed.batched_data_dict.BatchedDataDict.truncate_tensors(
    dim: int,
    truncated_len: int
)
```

</CodeBlock>
</Anchor>

<Indent>

Truncates tensors in this dict of a given dim to a given length.


</Indent>
</Indent>

<Anchor id="nemo_rl-distributed-batched_data_dict-DynamicBatchingArgs">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
class nemo_rl.distributed.batched_data_dict.DynamicBatchingArgs
```

</CodeBlock>
</Anchor>

<Indent>

**Bases:** `typing.TypedDict`

Configuration settings for dynamic batching.

Pass this to 'shard_by_batch_size()' to preprocess batches for dynamic batching.

<ParamField path="input_key" type="str">

</ParamField>

<ParamField path="input_lengths_key" type="str">

</ParamField>

<ParamField path="max_tokens_per_microbatch" type="int">

</ParamField>

<ParamField path="sequence_length_round" type="int">

</ParamField>

</Indent>

<Anchor id="nemo_rl-distributed-batched_data_dict-SequencePackingArgs">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
class nemo_rl.distributed.batched_data_dict.SequencePackingArgs
```

</CodeBlock>
</Anchor>

<Indent>

**Bases:** `typing.TypedDict`

Configuration settings for sequence packing.

Pass this to 'shard_by_batch_size()' to preprocess batches for sequence packing.

<ParamField path="algorithm" type="str">

</ParamField>

<ParamField path="input_key" type="str">

</ParamField>

<ParamField path="input_lengths_key" type="str">

</ParamField>

<ParamField path="max_tokens_per_microbatch" type="int">

</ParamField>

<ParamField path="sequence_length_pad_multiple" type="int">

</ParamField>

</Indent>

<Anchor id="nemo_rl-distributed-batched_data_dict-SlicedDataDict">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
class nemo_rl.distributed.batched_data_dict.SlicedDataDict()
```

</CodeBlock>
</Anchor>

<Indent>

**Bases:** [BatchedDataDict](#nemo_rl-distributed-batched_data_dict-BatchedDataDict)

A specialized subclass of BatchedDataDict that represents a slice or shard of a larger batch.

This class provides a distinct type to differentiate between full batches and sliced/sharded batches, which can be helpful for
type checking.


</Indent>

<Anchor id="nemo_rl-distributed-batched_data_dict-DictT">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.distributed.batched_data_dict.DictT = TypeVar('DictT', bound=(Mapping[str, Any]))
```

</CodeBlock>
</Anchor>

