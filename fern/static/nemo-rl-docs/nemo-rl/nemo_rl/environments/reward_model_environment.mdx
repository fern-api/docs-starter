---
layout: overview
slug: nemo-rl/nemo_rl/environments/reward_model_environment
title: nemo_rl.environments.reward_model_environment
---

## Module Contents

### Classes

| Name | Description |
|------|-------------|
| [`RewardModelEnvironment`](#nemo_rl-environments-reward_model_environment-RewardModelEnvironment) | Environment that uses a reward model to score conversations. |
| [`RewardModelEnvironmentConfig`](#nemo_rl-environments-reward_model_environment-RewardModelEnvironmentConfig) | Configuration for RewardModelEnvironment. |

### API

<Anchor id="nemo_rl-environments-reward_model_environment-RewardModelEnvironment">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
class nemo_rl.environments.reward_model_environment.RewardModelEnvironment(
    config: typing.Dict[str, typing.Any]
)
```

</CodeBlock>
</Anchor>

<Indent>

**Bases:** [EnvironmentInterface](/nemo-rl/nemo_rl/environments/interfaces#nemo_rl-environments-interfaces-EnvironmentInterface)

Environment that uses a reward model to score conversations.

This environment implements a reward model-based scoring system for reinforcement
learning tasks. It takes conversation logs as input and returns rewards based on
the quality of the assistant's responses as judged by a pre-trained reward model.


<ParamField path="DEFAULT_PY_EXECUTABLE" type="= PY_EXECUTABLES.BASE">
</ParamField>

<ParamField path="reward_model_policy">
</ParamField>

<ParamField path="task_data_spec" type="= TaskDataSpec(task_name='reward_model_env')">
</ParamField>

<ParamField path="tokenizer" type="= get_tokenizer(self.config['tokenizer'])">
</ParamField>

<ParamField path="virtual_cluster">
</ParamField>
<Anchor id="nemo_rl-environments-reward_model_environment-RewardModelEnvironment-__del__">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.environments.reward_model_environment.RewardModelEnvironment.__del__()
```

</CodeBlock>
</Anchor>

<Indent>

Destructor that ensures proper cleanup when the object is garbage collected.

This is an extra safety net in case the user forgets to call shutdown() and
the pointer to the object is lost due to leaving a function scope. It's always
recommended that the user calls shutdown() explicitly for better resource
management.


</Indent>
<Anchor id="nemo_rl-environments-reward_model_environment-RewardModelEnvironment-global_post_process_and_metrics">

<CodeBlock links={{"nemo_rl.distributed.batched_data_dict.BatchedDataDict":"/nemo-rl/nemo_rl/distributed/batched_data_dict#nemo_rl-distributed-batched_data_dict-BatchedDataDict"}} showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.environments.reward_model_environment.RewardModelEnvironment.global_post_process_and_metrics(
    batch: nemo_rl.distributed.batched_data_dict.BatchedDataDict
) -> typing.Tuple[nemo_rl.distributed.batched_data_dict.BatchedDataDict, dict]
```

</CodeBlock>
</Anchor>

<Indent>

Post processing function after all rollouts are done for the batch and returns metrics.

This method computes aggregate statistics and metrics from the processed batch.
It provides insights into reward distribution and processing statistics.

**Parameters:**

<ParamField path="batch" type="BatchedDataDict">
The batch data dictionary containing processed conversations and rewards.
</ParamField>

**Returns:** `BatchedDataDict`

Tuple of (processed_batch, metrics_dict) where:


</Indent>
<Anchor id="nemo_rl-environments-reward_model_environment-RewardModelEnvironment-preprocess_data">

<CodeBlock links={{"nemo_rl.data.interfaces.LLMMessageLogType":"/nemo-rl/nemo_rl/data/interfaces#nemo_rl-data-interfaces-LLMMessageLogType","nemo_rl.distributed.batched_data_dict.BatchedDataDict":"/nemo-rl/nemo_rl/distributed/batched_data_dict#nemo_rl-distributed-batched_data_dict-BatchedDataDict","nemo_rl.models.generation.interfaces.GenerationDatumSpec":"/nemo-rl/nemo_rl/models/generation/interfaces#nemo_rl-models-generation-interfaces-GenerationDatumSpec"}} showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.environments.reward_model_environment.RewardModelEnvironment.preprocess_data(
    message_logs: typing.List[nemo_rl.data.interfaces.LLMMessageLogType]
) -> nemo_rl.distributed.batched_data_dict.BatchedDataDict[nemo_rl.models.generation.interfaces.GenerationDatumSpec]
```

</CodeBlock>
</Anchor>

<Indent>

Preprocess the message logs for the reward model.

This method tokenizes and formats conversation logs into the format expected
by the reward model. It handles:
- Tokenization of user and assistant messages
- Formatting with proper special tokens
- Batching and padding for efficient processing
- Sequence length validation and truncation

**Parameters:**

<ParamField path="message_logs" type="List[LLMMessageLogType]">
List of conversation message logs, where each log contains
         a list of messages with 'role' and 'content' fields.
</ParamField>

**Returns:** `BatchedDataDict[GenerationDatumSpec]`

BatchedDataDict containing tokenized and formatted data ready for


</Indent>
<Anchor id="nemo_rl-environments-reward_model_environment-RewardModelEnvironment-shutdown">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.environments.reward_model_environment.RewardModelEnvironment.shutdown()
```

</CodeBlock>
</Anchor>

<Indent>

Shutdown the reward model worker and virtual cluster.

This method properly cleans up resources by shutting down the reward model
policy and virtual cluster. It should be called when the environment is
no longer needed to prevent resource leaks.


</Indent>
<Anchor id="nemo_rl-environments-reward_model_environment-RewardModelEnvironment-step">

<CodeBlock links={{"nemo_rl.data.interfaces.LLMMessageLogType":"/nemo-rl/nemo_rl/data/interfaces#nemo_rl-data-interfaces-LLMMessageLogType","nemo_rl.environments.interfaces.EnvironmentReturn":"/nemo-rl/nemo_rl/environments/interfaces#nemo_rl-environments-interfaces-EnvironmentReturn"}} showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.environments.reward_model_environment.RewardModelEnvironment.step(
    message_logs: typing.List[nemo_rl.data.interfaces.LLMMessageLogType],
    env_infos: typing.List[typing.Dict[str, typing.Any]]
) -> nemo_rl.environments.interfaces.EnvironmentReturn
```

</CodeBlock>
</Anchor>

<Indent>

Calculate rewards for the given message logs using the reward model.

This method processes conversation logs through the reward model to compute
quality scores for each conversation. The rewards are based on the reward
model's assessment of how well the assistant's responses align with human
preferences.

**Parameters:**

<ParamField path="message_logs" type="List[LLMMessageLogType]">
List of conversation message logs to be scored.
         Each log should contain alternating user and assistant messages.
</ParamField>

<ParamField path="env_infos" type="List[Dict[str, Any]]">
List of environment info dictionaries (currently unused
      but required by the interface).
</ParamField>

**Returns:** `EnvironmentReturn`

EnvironmentReturn containing:


</Indent>
</Indent>

<Anchor id="nemo_rl-environments-reward_model_environment-RewardModelEnvironmentConfig">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
class nemo_rl.environments.reward_model_environment.RewardModelEnvironmentConfig
```

</CodeBlock>
</Anchor>

<Indent>

**Bases:** `typing.TypedDict`

Configuration for RewardModelEnvironment.

<ParamField path="batch_size" type="int">

</ParamField>

<ParamField path="checkpoint_path" type="str">

</ParamField>

<ParamField path="dtensor_cfg" type="Optional[Dict[str, Any]]">

</ParamField>

<ParamField path="dynamic_batching" type="DynamicBatchingConfig">

</ParamField>

<ParamField path="enabled" type="bool">

</ParamField>

<ParamField path="generation" type="Optional[VllmConfig]">

</ParamField>

<ParamField path="logprob_batch_size" type="int">

</ParamField>

<ParamField path="max_grad_norm" type="Optional[float]">

</ParamField>

<ParamField path="model_name" type="str">

</ParamField>

<ParamField path="precision" type="str">

</ParamField>

<ParamField path="resources" type="Dict[str, Any]">

</ParamField>

<ParamField path="sequence_packing" type="NotRequired[SequencePackingConfig]">

</ParamField>

</Indent>
