---
layout: overview
slug: nemo-rl/nemo_rl/algorithms/utils
title: nemo_rl.algorithms.utils
---

## Module Contents

### Functions

| Name | Description |
|------|-------------|
| [`calculate_baseline_and_std_per_prompt`](#nemo_rl-algorithms-utils-calculate_baseline_and_std_per_prompt) | Function to compute a baseline for each (prompt, response) pair in the batch. |
| [`calculate_kl`](#nemo_rl-algorithms-utils-calculate_kl) | Calculates a per-token estimate of the KL Divergence between two logprobs. |
| [`get_tokenizer`](#nemo_rl-algorithms-utils-get_tokenizer) | Get the tokenizer and set pad token to eos token if it is not already set. |
| [`log_generation_metrics_to_wandb`](#nemo_rl-algorithms-utils-log_generation_metrics_to_wandb) | Log generation metrics to wandb. |
| [`masked_mean`](#nemo_rl-algorithms-utils-masked_mean) | Computes the mean of a microbatch, using a global statistic as the normalization factor. |
| [`maybe_pad_last_batch`](#nemo_rl-algorithms-utils-maybe_pad_last_batch) | Pads the given batch so that its size is divisible by (mbs * dp_size). |
| [`print_performance_metrics`](#nemo_rl-algorithms-utils-print_performance_metrics) | Print performance metrics for GRPO. |
| [`set_seed`](#nemo_rl-algorithms-utils-set_seed) | Sets the seed for python, numpy, and pytorch. |
| [`surpress_user_warnings`](#nemo_rl-algorithms-utils-surpress_user_warnings) | - |

### API

<Anchor id="nemo_rl-algorithms-utils-calculate_baseline_and_std_per_prompt">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.algorithms.utils.calculate_baseline_and_std_per_prompt(
    prompts: torch.Tensor,
    rewards: torch.Tensor,
    valid_mask: torch.Tensor,
    leave_one_out_baseline: bool = True
) -> tuple[torch.Tensor, torch.Tensor]
```

</CodeBlock>
</Anchor>

<Indent>

Function to compute a baseline for each (prompt, response) pair in the batch.

The same baseline is calculated for each prompt. Samples set to 0 in 'valid_mask'
are not included in the baseline calculation.

prompts:    tensor (b, s)     Tensor of prompts the model used. May be on any device
rewards:    tensor (b,)       Float-valued rewards. May be on any device
valid_mask: tensor (b,)       Vector of 0/1, where 0 is to ignore and 1 is to keep
leave_one_out_baseline: bool  Compute an unbiased baseline by leaving out the sample that
                              the baseline is for (from RLOO https://arxiv.org/abs/2402.14740)

Returns:
tensor (b,), tensor (b,) of baselines and std on the same device as 'rewards'


</Indent>

<Anchor id="nemo_rl-algorithms-utils-calculate_kl">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.algorithms.utils.calculate_kl(
    logprobs: torch.Tensor,
    logprobs_reference: torch.Tensor,
    kl_type: str = 'k3',
    input_clamp_value: float | None = 20.0,
    output_clamp_value: float | None = 10.0
) -> torch.Tensor
```

</CodeBlock>
</Anchor>

<Indent>

Calculates a per-token estimate of the KL Divergence between two logprobs.

From Schulman 2020, http://joschu.net/blog/kl-approx.html.

**Parameters:**

<ParamField path="logprobs" type="torch.Tensor">
torch.Tensor (b, s)
</ParamField>

<ParamField path="logprobs_reference" type="torch.Tensor">
torch.Tensor (b, s)
</ParamField>

<ParamField path="kl_type" type="str" default="'k3'">
Type of KL approximation to use. Valid values: "k1", "k2", "k3".
</ParamField>

<ParamField path="input_clamp_value" type="float | None" default="20.0">
Optional clamping value for logr to prevent numerical instability.
               If None, no clamping is applied.
</ParamField>

<ParamField path="output_clamp_value" type="float | None" default="10.0">
Optional clamping value for kl to prevent numerical instability.
               If None, no clamping is applied.
</ParamField>

**Returns:** `torch.Tensor`

torch.Tensor: Per-token KL penalty values (b, s)


</Indent>

<Anchor id="nemo_rl-algorithms-utils-get_tokenizer">

<CodeBlock links={{"nemo_rl.models.policy.TokenizerConfig":"/nemo-rl/nemo_rl/models/policy#nemo_rl-models-policy-TokenizerConfig"}} showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.algorithms.utils.get_tokenizer(
    tokenizer_config: nemo_rl.models.policy.TokenizerConfig,
    get_processor: bool = False
) -> transformers.PreTrainedTokenizerBase
```

</CodeBlock>
</Anchor>

<Indent>

Get the tokenizer and set pad token to eos token if it is not already set.

This function initializes a tokenizer from the Hugging Face transformers library
and configures it with appropriate chat templates and padding tokens.

**Parameters:**

<ParamField path="tokenizer_config" type="TokenizerConfig">
A dictionary containing tokenizer configuration.
Required keys:
    - name: The name or path of the pretrained tokenizer
Optional keys:
    - chat_template: The chat template to use. Can be:
        - None: Uses a passthrough template that just returns message content
        - "default": Uses the tokenizer's default template
        - A custom jinja2 template string
        If not specified, the tokenizer's default template will be used.
</ParamField>

<ParamField path="get_processor" type="bool" default="False">
Whether to return a processor (via AutoProcessor) instead of a tokenizer.
</ParamField>

**Returns:** `PreTrainedTokenizerBase`

The configured tokenizer instance

**Examples:**

<CodeBlock showLineNumbers={false}>

```python
>>> from transformers import AutoTokenizer
>>> from nemo_rl.algorithms.utils import get_tokenizer
>>> # not specifying a chat template uses the tokenizer's default
>>> config = {"name": "meta-llama/Llama-3.2-1B-Instruct"}
>>> tokenizer = get_tokenizer(config)
No chat template provided, using tokenizer's default
>>> messages = [
...     {"role": "system", "content": "You are a helpful AI assistant."},
...     {"role": "user", "content": "Hello!"}
... ]
>>> formatted = tokenizer.apply_chat_template(messages, tokenize=False)
>>> assert formatted == AutoTokenizer.from_pretrained("meta-llama/Llama-3.2-1B-Instruct").apply_chat_template(messages, tokenize=False)

>>> # Using a passthrough template
>>> config = {
...     "name": "meta-llama/Llama-3.2-1B-Instruct",
...     "chat_template": None
... }
>>> tokenizer = get_tokenizer(config)
Using passthrough chat template
>>> formatted = tokenizer.apply_chat_template(messages, tokenize=False)
>>> assert formatted == "".join(msg["content"] for msg in messages)

>>> # Using a custom template
>>> config = {
...     "name": "meta-llama/Llama-3.2-1B-Instruct",
...     "chat_template": "{% for message in messages %}{{ ' START: ' + message['content'] + ' END.' }}{% endfor %}"
... }
>>> tokenizer = get_tokenizer(config)
Using custom chat template
>>> formatted = tokenizer.apply_chat_template(messages, tokenize=False)
>>> assert formatted == " START: You are a helpful AI assistant. END. START: Hello! END."

>>> # Requesting a processor (for multimodal models like Qwen-VL)
>>> config = {"name": "Qwen/Qwen2.5-VL-3B-Instruct"}
>>> processor = get_tokenizer(config, get_processor=True)
No chat template provided, using tokenizer's default
>>> messages = [
...     {"role": "system", "content": "You are a helpful AI assistant."},
...     {"role": "user", "content": "Hello!"}
... ]
>>> formatted = processor.tokenizer.apply_chat_template(messages, tokenize=False)
>>> assert formatted == AutoTokenizer.from_pretrained(
...     "Qwen/Qwen2.5-VL-3B-Instruct", trust_remote_code=True
... ).apply_chat_template(messages, tokenize=False)
>>> assert processor.pad_token_id == processor.tokenizer.pad_token_id
>>>
```

</CodeBlock>


</Indent>

<Anchor id="nemo_rl-algorithms-utils-log_generation_metrics_to_wandb">

<CodeBlock links={{"nemo_rl.utils.logger.Logger":"/nemo-rl/nemo_rl/utils/logger#nemo_rl-utils-logger-Logger"}} showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.algorithms.utils.log_generation_metrics_to_wandb(
    generation_logger_metrics: dict[str, dict[int, list[typing.Any]]],
    step: int,
    timeline_interval: float,
    logger: nemo_rl.utils.logger.Logger
) -> None
```

</CodeBlock>
</Anchor>

<Indent>

Log generation metrics to wandb.

**Parameters:**

<ParamField path="generation_logger_metrics" type="dict[str, dict[int, list[Any]]]">
Dictionary of generation logger metrics
</ParamField>

<ParamField path="step" type="int">
Global step value
</ParamField>

<ParamField path="timeline_interval" type="float">
Interval between timeline points (in seconds)
</ParamField>

<ParamField path="logger" type="Logger">
Logger instance
</ParamField>


</Indent>

<Anchor id="nemo_rl-algorithms-utils-masked_mean">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.algorithms.utils.masked_mean(
    values: torch.Tensor,
    mask: torch.Tensor,
    dim: typing.Optional[int] = None,
    global_normalization_factor: typing.Optional[torch.Tensor | float] = None
)
```

</CodeBlock>
</Anchor>

<Indent>

Computes the mean of a microbatch, using a global statistic as the normalization factor.


</Indent>

<Anchor id="nemo_rl-algorithms-utils-maybe_pad_last_batch">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.algorithms.utils.maybe_pad_last_batch(
    batch: dict,
    dp_size: int,
    mbs: int
) -> dict
```

</CodeBlock>
</Anchor>

<Indent>

Pads the given batch so that its size is divisible by (mbs * dp_size).

**Parameters:**

<ParamField path="batch" type="dict">
The batch to pad.
</ParamField>

<ParamField path="dp_size" type="int">
Data parallel size.
</ParamField>

<ParamField path="mbs" type="int">
Micro batch size.
</ParamField>

**Returns:** `dict`

The padded batch.


</Indent>

<Anchor id="nemo_rl-algorithms-utils-print_performance_metrics">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.algorithms.utils.print_performance_metrics(
    train_results: dict[str, float],
    metrics: dict[str, typing.Any],
    timing_metrics: dict[str, float],
    master_config: dict
) -> dict[str, float]
```

</CodeBlock>
</Anchor>

<Indent>

Print performance metrics for GRPO.


</Indent>

<Anchor id="nemo_rl-algorithms-utils-set_seed">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.algorithms.utils.set_seed(
    seed: int
) -> None
```

</CodeBlock>
</Anchor>

<Indent>

Sets the seed for python, numpy, and pytorch.


</Indent>

<Anchor id="nemo_rl-algorithms-utils-surpress_user_warnings">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.algorithms.utils.surpress_user_warnings(
    f
)
```

</CodeBlock>
</Anchor>

<Indent>


</Indent>
