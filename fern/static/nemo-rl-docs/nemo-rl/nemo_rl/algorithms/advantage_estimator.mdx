---
layout: overview
slug: nemo-rl/nemo_rl/algorithms/advantage_estimator
title: nemo_rl.algorithms.advantage_estimator
---

Advantage Estimators for RL algorithms.

This module provides different advantage estimation strategies:
- GRPOAdvantageEstimator: Standard GRPO advantage with leave-one-out baseline
- ReinforcePlusPlusAdvantageEstimator: Reinforce++ with optional baseline subtraction (minus_baseline) and KL penalty in reward
Reference papers:
- ProRLv2: https://developer.nvidia.com/blog/scaling-llm-reinforcement-learning-with-prolonged-training-using-prorl-v2/
- Reinforce++: https://arxiv.org/abs/2501.03262

## Module Contents

### Classes

| Name | Description |
|------|-------------|
| [`GRPOAdvantageEstimator`](#nemo_rl-algorithms-advantage_estimator-GRPOAdvantageEstimator) | GRPO-style advantage estimator with leave-one-out baseline. |
| [`ReinforcePlusPlusAdvantageEstimator`](#nemo_rl-algorithms-advantage_estimator-ReinforcePlusPlusAdvantageEstimator) | Reinforce++ advantage estimator with optional baseline subtraction and KL penalty in reward. |

### API

<Anchor id="nemo_rl-algorithms-advantage_estimator-GRPOAdvantageEstimator">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
class nemo_rl.algorithms.advantage_estimator.GRPOAdvantageEstimator(
    estimator_config: dict,
    loss_config: dict
)
```

</CodeBlock>
</Anchor>

<Indent>

GRPO-style advantage estimator with leave-one-out baseline.

Note: GRPO computes advantages over all responses for each prompt.


<ParamField path="normalize_rewards" type="= estimator_config['normalize_rewards']">
</ParamField>

<ParamField path="use_leave_one_out_baseline" type="= estimator_config['use_leave_one_out_baseline']">
</ParamField>
<Anchor id="nemo_rl-algorithms-advantage_estimator-GRPOAdvantageEstimator-compute_advantage">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.algorithms.advantage_estimator.GRPOAdvantageEstimator.compute_advantage(
    prompt_ids,
    rewards,
    mask,
    kwargs = {}
)
```

</CodeBlock>
</Anchor>

<Indent>

Compute GRPO advantages.

**Parameters:**

<ParamField path="prompt_ids">
Tensor of shape [batch_size] identifying which prompt each sample belongs to.
</ParamField>

<ParamField path="rewards">
Tensor of shape [batch_size] containing reward for each sample.
</ParamField>

<ParamField path="mask">
Response token mask of shape [batch_size, seq_len], 1 for valid response tokens, 0 for padding.
  Used only for expanding advantages to token-level shape.
</ParamField>

<ParamField path="**kwargs" default="&#123;&#125;">
Additional arguments (unused).
</ParamField>

**Returns:**

Advantages tensor of shape [batch_size, seq_len].


</Indent>
</Indent>

<Anchor id="nemo_rl-algorithms-advantage_estimator-ReinforcePlusPlusAdvantageEstimator">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
class nemo_rl.algorithms.advantage_estimator.ReinforcePlusPlusAdvantageEstimator(
    estimator_config: dict,
    loss_config: dict
)
```

</CodeBlock>
</Anchor>

<Indent>

Reinforce++ advantage estimator with optional baseline subtraction and KL penalty in reward.

**Parameters:**

<ParamField path="minus_baseline">
If True, subtract per-prompt mean baseline from rewards.
</ParamField>

<ParamField path="use_kl_in_reward">
If True, add KL penalty to reward instead of loss.
</ParamField>


<ParamField path="kl_coef" type="= loss_config['reference_policy_kl_penalty']">
</ParamField>

<ParamField path="kl_type" type="= loss_config['reference_policy_kl_type']">
</ParamField>

<ParamField path="minus_baseline" type="= estimator_config['minus_baseline']">
</ParamField>

<ParamField path="use_kl_in_reward" type="= loss_config['use_kl_in_reward']">
</ParamField>
<Anchor id="nemo_rl-algorithms-advantage_estimator-ReinforcePlusPlusAdvantageEstimator-compute_advantage">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.algorithms.advantage_estimator.ReinforcePlusPlusAdvantageEstimator.compute_advantage(
    prompt_ids,
    rewards,
    mask,
    logprobs_policy = None,
    logprobs_reference = None,
    kwargs = {}
)
```

</CodeBlock>
</Anchor>

<Indent>

Compute Reinforce++ advantages with optional KL penalty.

**Parameters:**

<ParamField path="prompt_ids">
Tensor of shape [batch_size] identifying which prompt each sample belongs to.
</ParamField>

<ParamField path="rewards">
Tensor of shape [batch_size] containing reward for each sample.
</ParamField>

<ParamField path="mask">
Response token mask of shape [batch_size, seq_len], 1 for valid response tokens, 0 for padding.
  Used for: (1) expanding advantages to token-level shape, (2) global normalization
  that only considers valid tokens.
</ParamField>

<ParamField path="logprobs_policy" default="None">
Policy log probabilities of shape [batch_size, seq_len], required if use_kl_in_reward.
</ParamField>

<ParamField path="logprobs_reference" default="None">
Reference policy log probabilities of shape [batch_size, seq_len], required if use_kl_in_reward.
</ParamField>

<ParamField path="**kwargs" default="&#123;&#125;">
Additional arguments (unused).
</ParamField>

**Returns:**

Advantages tensor of shape [batch_size, seq_len], globally normalized across valid tokens.


</Indent>
</Indent>
