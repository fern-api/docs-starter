---
layout: overview
slug: nemo-rl/nemo_rl/algorithms/loss_functions
title: nemo_rl.algorithms.loss_functions
---

## Module Contents

### Classes

| Name | Description |
|------|-------------|
| [`ClippedPGLossConfig`](#nemo_rl-algorithms-loss_functions-ClippedPGLossConfig) | - |
| [`ClippedPGLossDataDict`](#nemo_rl-algorithms-loss_functions-ClippedPGLossDataDict) | Required keys for the Clipped Policy Gradient loss function. |
| [`ClippedPGLossFn`](#nemo_rl-algorithms-loss_functions-ClippedPGLossFn) | Generalized Clipped Policy Gradient loss function w/ KL regularization. |
| [`DPOLossConfig`](#nemo_rl-algorithms-loss_functions-DPOLossConfig) | - |
| [`DPOLossDataDict`](#nemo_rl-algorithms-loss_functions-DPOLossDataDict) | Required keys for the DPO loss function. |
| [`DPOLossFn`](#nemo_rl-algorithms-loss_functions-DPOLossFn) | Direct Preference Optimization (DPO) loss function. |
| [`DistillationLossConfig`](#nemo_rl-algorithms-loss_functions-DistillationLossConfig) | - |
| [`DistillationLossDataDict`](#nemo_rl-algorithms-loss_functions-DistillationLossDataDict) | - |
| [`DistillationLossFn`](#nemo_rl-algorithms-loss_functions-DistillationLossFn) | Distillation loss function. |
| [`NLLLoss`](#nemo_rl-algorithms-loss_functions-NLLLoss) | Negative Log Likelihood Loss function. |
| [`PreferenceLoss`](#nemo_rl-algorithms-loss_functions-PreferenceLoss) | Preference Loss function. |
| [`PreferenceLossDataDict`](#nemo_rl-algorithms-loss_functions-PreferenceLossDataDict) | Required keys for the preference loss function. |
| [`SequencePackingLossWrapper`](#nemo_rl-algorithms-loss_functions-SequencePackingLossWrapper) | - |

### Data

[`Tensor`](#nemo_rl-algorithms-loss_functions-Tensor)

### API

<Anchor id="nemo_rl-algorithms-loss_functions-ClippedPGLossConfig">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
class nemo_rl.algorithms.loss_functions.ClippedPGLossConfig
```

</CodeBlock>
</Anchor>

<Indent>

**Bases:** `typing.TypedDict`

<ParamField path="disable_ppo_ratio" type="NotRequired[bool]">

</ParamField>

<ParamField path="force_on_policy_ratio" type="NotRequired[bool]">

</ParamField>

<ParamField path="kl_input_clamp_value" type="float | None">

</ParamField>

<ParamField path="kl_output_clamp_value" type="float | None">

</ParamField>

<ParamField path="ratio_clip_c" type="float | None">

</ParamField>

<ParamField path="ratio_clip_max" type="float">

</ParamField>

<ParamField path="ratio_clip_min" type="float">

</ParamField>

<ParamField path="reference_policy_kl_penalty" type="float">

</ParamField>

<ParamField path="reference_policy_kl_type" type="str">

</ParamField>

<ParamField path="sequence_level_importance_ratios" type="NotRequired[bool]">

</ParamField>

<ParamField path="token_level_loss" type="bool">

</ParamField>

<ParamField path="truncated_importance_sampling_ratio" type="float | None">

</ParamField>

<ParamField path="truncated_importance_sampling_ratio_min" type="NotRequired[float | None]">

</ParamField>

<ParamField path="truncated_importance_sampling_type" type="NotRequired[str | None]">

</ParamField>

<ParamField path="use_importance_sampling_correction" type="bool">

</ParamField>

<ParamField path="use_kl_in_reward" type="NotRequired[bool]">

</ParamField>

<ParamField path="use_on_policy_kl_approximation" type="bool">

</ParamField>

</Indent>

<Anchor id="nemo_rl-algorithms-loss_functions-ClippedPGLossDataDict">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
class nemo_rl.algorithms.loss_functions.ClippedPGLossDataDict
```

</CodeBlock>
</Anchor>

<Indent>

**Bases:** `typing.TypedDict`

Required keys for the Clipped Policy Gradient loss function.

<ParamField path="__extra__" type="Any">

</ParamField>

<ParamField path="advantages" type="Tensor">

</ParamField>

<ParamField path="generation_logprobs" type="Tensor">

</ParamField>

<ParamField path="input_ids" type="Tensor">

</ParamField>

<ParamField path="prev_logprobs" type="Tensor">

</ParamField>

<ParamField path="reference_policy_logprobs" type="Tensor">

</ParamField>

<ParamField path="sample_mask" type="Tensor">

</ParamField>

<ParamField path="token_mask" type="Tensor">

</ParamField>

</Indent>

<Anchor id="nemo_rl-algorithms-loss_functions-ClippedPGLossFn">

<CodeBlock links={{"nemo_rl.algorithms.loss_functions.ClippedPGLossConfig":"#nemo_rl-algorithms-loss_functions-ClippedPGLossConfig"}} showLineNumbers={false} wordWrap={true}>

```python
class nemo_rl.algorithms.loss_functions.ClippedPGLossFn(
    cfg: nemo_rl.algorithms.loss_functions.ClippedPGLossConfig
)
```

</CodeBlock>
</Anchor>

<Indent>

**Bases:** [LossFunction](/nemo-rl/nemo_rl/algorithms/interfaces#nemo_rl-algorithms-interfaces-LossFunction)

Generalized Clipped Policy Gradient loss function w/ KL regularization.

This implements:

- PPO (Clipped) - https://arxiv.org/abs/1707.06347
- GRPO - https://arxiv.org/abs/2402.03300
- REINFORCE/RLOO (set disable_ppo_ratio = True and ignores ratio_clip_min/ratio_clip_max) - https://arxiv.org/abs/2402.14740
- GSPO (set sequence_level_importance_ratios = True and token_level_loss = False) - https://arxiv.org/abs/2507.18071
- Truly on-policy (set force_on_policy_ratio = True to force ratio = 1.0, requires one update per rollout)

Formula:
L(θ) = E_t [ min(r_t(θ) * A_t, clip(r_t(θ), 1-ε, 1+ε) * A_t) ] - β * KL(π_θ || π_ref)

where:
- r_t(θ) = π_θ(a_t|s_t) / π_θ_old(a_t|s_t) is the probability ratio
- A_t is the advantage estimate
- ε is the clip parameter (ratio_clip_min/ratio_clip_max)
    - As proposed in the DAPO paper (https://arxiv.org/pdf/2503.14476),
      we allow setting a distinct minimum and maximum value for the clip parameter (set to the same value for PPO/GRPO/etc.)
        - ratio_clip_min: minimum value for the clip parameter
        - ratio_clip_max: maximum value for the clip parameter
- β is the KL penalty coefficient (reference_policy_kl_penalty)
- KL(π_θ || π_ref) is the KL divergence between the current policy and reference policy (Schulman Approx.)

For REINFORCE/RLOO (when disable_ppo_ratio=True), the formula simplifies to:
L(θ) = E_t [ π_θ(a_t|s_t) * A_t ] - β * KL(π_θ || π_ref)

Also supports "Dual-Clipping" from https://arxiv.org/pdf/1912.09729, which
imposes an additional upper bound on the probability ratio when advantages are negative.
This prevents excessive policy updates. $rA &lt;&lt; 0$ -&gt; $cA$(clipped)
The loss function is modified to the following when A_t &lt; 0:
L(θ) = E_t [ max(min(r_t(θ) * A_t, clip(r_t(θ), 1-ε, 1+ε) * A_t), c * A_t) ] - β * KL(π_θ || π_ref)

where:
- c is the dual-clip parameter (ratio_clip_c), which must be greater than 1 and is
  usually set as 3 empirically.

Due to potential numerical instability, we cast the logits to float32 before computing the loss.


<ParamField path="disable_ppo_ratio" type="= cfg.get('disable_ppo_ratio', False)">
</ParamField>

<ParamField path="force_on_policy_ratio" type="= cfg.get('force_on_policy_ratio', False)">
</ParamField>

<ParamField path="kl_input_clamp_value" type="= cfg['kl_input_clamp_value']">
</ParamField>

<ParamField path="kl_output_clamp_value" type="= cfg['kl_output_clamp_value']">
</ParamField>

<ParamField path="loss_type">
</ParamField>

<ParamField path="ratio_clip_c" type="= cfg['ratio_clip_c']">
</ParamField>

<ParamField path="ratio_clip_max" type="= cfg['ratio_clip_max']">
</ParamField>

<ParamField path="ratio_clip_min" type="= cfg['ratio_clip_min']">
</ParamField>

<ParamField path="reference_policy_kl_penalty" type="= cfg['reference_policy_kl_penalty']">
</ParamField>

<ParamField path="reference_policy_kl_type" type="= cfg['reference_policy_kl_type']">
</ParamField>

<ParamField path="sequence_level_importance_ratios" type="= cfg.get('sequence_level_importance_ratios', False)">
</ParamField>

<ParamField path="truncated_importance_sampling_ratio" type="= cfg['truncated_importance_sampling_ratio']">
</ParamField>

<ParamField path="truncated_importance_sampling_ratio_min" type="= cfg.get('truncated_importance_sampling_ratio_min')">
</ParamField>

<ParamField path="truncated_importance_sampling_type" type="= cfg.get('truncated_importance_sampling_type')">
</ParamField>

<ParamField path="use_importance_sampling_correction" type="= cfg['use_importance_sampling_correction']">
</ParamField>

<ParamField path="use_on_policy_kl_approximation" type="= cfg['use_on_policy_kl_approximation']">
</ParamField>
<Anchor id="nemo_rl-algorithms-loss_functions-ClippedPGLossFn-__call__">

<CodeBlock links={{"nemo_rl.algorithms.loss_functions.Tensor":"#nemo_rl-algorithms-loss_functions-Tensor","nemo_rl.distributed.batched_data_dict.BatchedDataDict":"/nemo-rl/nemo_rl/distributed/batched_data_dict#nemo_rl-distributed-batched_data_dict-BatchedDataDict","nemo_rl.algorithms.loss_functions.ClippedPGLossDataDict":"#nemo_rl-algorithms-loss_functions-ClippedPGLossDataDict"}} showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.algorithms.loss_functions.ClippedPGLossFn.__call__(
    next_token_logits: nemo_rl.algorithms.loss_functions.Tensor,
    data: nemo_rl.distributed.batched_data_dict.BatchedDataDict[nemo_rl.algorithms.loss_functions.ClippedPGLossDataDict],
    global_valid_seqs: torch.Tensor,
    global_valid_toks: torch.Tensor,
    vocab_parallel_rank: typing.Optional[int] = None,
    vocab_parallel_group: typing.Optional[torch.distributed.ProcessGroup] = None,
    context_parallel_group: typing.Optional[torch.distributed.ProcessGroup] = None
) -> tuple[torch.Tensor, dict]
```

</CodeBlock>
</Anchor>

<Indent>

Clipped Policy Gradient RL loss function.


</Indent>
</Indent>

<Anchor id="nemo_rl-algorithms-loss_functions-DPOLossConfig">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
class nemo_rl.algorithms.loss_functions.DPOLossConfig
```

</CodeBlock>
</Anchor>

<Indent>

**Bases:** `typing.TypedDict`

<ParamField path="preference_average_log_probs" type="bool">

</ParamField>

<ParamField path="preference_loss_weight" type="float">

</ParamField>

<ParamField path="reference_policy_kl_penalty" type="float">

</ParamField>

<ParamField path="sft_average_log_probs" type="bool">

</ParamField>

<ParamField path="sft_loss_weight" type="float">

</ParamField>

</Indent>

<Anchor id="nemo_rl-algorithms-loss_functions-DPOLossDataDict">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
class nemo_rl.algorithms.loss_functions.DPOLossDataDict
```

</CodeBlock>
</Anchor>

<Indent>

**Bases:** `typing.TypedDict`

Required keys for the DPO loss function.

<ParamField path="input_ids" type="Tensor">

</ParamField>

<ParamField path="reference_policy_logprobs" type="Tensor">

</ParamField>

<ParamField path="sample_mask" type="Tensor">

</ParamField>

<ParamField path="token_mask" type="Tensor">

</ParamField>

</Indent>

<Anchor id="nemo_rl-algorithms-loss_functions-DPOLossFn">

<CodeBlock links={{"nemo_rl.algorithms.loss_functions.DPOLossConfig":"#nemo_rl-algorithms-loss_functions-DPOLossConfig"}} showLineNumbers={false} wordWrap={true}>

```python
class nemo_rl.algorithms.loss_functions.DPOLossFn(
    cfg: nemo_rl.algorithms.loss_functions.DPOLossConfig
)
```

</CodeBlock>
</Anchor>

<Indent>

**Bases:** [PreferenceLoss](#nemo_rl-algorithms-loss_functions-PreferenceLoss)

Direct Preference Optimization (DPO) loss function.

This loss function implements the DPO algorithm as described in:
"Direct Preference Optimization: Your Language Model is Secretly a Reward Model"
(https://arxiv.org/abs/2305.18290)

The loss combines two main components:
1. Preference Loss: Optimizes the model to prefer chosen responses over rejected ones
2. SFT Loss (optional): Auxiliary supervised fine-tuning loss on chosen responses

The total loss is computed as:
L(θ) = w_p * L_pref(θ) + w_s * L_sft(θ)

where:
- w_p is the preference_loss_weight
- w_s is the sft_loss_weight
- L_pref(θ) is the preference loss term
- L_sft(θ) is the supervised fine-tuning loss term

The preference loss term is computed as:
L_pref(θ) = -E[log(σ(β * (r_chosen - r_rejected)))]

where:
- σ is the sigmoid function
- β is the reference_policy_kl_penalty
- r_chosen and r_rejected are the rewards for chosen and rejected responses
- The rewards are computed as the sum of log probability differences between
  the current policy and reference policy

If preference_average_log_probs is True, the rewards are averaged over tokens:
r = (1/n) * Σ_t (log π_θ(a_t|s_t) - log π_ref(a_t|s_t))

Otherwise, the rewards are summed over tokens.

The SFT loss term is a standard negative log likelihood loss on the chosen responses.
If sft_average_log_probs is True, the loss is averaged over tokens.

**Parameters:**

<ParamField path="cfg" type="DPOLossConfig">
Configuration dictionary containing:
- reference_policy_kl_penalty (float): Strength of the KL penalty term (β)
- preference_loss_weight (float): Weight for the preference loss term (w_p)
- sft_loss_weight (float): Weight for the SFT loss term (w_s)
- preference_average_log_probs (bool): Whether to average log probs across tokens in preference loss
- sft_average_log_probs (bool): Whether to average log probs across tokens in SFT loss
</ParamField>

**Returns:**

tuple[torch.Tensor, dict]: A tuple containing:
- The total loss value
- A dictionary with metrics including:
    - loss: Total loss value
    - sft_loss: SFT loss component
    - preference_loss: Preference loss component
    - accuracy: Fraction of examples where chosen response has higher reward


<ParamField path="loss_type" type="= LossType.SEQUENCE_LEVEL">
</ParamField>

<ParamField path="preference_average_log_probs" type="= cfg['preference_average_log_probs']">
</ParamField>

<ParamField path="preference_loss_weight" type="= cfg['preference_loss_weight']">
</ParamField>

<ParamField path="reference_policy_kl_penalty" type="= cfg['reference_policy_kl_penalty']">
</ParamField>

<ParamField path="sft_average_log_probs" type="= cfg['sft_average_log_probs']">
</ParamField>

<ParamField path="sft_loss" type="= NLLLoss()">
</ParamField>

<ParamField path="sft_loss_weight" type="= cfg['sft_loss_weight']">
</ParamField>
<Anchor id="nemo_rl-algorithms-loss_functions-DPOLossFn-__call__">

<CodeBlock links={{"nemo_rl.algorithms.loss_functions.Tensor":"#nemo_rl-algorithms-loss_functions-Tensor","nemo_rl.distributed.batched_data_dict.BatchedDataDict":"/nemo-rl/nemo_rl/distributed/batched_data_dict#nemo_rl-distributed-batched_data_dict-BatchedDataDict","nemo_rl.algorithms.loss_functions.DPOLossDataDict":"#nemo_rl-algorithms-loss_functions-DPOLossDataDict"}} showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.algorithms.loss_functions.DPOLossFn.__call__(
    next_token_logits: nemo_rl.algorithms.loss_functions.Tensor,
    data: nemo_rl.distributed.batched_data_dict.BatchedDataDict[nemo_rl.algorithms.loss_functions.DPOLossDataDict],
    global_valid_seqs: nemo_rl.algorithms.loss_functions.Tensor,
    global_valid_toks: nemo_rl.algorithms.loss_functions.Tensor | None,
    vocab_parallel_rank: typing.Optional[int] = None,
    vocab_parallel_group: typing.Optional[torch.distributed.ProcessGroup] = None,
    context_parallel_group: typing.Optional[torch.distributed.ProcessGroup] = None
) -> tuple[torch.Tensor, dict[str, typing.Any]]
```

</CodeBlock>
</Anchor>

<Indent>


</Indent>
<Anchor id="nemo_rl-algorithms-loss_functions-DPOLossFn-_dpo_loss">

<CodeBlock links={{"nemo_rl.algorithms.loss_functions.Tensor":"#nemo_rl-algorithms-loss_functions-Tensor","nemo_rl.distributed.batched_data_dict.BatchedDataDict":"/nemo-rl/nemo_rl/distributed/batched_data_dict#nemo_rl-distributed-batched_data_dict-BatchedDataDict","nemo_rl.algorithms.loss_functions.DPOLossDataDict":"#nemo_rl-algorithms-loss_functions-DPOLossDataDict"}} showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.algorithms.loss_functions.DPOLossFn._dpo_loss(
    next_token_logits: nemo_rl.algorithms.loss_functions.Tensor,
    data: nemo_rl.distributed.batched_data_dict.BatchedDataDict[nemo_rl.algorithms.loss_functions.DPOLossDataDict],
    global_valid_seqs: nemo_rl.algorithms.loss_functions.Tensor,
    vocab_parallel_rank: typing.Optional[int] = None,
    vocab_parallel_group: typing.Optional[torch.distributed.ProcessGroup] = None,
    context_parallel_group: typing.Optional[torch.distributed.ProcessGroup] = None
) -> tuple[nemo_rl.algorithms.loss_functions.Tensor, nemo_rl.algorithms.loss_functions.Tensor, nemo_rl.algorithms.loss_functions.Tensor, nemo_rl.algorithms.loss_functions.Tensor]
```

</CodeBlock>
</Anchor>

<Indent>


</Indent>
</Indent>

<Anchor id="nemo_rl-algorithms-loss_functions-DistillationLossConfig">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
class nemo_rl.algorithms.loss_functions.DistillationLossConfig
```

</CodeBlock>
</Anchor>

<Indent>

**Bases:** `typing.TypedDict`

<ParamField path="kl_type" type="str">

</ParamField>

<ParamField path="mixed_kl_weight" type="float">

</ParamField>

<ParamField path="zero_outside_topk" type="bool">

</ParamField>

</Indent>

<Anchor id="nemo_rl-algorithms-loss_functions-DistillationLossDataDict">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
class nemo_rl.algorithms.loss_functions.DistillationLossDataDict
```

</CodeBlock>
</Anchor>

<Indent>

**Bases:** `typing.TypedDict`

<ParamField path="input_ids" type="Tensor">

</ParamField>

<ParamField path="input_lengths" type="Tensor">

</ParamField>

<ParamField path="sample_mask" type="Tensor">

</ParamField>

<ParamField path="teacher_topk_indices" type="Tensor">

</ParamField>

<ParamField path="teacher_topk_logits" type="Tensor">

</ParamField>

<ParamField path="token_mask" type="Tensor">

</ParamField>

</Indent>

<Anchor id="nemo_rl-algorithms-loss_functions-DistillationLossFn">

<CodeBlock links={{"nemo_rl.algorithms.loss_functions.DistillationLossConfig":"#nemo_rl-algorithms-loss_functions-DistillationLossConfig"}} showLineNumbers={false} wordWrap={true}>

```python
class nemo_rl.algorithms.loss_functions.DistillationLossFn(
    cfg: nemo_rl.algorithms.loss_functions.DistillationLossConfig
)
```

</CodeBlock>
</Anchor>

<Indent>

**Bases:** [LossFunction](/nemo-rl/nemo_rl/algorithms/interfaces#nemo_rl-algorithms-interfaces-LossFunction)

Distillation loss function.


<ParamField path="kl_type" type="= cfg['kl_type']">
</ParamField>

<ParamField path="log_infinitesimal" type="= -100">
</ParamField>

<ParamField path="loss_type" type="= LossType.TOKEN_LEVEL">
</ParamField>

<ParamField path="mixed_kl_weight" type="= cfg['mixed_kl_weight']">
</ParamField>

<ParamField path="zero_outside_topk" type="= cfg['zero_outside_topk']">
</ParamField>
<Anchor id="nemo_rl-algorithms-loss_functions-DistillationLossFn-__call__">

<CodeBlock links={{"nemo_rl.algorithms.loss_functions.DistillationLossDataDict":"#nemo_rl-algorithms-loss_functions-DistillationLossDataDict"}} showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.algorithms.loss_functions.DistillationLossFn.__call__(
    next_token_logits: torch.Tensor,
    data: nemo_rl.algorithms.loss_functions.DistillationLossDataDict,
    global_valid_seqs: torch.Tensor,
    global_valid_toks: torch.Tensor,
    vocab_parallel_rank: typing.Optional[int] = None,
    vocab_parallel_group: typing.Optional[torch.distributed.ProcessGroup] = None,
    context_parallel_group: typing.Optional[torch.distributed.ProcessGroup] = None
) -> tuple[torch.Tensor, dict[str, typing.Any]]
```

</CodeBlock>
</Anchor>

<Indent>

Compute distillation loss between teacher and student logits.


</Indent>
</Indent>

<Anchor id="nemo_rl-algorithms-loss_functions-NLLLoss">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
class nemo_rl.algorithms.loss_functions.NLLLoss()
```

</CodeBlock>
</Anchor>

<Indent>

**Bases:** [LossFunction](/nemo-rl/nemo_rl/algorithms/interfaces#nemo_rl-algorithms-interfaces-LossFunction)

Negative Log Likelihood Loss function.


<ParamField path="loss_type" type="= LossType.TOKEN_LEVEL">
</ParamField>
<Anchor id="nemo_rl-algorithms-loss_functions-NLLLoss-__call__">

<CodeBlock links={{"nemo_rl.algorithms.loss_functions.Tensor":"#nemo_rl-algorithms-loss_functions-Tensor","nemo_rl.distributed.batched_data_dict.BatchedDataDict":"/nemo-rl/nemo_rl/distributed/batched_data_dict#nemo_rl-distributed-batched_data_dict-BatchedDataDict"}} showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.algorithms.loss_functions.NLLLoss.__call__(
    next_token_logits: nemo_rl.algorithms.loss_functions.Tensor,
    data: nemo_rl.distributed.batched_data_dict.BatchedDataDict[typing.Any],
    global_valid_seqs: nemo_rl.algorithms.loss_functions.Tensor | None,
    global_valid_toks: nemo_rl.algorithms.loss_functions.Tensor,
    vocab_parallel_rank: typing.Optional[int] = None,
    vocab_parallel_group: typing.Optional[torch.distributed.ProcessGroup] = None,
    context_parallel_group: typing.Optional[torch.distributed.ProcessGroup] = None,
    dpo_loss: bool = False,
    dpo_average_log_probs: bool = False
) -> tuple[torch.Tensor, dict[str, typing.Any]]
```

</CodeBlock>
</Anchor>

<Indent>


</Indent>
</Indent>

<Anchor id="nemo_rl-algorithms-loss_functions-PreferenceLoss">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
class nemo_rl.algorithms.loss_functions.PreferenceLoss()
```

</CodeBlock>
</Anchor>

<Indent>

**Bases:** [LossFunction](/nemo-rl/nemo_rl/algorithms/interfaces#nemo_rl-algorithms-interfaces-LossFunction)

Preference Loss function.

Optimizes the model to prefer chosen responses over rejected ones

The preference loss is computed as:
L_pref(θ) = -E[log(σ(β * (r_chosen - r_rejected)))]

where:
- σ is the sigmoid function
- β is a scaling factor (ex: `reference_policy_kl_penalty` in DPO)
- r_chosen and r_rejected are the rewards for chosen and rejected responses

**Returns:**

tuple[torch.Tensor, dict]: A tuple containing:
- The preference loss value
- A dictionary with metrics including:
    - loss: Preference loss
    - accuracy: Fraction of examples where chosen response has higher reward


<ParamField path="loss_type" type="= LossType.SEQUENCE_LEVEL">
</ParamField>
<Anchor id="nemo_rl-algorithms-loss_functions-PreferenceLoss-__call__">

<CodeBlock links={{"nemo_rl.algorithms.loss_functions.Tensor":"#nemo_rl-algorithms-loss_functions-Tensor","nemo_rl.distributed.batched_data_dict.BatchedDataDict":"/nemo-rl/nemo_rl/distributed/batched_data_dict#nemo_rl-distributed-batched_data_dict-BatchedDataDict","nemo_rl.algorithms.loss_functions.PreferenceLossDataDict":"#nemo_rl-algorithms-loss_functions-PreferenceLossDataDict"}} showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.algorithms.loss_functions.PreferenceLoss.__call__(
    rewards: nemo_rl.algorithms.loss_functions.Tensor,
    data: nemo_rl.distributed.batched_data_dict.BatchedDataDict[nemo_rl.algorithms.loss_functions.PreferenceLossDataDict],
    global_valid_seqs: nemo_rl.algorithms.loss_functions.Tensor,
    global_valid_toks: nemo_rl.algorithms.loss_functions.Tensor | None
) -> tuple[torch.Tensor, dict[str, typing.Any]]
```

</CodeBlock>
</Anchor>

<Indent>


</Indent>
<Anchor id="nemo_rl-algorithms-loss_functions-PreferenceLoss-_preference_loss">

<CodeBlock links={{"nemo_rl.algorithms.loss_functions.Tensor":"#nemo_rl-algorithms-loss_functions-Tensor"}} showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.algorithms.loss_functions.PreferenceLoss._preference_loss(
    rewards: nemo_rl.algorithms.loss_functions.Tensor,
    sample_mask: nemo_rl.algorithms.loss_functions.Tensor,
    global_valid_seqs: nemo_rl.algorithms.loss_functions.Tensor,
    beta: float = 1.0
) -> tuple[nemo_rl.algorithms.loss_functions.Tensor, nemo_rl.algorithms.loss_functions.Tensor, nemo_rl.algorithms.loss_functions.Tensor, nemo_rl.algorithms.loss_functions.Tensor]
```

</CodeBlock>
</Anchor>

<Indent>


</Indent>
<Anchor id="nemo_rl-algorithms-loss_functions-PreferenceLoss-split_output_tensor">

<CodeBlock links={{"nemo_rl.algorithms.loss_functions.Tensor":"#nemo_rl-algorithms-loss_functions-Tensor"}} showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.algorithms.loss_functions.PreferenceLoss.split_output_tensor(
    tensor: nemo_rl.algorithms.loss_functions.Tensor
) -> tuple[nemo_rl.algorithms.loss_functions.Tensor, nemo_rl.algorithms.loss_functions.Tensor]
```

</CodeBlock>
</Anchor>

<Indent>


</Indent>
</Indent>

<Anchor id="nemo_rl-algorithms-loss_functions-PreferenceLossDataDict">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
class nemo_rl.algorithms.loss_functions.PreferenceLossDataDict
```

</CodeBlock>
</Anchor>

<Indent>

**Bases:** `typing.TypedDict`

Required keys for the preference loss function.

<ParamField path="input_ids" type="Tensor">

</ParamField>

<ParamField path="sample_mask" type="Tensor">

</ParamField>

<ParamField path="token_mask" type="Tensor">

</ParamField>

</Indent>

<Anchor id="nemo_rl-algorithms-loss_functions-SequencePackingLossWrapper">

<CodeBlock links={{"nemo_rl.algorithms.interfaces.LossFunction":"/nemo-rl/nemo_rl/algorithms/interfaces#nemo_rl-algorithms-interfaces-LossFunction","nemo_rl.algorithms.loss_functions.Tensor":"#nemo_rl-algorithms-loss_functions-Tensor"}} showLineNumbers={false} wordWrap={true}>

```python
class nemo_rl.algorithms.loss_functions.SequencePackingLossWrapper(
    loss_fn: nemo_rl.algorithms.interfaces.LossFunction,
    cu_seqlens_q: nemo_rl.algorithms.loss_functions.Tensor,
    cu_seqlens_q_padded: typing.Optional[nemo_rl.algorithms.loss_functions.Tensor] = None
)
```

</CodeBlock>
</Anchor>

<Indent>

<Anchor id="nemo_rl-algorithms-loss_functions-SequencePackingLossWrapper-__call__">

<CodeBlock links={{"nemo_rl.algorithms.loss_functions.Tensor":"#nemo_rl-algorithms-loss_functions-Tensor","nemo_rl.distributed.batched_data_dict.BatchedDataDict":"/nemo-rl/nemo_rl/distributed/batched_data_dict#nemo_rl-distributed-batched_data_dict-BatchedDataDict"}} showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.algorithms.loss_functions.SequencePackingLossWrapper.__call__(
    next_token_logits: nemo_rl.algorithms.loss_functions.Tensor,
    data: nemo_rl.distributed.batched_data_dict.BatchedDataDict[typing.Any],
    global_valid_seqs: nemo_rl.algorithms.loss_functions.Tensor | None,
    global_valid_toks: nemo_rl.algorithms.loss_functions.Tensor | None,
    vocab_parallel_rank: typing.Optional[int] = None,
    vocab_parallel_group: typing.Optional[torch.distributed.ProcessGroup] = None,
    context_parallel_group: typing.Optional[torch.distributed.ProcessGroup] = None
) -> tuple[nemo_rl.algorithms.loss_functions.Tensor, dict[str, typing.Any]]
```

</CodeBlock>
</Anchor>

<Indent>

Wraps a loss function to handle sequence packing by doing one sequence at a time to avoid excessive padding.


</Indent>
</Indent>

<Anchor id="nemo_rl-algorithms-loss_functions-Tensor">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.algorithms.loss_functions.Tensor = TypeVar('Tensor', bound=(torch.Tensor))
```

</CodeBlock>
</Anchor>

