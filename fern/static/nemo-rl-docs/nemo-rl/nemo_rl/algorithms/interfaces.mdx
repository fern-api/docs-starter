---
layout: overview
slug: nemo-rl/nemo_rl/algorithms/interfaces
title: nemo_rl.algorithms.interfaces
---

## Module Contents

### Classes

| Name | Description |
|------|-------------|
| [`LossFunction`](#nemo_rl-algorithms-interfaces-LossFunction) | Signature for loss functions used in reinforcement learning algorithms. |
| [`LossType`](#nemo_rl-algorithms-interfaces-LossType) | - |

### API

<Anchor id="nemo_rl-algorithms-interfaces-LossFunction">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
class nemo_rl.algorithms.interfaces.LossFunction()
```

</CodeBlock>
</Anchor>

<Indent>

<Badge>Protocol</Badge>

Signature for loss functions used in reinforcement learning algorithms.

Loss functions compute a scalar loss value and associated metrics from
model logprobs and other data contained in a BatchedDataDict.


<ParamField path="loss_type" type="LossType">
</ParamField>
<Anchor id="nemo_rl-algorithms-interfaces-LossFunction-__call__">

<CodeBlock links={{"nemo_rl.distributed.batched_data_dict.BatchedDataDict":"/nemo-rl/nemo_rl/distributed/batched_data_dict#nemo_rl-distributed-batched_data_dict-BatchedDataDict"}} showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.algorithms.interfaces.LossFunction.__call__(
    next_token_logits: torch.Tensor,
    data: nemo_rl.distributed.batched_data_dict.BatchedDataDict,
    global_valid_seqs: torch.Tensor,
    global_valid_toks: torch.Tensor
) -> tuple[torch.Tensor, dict[str, typing.Any]]
```

</CodeBlock>
</Anchor>

<Indent>

Compute loss and metrics from logprobs and other data.

**Parameters:**

<ParamField path="next_token_logits" type="torch.Tensor">
Logits from the model, typically with shape [batch_size, seq_len, vocab_size].
               For each position (b, i), contains the logit distribution over the entire vocabulary
               for predicting the next token (at position i+1). For example, if processing "The cat sat on",
               then next_token_logits[b, 3] would contain the logits for predicting the word
               that follows "on".
</ParamField>

<ParamField path="data" type="BatchedDataDict">
Dictionary containing all relevant data for loss computation
  such as rewards, values, actions, advantages, masks, and other
  algorithm-specific information needed for the particular loss calculation.
</ParamField>

<ParamField path="global_valid_seqs" type="torch.Tensor">
torch.Tensor
this tensor should contain the number of valid sequences in the microbatch.
It's used for global normalization for losses/metrics that are computed at the sequence level
and needs to be aggregated across all microbatches.
</ParamField>

<ParamField path="global_valid_toks" type="torch.Tensor">
torch.Tensor
This tensor should contain the number of valid tokens in the microbatch.
It's used for global normalization for losses/metrics that are computed at the token level
and needs to be aggregated across all microbatches.
</ParamField>

**Returns:** `tuple[torch.Tensor, dict[str, Any]]`

(loss, metrics)
- loss: A scalar tensor representing the loss value to be minimized during training
- metrics: A dictionary of metrics related to the loss computation, which may include
  component losses, statistics about gradients/rewards, and other diagnostic information


</Indent>
</Indent>

<Anchor id="nemo_rl-algorithms-interfaces-LossType">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
class nemo_rl.algorithms.interfaces.LossType
```

</CodeBlock>
</Anchor>

<Indent>

**Bases:** `enum.Enum`

<ParamField path="SEQUENCE_LEVEL" type="= 'sequence_level'">
</ParamField>

<ParamField path="TOKEN_LEVEL" type="= 'token_level'">
</ParamField>

</Indent>
