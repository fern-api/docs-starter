---
layout: overview
slug: nemo-rl/nemo_rl/models/policy/workers/megatron_policy_worker
title: nemo_rl.models.policy.workers.megatron_policy_worker
---

## Module Contents

### Classes

| Name | Description |
|------|-------------|
| [`MegatronPolicyWorker`](#nemo_rl-models-policy-workers-megatron_policy_worker-MegatronPolicyWorker) | - |

### Functions

| Name | Description |
|------|-------------|
| [`broadcast_object_across_pp_ranks`](#nemo_rl-models-policy-workers-megatron_policy_worker-broadcast_object_across_pp_ranks) | Broadcast an object across pipeline parallel ranks. |

### Data

[`TokenizerType`](#nemo_rl-models-policy-workers-megatron_policy_worker-TokenizerType)

### API

<Anchor id="nemo_rl-models-policy-workers-megatron_policy_worker-MegatronPolicyWorker">

<CodeBlock links={{"nemo_rl.models.policy.PolicyConfig":"/nemo-rl/nemo_rl/models/policy#nemo_rl-models-policy-PolicyConfig","nemo_rl.models.policy.workers.megatron_policy_worker.TokenizerType":"#nemo_rl-models-policy-workers-megatron_policy_worker-TokenizerType","nemo_rl.distributed.named_sharding.NamedSharding":"/nemo-rl/nemo_rl/distributed/named_sharding#nemo_rl-distributed-named_sharding-NamedSharding"}} showLineNumbers={false} wordWrap={true}>

```python
class nemo_rl.models.policy.workers.megatron_policy_worker.MegatronPolicyWorker(
    config: nemo_rl.models.policy.PolicyConfig,
    tokenizer: nemo_rl.models.policy.workers.megatron_policy_worker.TokenizerType,
    weights_path: typing.Optional[str] = None,
    optimizer_path: typing.Optional[str] = None,
    init_optimizer: bool = True,
    init_reference_model: bool = True,
    worker_sharding_annotations: nemo_rl.distributed.named_sharding.NamedSharding,
    kwargs: typing.Any = {}
)
```

</CodeBlock>
</Anchor>

<Indent>

**Bases:** [AbstractPolicyWorker](/nemo-rl/nemo_rl/models/policy/workers/base_policy_worker#nemo_rl-models-policy-workers-base_policy_worker-AbstractPolicyWorker), [ColocatablePolicyInterface](/nemo-rl/nemo_rl/models/policy/interfaces#nemo_rl-models-policy-interfaces-ColocatablePolicyInterface)

<ParamField path="checkpointing_context" type="= model_and_optimizer_state.checkpointing_context">
</ParamField>

<ParamField path="defer_fp32_logits">
</ParamField>

<ParamField path="dtype" type="= runtime_config.dtype">
</ParamField>

<ParamField path="final_padded_vocab_size" type="= runtime_config.final_padded_vocab_size">
</ParamField>

<ParamField path="fp8_cfg" type="= config['megatron_cfg'].get('fp8_cfg', None)">
</ParamField>

<ParamField path="is_generation_colocated" type="= runtime_config.is_generation_colocated">
</ParamField>

<ParamField path="mcore_state" type="= model_and_optimizer_state.state">
</ParamField>

<ParamField path="megatron_cfg" type="= runtime_config.megatron_cfg">
</ParamField>

<ParamField path="model" type="= model_and_optimizer_state.model">
</ParamField>

<ParamField path="offload_optimizer_for_logprob" type="= runtime_config.offload_optimizer_for_logprob">
</ParamField>

<ParamField path="optimizer" type="= model_and_optimizer_state.optimizer">
</ParamField>

<ParamField path="optimizer_cpu_offload" type="= runtime_config.optimizer_cpu_offload">
</ParamField>

<ParamField path="rank" type="= get_rank_safe()">
</ParamField>

<ParamField path="reference_state_dict">
</ParamField>

<ParamField path="scheduler" type="= model_and_optimizer_state.scheduler">
</ParamField>
<Anchor id="nemo_rl-models-policy-workers-megatron_policy_worker-MegatronPolicyWorker-__repr__">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.models.policy.workers.megatron_policy_worker.MegatronPolicyWorker.__repr__()
```

</CodeBlock>
</Anchor>

<Indent>

Customizes the actor's prefix in the Ray logs.

This makes it easier to identify which worker is producing specific log messages.


</Indent>
<Anchor id="nemo_rl-models-policy-workers-megatron_policy_worker-MegatronPolicyWorker-_calculate_refit_param_info">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.models.policy.workers.megatron_policy_worker.MegatronPolicyWorker._calculate_refit_param_info() -> list[tuple[str, int]]
```

</CodeBlock>
</Anchor>

<Indent>

Calculate parameter information for refit.

Each task contains:
- param_name: Local parameter name without module prefixes
- mapping: MegatronParamMapping instance for weight transformation
- pp_rank: Pipeline-parallel rank owning the parameter
- vp_stage: Virtual-pipeline stage index
- megatron_module: Reference to Megatron model/submodule
- param_weight: Target parameter tensor for converted weight

**Returns:** `list[tuple[str, int]]`

List of (parameter_name, size_in_bytes) tuples.


</Indent>
<Anchor id="nemo_rl-models-policy-workers-megatron_policy_worker-MegatronPolicyWorker-_iter_params_with_optional_kv_scales">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.models.policy.workers.megatron_policy_worker.MegatronPolicyWorker._iter_params_with_optional_kv_scales(
    kv_scales: typing.Optional[dict[str, float]] = None
) -> typing.Iterator[tuple[str, torch.Tensor]]
```

</CodeBlock>
</Anchor>

<Indent>

Yield exported HF parameters and optionally append FP8 KV/Q scale tensors.

This helper is used by both IPC-based streaming and collective broadcast
so that the logic for adding KV scales stays consistent in one place.


</Indent>
<Anchor id="nemo_rl-models-policy-workers-megatron_policy_worker-MegatronPolicyWorker-broadcast_weights_for_collective">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.models.policy.workers.megatron_policy_worker.MegatronPolicyWorker.broadcast_weights_for_collective(
    kv_scales: typing.Optional[dict[str, float]] = None
) -> None
```

</CodeBlock>
</Anchor>

<Indent>

Broadcast the weights for collective communication.


</Indent>
<Anchor id="nemo_rl-models-policy-workers-megatron_policy_worker-MegatronPolicyWorker-calibrate_qkv_fp8_scales">

<CodeBlock links={{"nemo_rl.distributed.batched_data_dict.BatchedDataDict":"/nemo-rl/nemo_rl/distributed/batched_data_dict#nemo_rl-distributed-batched_data_dict-BatchedDataDict"}} showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.models.policy.workers.megatron_policy_worker.MegatronPolicyWorker.calibrate_qkv_fp8_scales(
    data: nemo_rl.distributed.batched_data_dict.BatchedDataDict[typing.Any],
    micro_batch_size: typing.Optional[int] = None,
    percentile: float = 99.9,
    margin: float = 1.05,
    include_q: bool = False
) -> dict[str, typing.Any]
```

</CodeBlock>
</Anchor>

<Indent>

One-shot calibration of Q/K/V activation scales (for FP8 KV cache).

- Captures each layer's `query_key_value` output through forward hooks, splits Q/K/V, and computes percentile amax.
- In parallel (DP/TP/PP) environments, first computes local percentiles, then takes max across all ranks for conservativeness.
- By default only returns and saves K/V scales, optionally returns Q.

**Parameters:**

<ParamField path="data" type="BatchedDataDict[Any]">
Representative sample batch for calibration, following get_logprobs input conventions.
</ParamField>

<ParamField path="micro_batch_size" type="Optional[int]" default="None">
Micro batch size during calibration; if None, reuses logprob_batch_size.
</ParamField>

<ParamField path="percentile" type="float" default="99.9">
Percentile for amax (e.g. 99.9).
</ParamField>

<ParamField path="margin" type="float" default="1.05">
Margin factor, e.g. 1.05.
</ParamField>

<ParamField path="save_path">
If provided, rank0 will save results as JSON.
</ParamField>

<ParamField path="include_q" type="bool" default="False">
Whether to also return Q scale (usually only K/V needed).
</ParamField>

**Returns:** `dict[str, Any]`

&#123; "format": "fp8", "percentile": float, "margin": float,
"layers": &#123; layer_name: &#123;"k_scale": float, "v_scale": float[, "q_scale": float] &#125; &#125; &#125;


</Indent>
<Anchor id="nemo_rl-models-policy-workers-megatron_policy_worker-MegatronPolicyWorker-check_tensor_parallel_attributes">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.models.policy.workers.megatron_policy_worker.MegatronPolicyWorker.check_tensor_parallel_attributes() -> dict[str, typing.Any]
```

</CodeBlock>
</Anchor>

<Indent>

Check tensor parallel attributes on model parameters.

**Returns:** `dict[str, Any]`

Dictionary containing information about tensor parallel parameters:


</Indent>
<Anchor id="nemo_rl-models-policy-workers-megatron_policy_worker-MegatronPolicyWorker-disable_forward_pre_hook">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.models.policy.workers.megatron_policy_worker.MegatronPolicyWorker.disable_forward_pre_hook(
    param_sync = True
)
```

</CodeBlock>
</Anchor>

<Indent>


</Indent>
<Anchor id="nemo_rl-models-policy-workers-megatron_policy_worker-MegatronPolicyWorker-enable_forward_pre_hook">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.models.policy.workers.megatron_policy_worker.MegatronPolicyWorker.enable_forward_pre_hook()
```

</CodeBlock>
</Anchor>

<Indent>


</Indent>
<Anchor id="nemo_rl-models-policy-workers-megatron_policy_worker-MegatronPolicyWorker-generate">

<CodeBlock links={{"nemo_rl.distributed.batched_data_dict.BatchedDataDict":"/nemo-rl/nemo_rl/distributed/batched_data_dict#nemo_rl-distributed-batched_data_dict-BatchedDataDict","nemo_rl.models.generation.interfaces.GenerationDatumSpec":"/nemo-rl/nemo_rl/models/generation/interfaces#nemo_rl-models-generation-interfaces-GenerationDatumSpec","nemo_rl.models.generation.interfaces.GenerationOutputSpec":"/nemo-rl/nemo_rl/models/generation/interfaces#nemo_rl-models-generation-interfaces-GenerationOutputSpec"}} showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.models.policy.workers.megatron_policy_worker.MegatronPolicyWorker.generate(
    data: nemo_rl.distributed.batched_data_dict.BatchedDataDict[nemo_rl.models.generation.interfaces.GenerationDatumSpec],
    greedy: bool = False
) -> nemo_rl.distributed.batched_data_dict.BatchedDataDict[nemo_rl.models.generation.interfaces.GenerationOutputSpec]
```

</CodeBlock>
</Anchor>

<Indent>

Generate a batch of data using huggingface framework generation.

Returns:
    BatchedDataDict conforming to GenerationOutputSpec:
        - output_ids: input + generated token IDs
        - logprobs: Log probabilities for each token
        - generation_lengths: Lengths of each response

**Parameters:**

<ParamField path="data" type="BatchedDataDict[GenerationDatumSpec]">
BatchedDataDict containing input_ids and input_lengths tensors
</ParamField>


</Indent>
<Anchor id="nemo_rl-models-policy-workers-megatron_policy_worker-MegatronPolicyWorker-get_logprobs">

<CodeBlock links={{"nemo_rl.distributed.batched_data_dict.BatchedDataDict":"/nemo-rl/nemo_rl/distributed/batched_data_dict#nemo_rl-distributed-batched_data_dict-BatchedDataDict","nemo_rl.models.policy.interfaces.LogprobOutputSpec":"/nemo-rl/nemo_rl/models/policy/interfaces#nemo_rl-models-policy-interfaces-LogprobOutputSpec"}} showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.models.policy.workers.megatron_policy_worker.MegatronPolicyWorker.get_logprobs(
    data: nemo_rl.distributed.batched_data_dict.BatchedDataDict[typing.Any],
    micro_batch_size: typing.Optional[int] = None
) -> nemo_rl.distributed.batched_data_dict.BatchedDataDict[nemo_rl.models.policy.interfaces.LogprobOutputSpec]
```

</CodeBlock>
</Anchor>

<Indent>

Get the logprobs of the model for a batch of data.

Uses the configured logprob_batch_size to do microbatching.
Input data is assumed to be right-padded. The method internally converts to
left-padded format for computation, and returns outputs in right-padded format.
If micro_batch_size is provided, it will be used instead of the configured
logprob_batch_size.

**Returns:** `BatchedDataDict[LogprobOutputSpec]`

a BatchedDataDict with key "logprobs" and shape [batch_size, sequence_length].


</Indent>
<Anchor id="nemo_rl-models-policy-workers-megatron_policy_worker-MegatronPolicyWorker-get_topk_logits">

<CodeBlock links={{"nemo_rl.distributed.batched_data_dict.BatchedDataDict":"/nemo-rl/nemo_rl/distributed/batched_data_dict#nemo_rl-distributed-batched_data_dict-BatchedDataDict","nemo_rl.models.generation.interfaces.GenerationDatumSpec":"/nemo-rl/nemo_rl/models/generation/interfaces#nemo_rl-models-generation-interfaces-GenerationDatumSpec"}} showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.models.policy.workers.megatron_policy_worker.MegatronPolicyWorker.get_topk_logits(
    data: nemo_rl.distributed.batched_data_dict.BatchedDataDict[nemo_rl.models.generation.interfaces.GenerationDatumSpec],
    k: int,
    micro_batch_size: typing.Optional[int] = None
)
```

</CodeBlock>
</Anchor>

<Indent>

Get the top-k logits and indices for a batch of data.

The major difference from get_logprobs is that we compute top-k logits and indices for each position in the sequence.

**Returns:**

BatchedDataDict containing:
- topk_logits: Tensor of top-k logits for each position in the sequence
- topk_indices: Tensor of top-k indices for each position in the sequence


</Indent>
<Anchor id="nemo_rl-models-policy-workers-megatron_policy_worker-MegatronPolicyWorker-load_checkpoint">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.models.policy.workers.megatron_policy_worker.MegatronPolicyWorker.load_checkpoint(
    weights_path: str,
    optimizer_path: typing.Optional[str] = None
)
```

</CodeBlock>
</Anchor>

<Indent>

Load a training checkpoint.

**Parameters:**

<ParamField path="weights_path" type="str">
The exact directory path from which to load the checkpoint.
</ParamField>

<ParamField path="optimizer_path" type="Optional[str]" default="None">
If not None, attempts to load optimizer and scheduler states
            if self.optimizer and self.scheduler are initialized.
</ParamField>


</Indent>
<Anchor id="nemo_rl-models-policy-workers-megatron_policy_worker-MegatronPolicyWorker-move_model">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.models.policy.workers.megatron_policy_worker.MegatronPolicyWorker.move_model(
    model: torch.nn.Module,
    device: str,
    move_params: bool = True,
    move_grads: bool = True
) -> torch.nn.Module
```

</CodeBlock>
</Anchor>

<Indent>


</Indent>
<Anchor id="nemo_rl-models-policy-workers-megatron_policy_worker-MegatronPolicyWorker-move_optimizer">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.models.policy.workers.megatron_policy_worker.MegatronPolicyWorker.move_optimizer(
    device: str
)
```

</CodeBlock>
</Anchor>

<Indent>


</Indent>
<Anchor id="nemo_rl-models-policy-workers-megatron_policy_worker-MegatronPolicyWorker-offload_after_refit">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.models.policy.workers.megatron_policy_worker.MegatronPolicyWorker.offload_after_refit()
```

</CodeBlock>
</Anchor>

<Indent>

Offload as much as possible on the CPU.


</Indent>
<Anchor id="nemo_rl-models-policy-workers-megatron_policy_worker-MegatronPolicyWorker-offload_before_refit">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.models.policy.workers.megatron_policy_worker.MegatronPolicyWorker.offload_before_refit()
```

</CodeBlock>
</Anchor>

<Indent>

Offload the optimizer and buffers to the CPU.


</Indent>
<Anchor id="nemo_rl-models-policy-workers-megatron_policy_worker-MegatronPolicyWorker-prepare_for_lp_inference">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.models.policy.workers.megatron_policy_worker.MegatronPolicyWorker.prepare_for_lp_inference()
```

</CodeBlock>
</Anchor>

<Indent>


</Indent>
<Anchor id="nemo_rl-models-policy-workers-megatron_policy_worker-MegatronPolicyWorker-prepare_for_training">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.models.policy.workers.megatron_policy_worker.MegatronPolicyWorker.prepare_for_training(
    args = (),
    kwargs = {}
)
```

</CodeBlock>
</Anchor>

<Indent>


</Indent>
<Anchor id="nemo_rl-models-policy-workers-megatron_policy_worker-MegatronPolicyWorker-prepare_refit_info">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.models.policy.workers.megatron_policy_worker.MegatronPolicyWorker.prepare_refit_info() -> None
```

</CodeBlock>
</Anchor>

<Indent>

Prepare state dict metadata for weight refitting and IPC streaming.


</Indent>
<Anchor id="nemo_rl-models-policy-workers-megatron_policy_worker-MegatronPolicyWorker-save_checkpoint">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.models.policy.workers.megatron_policy_worker.MegatronPolicyWorker.save_checkpoint(
    weights_path: str,
    optimizer_path: typing.Optional[str] = None,
    kwargs = {}
)
```

</CodeBlock>
</Anchor>

<Indent>

Save a training checkpoint.

**Parameters:**

<ParamField path="weights_path" type="str">
The specific directory path where the checkpoint will be saved.
</ParamField>

<ParamField path="optimizer_path" type="Optional[str]" default="None">
If not None, optimizer and scheduler states are saved if they exist.
</ParamField>


</Indent>
<Anchor id="nemo_rl-models-policy-workers-megatron_policy_worker-MegatronPolicyWorker-stream_weights_via_ipc_zmq">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.models.policy.workers.megatron_policy_worker.MegatronPolicyWorker.stream_weights_via_ipc_zmq(
    buffer_size_bytes: int = 0,
    kv_scales: typing.Optional[dict[str, float]] = None
) -> None
```

</CodeBlock>
</Anchor>

<Indent>

Stream model weights to peer process via ZMQ IPC socket.


</Indent>
<Anchor id="nemo_rl-models-policy-workers-megatron_policy_worker-MegatronPolicyWorker-train">

<CodeBlock links={{"nemo_rl.distributed.batched_data_dict.BatchedDataDict":"/nemo-rl/nemo_rl/distributed/batched_data_dict#nemo_rl-distributed-batched_data_dict-BatchedDataDict","nemo_rl.algorithms.interfaces.LossFunction":"/nemo-rl/nemo_rl/algorithms/interfaces#nemo_rl-algorithms-interfaces-LossFunction"}} showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.models.policy.workers.megatron_policy_worker.MegatronPolicyWorker.train(
    data: nemo_rl.distributed.batched_data_dict.BatchedDataDict,
    loss_fn: nemo_rl.algorithms.interfaces.LossFunction,
    eval_mode: bool = False,
    gbs: typing.Optional[int] = None,
    mbs: typing.Optional[int] = None
) -> dict[str, typing.Any]
```

</CodeBlock>
</Anchor>

<Indent>

Train the policy on a batch of data with a given loss function.


</Indent>
<Anchor id="nemo_rl-models-policy-workers-megatron_policy_worker-MegatronPolicyWorker-use_reference_model">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.models.policy.workers.megatron_policy_worker.MegatronPolicyWorker.use_reference_model()
```

</CodeBlock>
</Anchor>

<Indent>

Context manager that temporarily swaps the reference model and active model.

On entry: Moves model to CPU, moves reference_model to CUDA. Swaps the references
On exit: Restores original references and re-flips cuda/cpu


</Indent>
</Indent>

<Anchor id="nemo_rl-models-policy-workers-megatron_policy_worker-broadcast_object_across_pp_ranks">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.models.policy.workers.megatron_policy_worker.broadcast_object_across_pp_ranks(
    obj
)
```

</CodeBlock>
</Anchor>

<Indent>

Broadcast an object across pipeline parallel ranks.

This utility function handles broadcasting an object from the rank that owns it
to all other pipeline parallel ranks. If only one rank has the object (non-None),
it will be broadcast to all other ranks.

**Parameters:**

<ParamField path="obj">
The object to broadcast. Can be None on ranks that don't own it.
</ParamField>

**Returns:**

The object on all ranks (either the original or the broadcast copy).

**Raises:**

- `ValueError`: If the object doesn't exist on any pipeline parallel rank.


</Indent>

<Anchor id="nemo_rl-models-policy-workers-megatron_policy_worker-TokenizerType">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.models.policy.workers.megatron_policy_worker.TokenizerType = TypeVar('TokenizerType', bound=PreTrainedTokenizerBase)
```

</CodeBlock>
</Anchor>

