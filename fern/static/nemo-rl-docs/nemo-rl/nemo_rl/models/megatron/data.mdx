---
layout: overview
slug: nemo-rl/nemo_rl/models/megatron/data
title: nemo_rl.models.megatron.data
---

## Module Contents

### Classes

| Name | Description |
|------|-------------|
| [`ProcessedInputs`](#nemo_rl-models-megatron-data-ProcessedInputs) | Processed microbatch inputs used for model forward pass. |
| [`ProcessedMicrobatch`](#nemo_rl-models-megatron-data-ProcessedMicrobatch) | Container for a processed microbatch ready for model forward pass. |

### Functions

| Name | Description |
|------|-------------|
| [`_get_pack_sequence_parameters_for_megatron`](#nemo_rl-models-megatron-data-_get_pack_sequence_parameters_for_megatron) | Get pack sequence parameters for Megatron model processing with optional context parallelism. |
| [`_pack_sequences_for_megatron`](#nemo_rl-models-megatron-data-_pack_sequences_for_megatron) | Pack sequences for Megatron model processing with optional context parallelism. |
| [`_unpack_sequences_from_megatron`](#nemo_rl-models-megatron-data-_unpack_sequences_from_megatron) | Unpack sequences from Megatron output format. |
| [`get_and_validate_seqlen`](#nemo_rl-models-megatron-data-get_and_validate_seqlen) | - |
| [`get_microbatch_iterator`](#nemo_rl-models-megatron-data-get_microbatch_iterator) | Create a processed microbatch iterator from a batch of data. |
| [`make_processed_microbatch_iterator`](#nemo_rl-models-megatron-data-make_processed_microbatch_iterator) | Wrap a raw microbatch iterator to yield processed microbatches. |
| [`process_global_batch`](#nemo_rl-models-megatron-data-process_global_batch) | Process a global batch and compute normalization factors. |
| [`process_microbatch`](#nemo_rl-models-megatron-data-process_microbatch) | Process a microbatch for Megatron model forward pass. |

### API

<Anchor id="nemo_rl-models-megatron-data-ProcessedInputs">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
class nemo_rl.models.megatron.data.ProcessedInputs(
    input_ids: torch.Tensor,
    input_ids_cp_sharded: torch.Tensor,
    attention_mask: typing.Optional[torch.Tensor],
    position_ids: typing.Optional[torch.Tensor],
    packed_seq_params: typing.Optional[megatron.core.packed_seq_params.PackedSeqParams],
    cu_seqlens_padded: typing.Optional[torch.Tensor]
)
```

</CodeBlock>
</Anchor>

<Indent>

<Badge>Dataclass</Badge>

Processed microbatch inputs used for model forward pass.


<ParamField path="attention_mask" type="Optional[Tensor]">
</ParamField>

<ParamField path="cu_seqlens_padded" type="Optional[Tensor]">
</ParamField>

<ParamField path="input_ids" type="Tensor">
</ParamField>

<ParamField path="input_ids_cp_sharded" type="Tensor">
</ParamField>

<ParamField path="packed_seq_params" type="Optional[PackedSeqParams]">
</ParamField>

<ParamField path="position_ids" type="Optional[Tensor]">
</ParamField>
</Indent>

<Anchor id="nemo_rl-models-megatron-data-ProcessedMicrobatch">

<CodeBlock links={{"nemo_rl.distributed.batched_data_dict.BatchedDataDict":"/nemo-rl/nemo_rl/distributed/batched_data_dict#nemo_rl-distributed-batched_data_dict-BatchedDataDict"}} showLineNumbers={false} wordWrap={true}>

```python
class nemo_rl.models.megatron.data.ProcessedMicrobatch(
    data_dict: nemo_rl.distributed.batched_data_dict.BatchedDataDict[typing.Any],
    input_ids: torch.Tensor,
    input_ids_cp_sharded: torch.Tensor,
    attention_mask: typing.Optional[torch.Tensor],
    position_ids: typing.Optional[torch.Tensor],
    packed_seq_params: typing.Optional[megatron.core.packed_seq_params.PackedSeqParams],
    cu_seqlens_padded: typing.Optional[torch.Tensor]
)
```

</CodeBlock>
</Anchor>

<Indent>

<Badge>Dataclass</Badge>

Container for a processed microbatch ready for model forward pass.

This dataclass holds both the original data dictionary and the processed
tensors needed for the Megatron model forward pass.


<ParamField path="attention_mask" type="Optional[Tensor]">
</ParamField>

<ParamField path="cu_seqlens_padded" type="Optional[Tensor]">
</ParamField>

<ParamField path="data_dict" type="BatchedDataDict[Any]">
</ParamField>

<ParamField path="input_ids" type="Tensor">
</ParamField>

<ParamField path="input_ids_cp_sharded" type="Tensor">
</ParamField>

<ParamField path="packed_seq_params" type="Optional[PackedSeqParams]">
</ParamField>

<ParamField path="position_ids" type="Optional[Tensor]">
</ParamField>
</Indent>

<Anchor id="nemo_rl-models-megatron-data-_get_pack_sequence_parameters_for_megatron">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.models.megatron.data._get_pack_sequence_parameters_for_megatron(
    megatron_cfg: dict,
    max_seq_len_in_batch: int
)
```

</CodeBlock>
</Anchor>

<Indent>

Get pack sequence parameters for Megatron model processing with optional context parallelism.

**Parameters:**

<ParamField path="megatron_cfg" type="dict">
Megatron configuration
</ParamField>

<ParamField path="max_seq_len_in_batch" type="int">
Maximum sequence length in batch
</ParamField>

**Returns:**

Tuple of:


</Indent>

<Anchor id="nemo_rl-models-megatron-data-_pack_sequences_for_megatron">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.models.megatron.data._pack_sequences_for_megatron(
    input_ids: torch.Tensor,
    seq_lengths: torch.Tensor,
    pad_individual_seqs_to_multiple_of: int = 1,
    pad_packed_seq_to_multiple_of: int = 1,
    pad_packed_seq_to: typing.Optional[int] = None,
    cp_rank: int = 0,
    cp_size: int = 1
) -> tuple[torch.Tensor, megatron.core.packed_seq_params.PackedSeqParams, torch.Tensor, typing.Optional[torch.Tensor]]
```

</CodeBlock>
</Anchor>

<Indent>

Pack sequences for Megatron model processing with optional context parallelism.

**Parameters:**

<ParamField path="input_ids" type="torch.Tensor">
Input token IDs [batch_size, seq_length]
</ParamField>

<ParamField path="seq_lengths" type="torch.Tensor">
Actual sequence lengths for each sample [batch_size]
</ParamField>

<ParamField path="pad_individual_seqs_to_multiple_of" type="int" default="1">
Pad individual sequences to a multiple of this value
</ParamField>

<ParamField path="pad_packed_seq_to_multiple_of" type="int" default="1">
Pad packed sequences to a multiple of this value
</ParamField>

<ParamField path="pad_packed_seq_to" type="Optional[int]" default="None">
Pad packed sequences to this value (before CP)
- The three parameters above can be calculated using _get_pack_sequence_parameters_for_megatron, we do not recommend users to set these parameters manually.
</ParamField>

<ParamField path="cp_size" type="int" default="1">
Context parallelism size
</ParamField>

**Returns:** `torch.Tensor`

Tuple of:


</Indent>

<Anchor id="nemo_rl-models-megatron-data-_unpack_sequences_from_megatron">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.models.megatron.data._unpack_sequences_from_megatron(
    output_tensor: torch.Tensor,
    seq_lengths: torch.Tensor,
    cu_seqlens: torch.Tensor,
    cu_seqlens_padded: typing.Optional[torch.Tensor],
    original_batch_size: int,
    original_seq_length: int
) -> torch.Tensor
```

</CodeBlock>
</Anchor>

<Indent>

Unpack sequences from Megatron output format.

**Parameters:**

<ParamField path="output_tensor" type="torch.Tensor">
Packed output tensor [1, T, vocab_size]
</ParamField>

<ParamField path="seq_lengths" type="torch.Tensor">
Actual sequence lengths for each sample
</ParamField>

<ParamField path="cu_seqlens" type="torch.Tensor">
Cumulative sequence lengths
</ParamField>

<ParamField path="cu_seqlens_padded" type="Optional[torch.Tensor]">
Padded cumulative sequence lengths (if CP was used)
</ParamField>

<ParamField path="original_batch_size" type="int">
Original batch size
</ParamField>

<ParamField path="original_seq_length" type="int">
Original maximum sequence length
</ParamField>

**Returns:** `torch.Tensor`

Unpacked output tensor [batch_size, seq_length, vocab_size]


</Indent>

<Anchor id="nemo_rl-models-megatron-data-get_and_validate_seqlen">

<CodeBlock links={{"nemo_rl.distributed.batched_data_dict.BatchedDataDict":"/nemo-rl/nemo_rl/distributed/batched_data_dict#nemo_rl-distributed-batched_data_dict-BatchedDataDict"}} showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.models.megatron.data.get_and_validate_seqlen(
    data: nemo_rl.distributed.batched_data_dict.BatchedDataDict[typing.Any]
)
```

</CodeBlock>
</Anchor>

<Indent>


</Indent>

<Anchor id="nemo_rl-models-megatron-data-get_microbatch_iterator">

<CodeBlock links={{"nemo_rl.distributed.batched_data_dict.BatchedDataDict":"/nemo-rl/nemo_rl/distributed/batched_data_dict#nemo_rl-distributed-batched_data_dict-BatchedDataDict","nemo_rl.models.megatron.data.ProcessedMicrobatch":"#nemo_rl-models-megatron-data-ProcessedMicrobatch"}} showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.models.megatron.data.get_microbatch_iterator(
    data: nemo_rl.distributed.batched_data_dict.BatchedDataDict[typing.Any],
    cfg: dict[str, typing.Any],
    mbs: int,
    straggler_timer: megatron.core.utils.StragglerDetector,
    seq_length_key: typing.Optional[str] = None
) -> typing.Tuple[typing.Iterator[nemo_rl.models.megatron.data.ProcessedMicrobatch], int, int, int, int]
```

</CodeBlock>
</Anchor>

<Indent>

Create a processed microbatch iterator from a batch of data.

This function creates an iterator that yields ProcessedMicrobatch objects,
which contain both the original data dictionary and the processed tensors
ready for model forward pass.

**Parameters:**

<ParamField path="data" type="BatchedDataDict[Any]">
The batch data to create microbatches from
</ParamField>

<ParamField path="cfg" type="dict[str, Any]">
Configuration dictionary
</ParamField>

<ParamField path="mbs" type="int">
Microbatch size
</ParamField>

<ParamField path="seq_length_key" type="Optional[str]" default="None">
Key for sequence lengths in data dict (auto-detected if None)
</ParamField>

**Returns:** `Iterator[ProcessedMicrobatch]`

Tuple containing the iterator and metadata


</Indent>

<Anchor id="nemo_rl-models-megatron-data-make_processed_microbatch_iterator">

<CodeBlock links={{"nemo_rl.distributed.batched_data_dict.BatchedDataDict":"/nemo-rl/nemo_rl/distributed/batched_data_dict#nemo_rl-distributed-batched_data_dict-BatchedDataDict","nemo_rl.models.megatron.data.ProcessedMicrobatch":"#nemo_rl-models-megatron-data-ProcessedMicrobatch"}} showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.models.megatron.data.make_processed_microbatch_iterator(
    raw_iterator: typing.Iterator[nemo_rl.distributed.batched_data_dict.BatchedDataDict[typing.Any]],
    cfg: dict[str, typing.Any],
    seq_length_key: typing.Optional[str],
    pad_individual_seqs_to_multiple_of: int,
    pad_packed_seq_to_multiple_of: int,
    straggler_timer: megatron.core.utils.StragglerDetector,
    pad_full_seq_to: typing.Optional[int]
) -> typing.Iterator[nemo_rl.models.megatron.data.ProcessedMicrobatch]
```

</CodeBlock>
</Anchor>

<Indent>

Wrap a raw microbatch iterator to yield processed microbatches.

This function takes a raw iterator that yields BatchedDataDict objects and
wraps it to yield ProcessedMicrobatch objects that contain both the original
data and the processed tensors ready for model forward pass.

**Parameters:**

<ParamField path="raw_iterator" type="Iterator[BatchedDataDict[Any]]">
Iterator yielding raw BatchedDataDict microbatches
</ParamField>

<ParamField path="cfg" type="dict[str, Any]">
Configuration dictionary containing sequence_packing settings
</ParamField>

<ParamField path="seq_length_key" type="Optional[str]">
Key for sequence length in data dict (required for packing)
</ParamField>

<ParamField path="pad_individual_seqs_to_multiple_of" type="int">
Padding multiple for individual sequences
</ParamField>

<ParamField path="pad_packed_seq_to_multiple_of" type="int">
Padding multiple for packed sequences
</ParamField>

<ParamField path="pad_full_seq_to" type="Optional[int]">
Target length for full sequence padding (optional)
</ParamField>


</Indent>

<Anchor id="nemo_rl-models-megatron-data-process_global_batch">

<CodeBlock links={{"nemo_rl.distributed.batched_data_dict.BatchedDataDict":"/nemo-rl/nemo_rl/distributed/batched_data_dict#nemo_rl-distributed-batched_data_dict-BatchedDataDict","nemo_rl.algorithms.interfaces.LossFunction":"/nemo-rl/nemo_rl/algorithms/interfaces#nemo_rl-algorithms-interfaces-LossFunction"}} showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.models.megatron.data.process_global_batch(
    data: nemo_rl.distributed.batched_data_dict.BatchedDataDict[typing.Any],
    loss_fn: nemo_rl.algorithms.interfaces.LossFunction,
    dp_group: torch.distributed.ProcessGroup,
    batch_idx: int,
    batch_size: int
) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]
```

</CodeBlock>
</Anchor>

<Indent>

Process a global batch and compute normalization factors.

**Parameters:**

<ParamField path="data" type="BatchedDataDict[Any]">
Full dataset
</ParamField>

<ParamField path="batch_idx" type="int">
Index of batch to extract
</ParamField>

<ParamField path="batch_size" type="int">
Size of batch to extract
</ParamField>

<ParamField path="loss_fn" type="LossFunction">
Loss function (used to check loss type)
</ParamField>

<ParamField path="dp_mesh">
Data parallel mesh
</ParamField>

**Returns:** `torch.Tensor`

Dictionary containing:


</Indent>

<Anchor id="nemo_rl-models-megatron-data-process_microbatch">

<CodeBlock links={{"nemo_rl.distributed.batched_data_dict.BatchedDataDict":"/nemo-rl/nemo_rl/distributed/batched_data_dict#nemo_rl-distributed-batched_data_dict-BatchedDataDict"}} showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.models.megatron.data.process_microbatch(
    data_dict: nemo_rl.distributed.batched_data_dict.BatchedDataDict[typing.Any],
    seq_length_key: typing.Optional[str] = None,
    pad_individual_seqs_to_multiple_of: int = 1,
    pad_packed_seq_to_multiple_of: int = 1,
    pad_full_seq_to: typing.Optional[int] = None,
    pack_sequences: bool = False,
    straggler_timer: megatron.core.utils.StragglerDetector = None
) -> tuple[torch.Tensor, torch.Tensor, typing.Optional[torch.Tensor], typing.Optional[torch.Tensor], typing.Optional[megatron.core.packed_seq_params.PackedSeqParams], typing.Optional[torch.Tensor]]
```

</CodeBlock>
</Anchor>

<Indent>

Process a microbatch for Megatron model forward pass.


</Indent>
