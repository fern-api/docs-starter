---
layout: overview
slug: nemo-rl/nemo_rl/models/generation/vllm/vllm_worker_async
title: nemo_rl.models.generation.vllm.vllm_worker_async
---

## Module Contents

### Classes

| Name | Description |
|------|-------------|
| [`VllmAsyncGenerationWorker`](#nemo_rl-models-generation-vllm-vllm_worker_async-VllmAsyncGenerationWorker) | - |

### Functions

| Name | Description |
|------|-------------|
| [`_replace_prefix_tokens`](#nemo_rl-models-generation-vllm-vllm_worker_async-_replace_prefix_tokens) | This is a subroutine used inside the vLLM Chat Completion server. |

### API

<Anchor id="nemo_rl-models-generation-vllm-vllm_worker_async-VllmAsyncGenerationWorker">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
class nemo_rl.models.generation.vllm.vllm_worker_async.VllmAsyncGenerationWorker()
```

</CodeBlock>
</Anchor>

<Indent>

**Bases:** [BaseVllmGenerationWorker](/nemo-rl/nemo_rl/models/generation/vllm/vllm_worker#nemo_rl-models-generation-vllm-vllm_worker-BaseVllmGenerationWorker)

<Anchor id="nemo_rl-models-generation-vllm-vllm_worker_async-VllmAsyncGenerationWorker-_create_engine">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.models.generation.vllm.vllm_worker_async.VllmAsyncGenerationWorker._create_engine(
    llm_kwargs: dict[str, typing.Any]
) -> None
```

</CodeBlock>
</Anchor>

<Indent>


</Indent>
<Anchor id="nemo_rl-models-generation-vllm-vllm_worker_async-VllmAsyncGenerationWorker-_setup_vllm_openai_api_server">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.models.generation.vllm.vllm_worker_async.VllmAsyncGenerationWorker._setup_vllm_openai_api_server(
    app: fastapi.FastAPI
) -> fastapi.FastAPI
```

</CodeBlock>
</Anchor>

<Indent>


</Indent>
<Anchor id="nemo_rl-models-generation-vllm-vllm_worker_async-VllmAsyncGenerationWorker-_setup_vllm_server">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.models.generation.vllm.vllm_worker_async.VllmAsyncGenerationWorker._setup_vllm_server() -> tuple[threading.Thread, str, uvicorn.Server]
```

</CodeBlock>
</Anchor>

<Indent>


</Indent>
<Anchor id="nemo_rl-models-generation-vllm-vllm_worker_async-VllmAsyncGenerationWorker-_start_vllm_metrics_logger">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.models.generation.vllm.vllm_worker_async.VllmAsyncGenerationWorker._start_vllm_metrics_logger() -> None
```

</CodeBlock>
</Anchor>

<Indent>

Start a background thread that periodically collects vLLM logger metrics.

Controlled by vllm_metrics_logger_interval (default: 0.5) in vllm_cfg.
Runs only on the model-owner actor.


</Indent>
<Anchor id="nemo_rl-models-generation-vllm-vllm_worker_async-VllmAsyncGenerationWorker-clear_vllm_logger_metrics">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.models.generation.vllm.vllm_worker_async.VllmAsyncGenerationWorker.clear_vllm_logger_metrics() -> None
```

</CodeBlock>
</Anchor>

<Indent>


</Indent>
<Anchor id="nemo_rl-models-generation-vllm-vllm_worker_async-VllmAsyncGenerationWorker-generate_async">

<CodeBlock links={{"nemo_rl.distributed.batched_data_dict.BatchedDataDict":"/nemo-rl/nemo_rl/distributed/batched_data_dict#nemo_rl-distributed-batched_data_dict-BatchedDataDict","nemo_rl.models.generation.interfaces.GenerationDatumSpec":"/nemo-rl/nemo_rl/models/generation/interfaces#nemo_rl-models-generation-interfaces-GenerationDatumSpec","nemo_rl.models.generation.interfaces.GenerationOutputSpec":"/nemo-rl/nemo_rl/models/generation/interfaces#nemo_rl-models-generation-interfaces-GenerationOutputSpec"}} showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.models.generation.vllm.vllm_worker_async.VllmAsyncGenerationWorker.generate_async(
    data: nemo_rl.distributed.batched_data_dict.BatchedDataDict[nemo_rl.models.generation.interfaces.GenerationDatumSpec],
    greedy: bool = False
) -> typing.AsyncGenerator[tuple[int, nemo_rl.distributed.batched_data_dict.BatchedDataDict[nemo_rl.models.generation.interfaces.GenerationOutputSpec]], None]
```

</CodeBlock>
</Anchor>

<Indent>

<Badge>async</Badge>

Generate a batch of data using vLLM's AsyncLLMEngine, yielding results as they are ready.

**Parameters:**

<ParamField path="data" type="BatchedDataDict[GenerationDatumSpec]">
BatchedDataDict with input_ids and input_lengths
</ParamField>

<ParamField path="greedy" type="bool" default="False">
Whether to use greedy decoding instead of sampling
</ParamField>


</Indent>
<Anchor id="nemo_rl-models-generation-vllm-vllm_worker_async-VllmAsyncGenerationWorker-generate_text_async">

<CodeBlock links={{"nemo_rl.distributed.batched_data_dict.BatchedDataDict":"/nemo-rl/nemo_rl/distributed/batched_data_dict#nemo_rl-distributed-batched_data_dict-BatchedDataDict","nemo_rl.models.generation.interfaces.GenerationDatumSpec":"/nemo-rl/nemo_rl/models/generation/interfaces#nemo_rl-models-generation-interfaces-GenerationDatumSpec","nemo_rl.models.generation.interfaces.GenerationOutputSpec":"/nemo-rl/nemo_rl/models/generation/interfaces#nemo_rl-models-generation-interfaces-GenerationOutputSpec"}} showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.models.generation.vllm.vllm_worker_async.VllmAsyncGenerationWorker.generate_text_async(
    data: nemo_rl.distributed.batched_data_dict.BatchedDataDict[nemo_rl.models.generation.interfaces.GenerationDatumSpec],
    greedy: bool = False
) -> typing.AsyncGenerator[tuple[int, nemo_rl.distributed.batched_data_dict.BatchedDataDict[nemo_rl.models.generation.interfaces.GenerationOutputSpec]], None]
```

</CodeBlock>
</Anchor>

<Indent>

<Badge>async</Badge>

Generate text responses asynchronously, yielding results as they are ready.

**Parameters:**

<ParamField path="data" type="BatchedDataDict[GenerationDatumSpec]">
BatchedDataDict containing prompts with text strings
</ParamField>

<ParamField path="greedy" type="bool" default="False">
Whether to use greedy decoding instead of sampling
</ParamField>


</Indent>
<Anchor id="nemo_rl-models-generation-vllm-vllm_worker_async-VllmAsyncGenerationWorker-get_vllm_logger_metrics">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.models.generation.vllm.vllm_worker_async.VllmAsyncGenerationWorker.get_vllm_logger_metrics() -> dict[str, typing.Any]
```

</CodeBlock>
</Anchor>

<Indent>


</Indent>
<Anchor id="nemo_rl-models-generation-vllm-vllm_worker_async-VllmAsyncGenerationWorker-init_collective_async">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.models.generation.vllm.vllm_worker_async.VllmAsyncGenerationWorker.init_collective_async(
    rank_prefix: int,
    ip: str,
    port: int,
    world_size: int,
    train_world_size: int
) -> None
```

</CodeBlock>
</Anchor>

<Indent>

<Badge>async</Badge>


</Indent>
<Anchor id="nemo_rl-models-generation-vllm-vllm_worker_async-VllmAsyncGenerationWorker-post_init_async">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.models.generation.vllm.vllm_worker_async.VllmAsyncGenerationWorker.post_init_async()
```

</CodeBlock>
</Anchor>

<Indent>

<Badge>async</Badge>


</Indent>
<Anchor id="nemo_rl-models-generation-vllm-vllm_worker_async-VllmAsyncGenerationWorker-prepare_refit_info_async">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.models.generation.vllm.vllm_worker_async.VllmAsyncGenerationWorker.prepare_refit_info_async(
    state_dict_info: dict[str, typing.Any]
) -> None
```

</CodeBlock>
</Anchor>

<Indent>

<Badge>async</Badge>

Async version of prepare_refit_info.


</Indent>
<Anchor id="nemo_rl-models-generation-vllm-vllm_worker_async-VllmAsyncGenerationWorker-report_device_id_async">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.models.generation.vllm.vllm_worker_async.VllmAsyncGenerationWorker.report_device_id_async() -> list[str]
```

</CodeBlock>
</Anchor>

<Indent>

<Badge>async</Badge>

Async version of report_device_id.


</Indent>
<Anchor id="nemo_rl-models-generation-vllm-vllm_worker_async-VllmAsyncGenerationWorker-report_dp_openai_server_base_url">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.models.generation.vllm.vllm_worker_async.VllmAsyncGenerationWorker.report_dp_openai_server_base_url() -> typing.Optional[str]
```

</CodeBlock>
</Anchor>

<Indent>

<Badge>async</Badge>


</Indent>
<Anchor id="nemo_rl-models-generation-vllm-vllm_worker_async-VllmAsyncGenerationWorker-reset_prefix_cache_async">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.models.generation.vllm.vllm_worker_async.VllmAsyncGenerationWorker.reset_prefix_cache_async()
```

</CodeBlock>
</Anchor>

<Indent>

<Badge>async</Badge>

Async version of reset_prefix_cache.


</Indent>
<Anchor id="nemo_rl-models-generation-vllm-vllm_worker_async-VllmAsyncGenerationWorker-shutdown">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.models.generation.vllm.vllm_worker_async.VllmAsyncGenerationWorker.shutdown() -> bool
```

</CodeBlock>
</Anchor>

<Indent>

<Badge>async</Badge>

Clean up vLLM resources.


</Indent>
<Anchor id="nemo_rl-models-generation-vllm-vllm_worker_async-VllmAsyncGenerationWorker-sleep_async">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.models.generation.vllm.vllm_worker_async.VllmAsyncGenerationWorker.sleep_async()
```

</CodeBlock>
</Anchor>

<Indent>

<Badge>async</Badge>

Async version of sleep.


</Indent>
<Anchor id="nemo_rl-models-generation-vllm-vllm_worker_async-VllmAsyncGenerationWorker-update_weights_from_collective_async">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.models.generation.vllm.vllm_worker_async.VllmAsyncGenerationWorker.update_weights_from_collective_async() -> bool
```

</CodeBlock>
</Anchor>

<Indent>

<Badge>async</Badge>

Async version of update_weights_from_collective.


</Indent>
<Anchor id="nemo_rl-models-generation-vllm-vllm_worker_async-VllmAsyncGenerationWorker-update_weights_via_ipc_zmq_async">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.models.generation.vllm.vllm_worker_async.VllmAsyncGenerationWorker.update_weights_via_ipc_zmq_async() -> bool
```

</CodeBlock>
</Anchor>

<Indent>

<Badge>async</Badge>

Async version of update_weights_via_ipc_zmq.


</Indent>
<Anchor id="nemo_rl-models-generation-vllm-vllm_worker_async-VllmAsyncGenerationWorker-wake_up_async">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.models.generation.vllm.vllm_worker_async.VllmAsyncGenerationWorker.wake_up_async(
    kwargs = {}
)
```

</CodeBlock>
</Anchor>

<Indent>

<Badge>async</Badge>

Async version of wake_up.


</Indent>
</Indent>

<Anchor id="nemo_rl-models-generation-vllm-vllm_worker_async-_replace_prefix_tokens">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.models.generation.vllm.vllm_worker_async._replace_prefix_tokens(
    tokenizer,
    model_prefix_token_ids: list[int],
    template_prefix_token_ids: list[int],
    template_token_ids: list[int]
) -> list[int]
```

</CodeBlock>
</Anchor>

<Indent>

This is a subroutine used inside the vLLM Chat Completion server.

This function is for fixing up the chat template-tokenized messages history
to match the model output tokenization up to the last assistant turn,
in order to preserve the monotonic tokens property for optimized multi-turn
training.

Some environments (namely NeMo-Gym) require an OpenAI compatible server
endpoint rather than an inference engine handle. This is fine for the most
part, but it may cause issues when the environment is used as a part of
training.

RL training frameworks train models on token IDs, but the OpenAI compatible
server communicates in what is basically de-tokenized text. When multiple
model calls are made to the OpenAI compatible server in a single trajectory,
model generations in previous model calls may be re-tokenized to something
that is different than what was generated. This is not too big of an issue
(that we know of) at inference time, but the log probs the model produces
are different enough for the differently re-tokenized generation result that
it causes the training to be off policy. Off policy isn't necessarily a bad
thing in isolation, but this source of off-policyness may cause unexpected
issues if not properly accounted for. It also mis-aligns the token ID
sequences across model calls, which feels very strange during training.

There are real cases where the model output string _does not match_ the chat
template tokenization of the parsed model output. A concrete example is
inconsistent whitespace tokens around tool call special tokens.

TODO When NeMo RL supports training image generation models, we want to
revisit and possibly update this function. This issue occurs when the model
generates tokens that are de-tokenized into text or images, and then
re-tokenized into tokens. So if there is a situation like that with images
and image tokenization is non-unique, then we will need to uppdate this
function.

Example (turn-by-turn, concise; eos_token_id = 2):
    Turn 1:
        - prefill_T1 (template prefill) = [11,12,13,40,41]
        - model output = [220,17,2]  # decodes to " 4" + EOS
        - model_prefix_token_ids = prefill_T1 + model output
          =&gt; [11,12,13,40,41,220,17,2]

    Turn 2 (template retokenizes prior assistant text differently):
        - template_prefix_token_ids = [11,12,13,40,41,1001,2]  # 1001 decodes to " 4"
        - template_token_ids = [11,12,13,40,41,1001,2,21,22,40,41]

    _replace_prefix_tokens keeps the exact prior model tokens up to EOS and
    resumes from the template after that EOS:
        output =&gt; [11,12,13,40,41,220,17,2,21,22,40,41]


</Indent>
