---
layout: overview
slug: nemo-rl/nemo_rl/models/generation/vllm/vllm_generation
title: nemo_rl.models.generation.vllm.vllm_generation
---

## Module Contents

### Classes

| Name | Description |
|------|-------------|
| [`VllmGeneration`](#nemo_rl-models-generation-vllm-vllm_generation-VllmGeneration) | - |

### Data

[`TOP_K_THRESHOLD`](#nemo_rl-models-generation-vllm-vllm_generation-TOP_K_THRESHOLD)

[`TOP_P_THRESHOLD`](#nemo_rl-models-generation-vllm-vllm_generation-TOP_P_THRESHOLD)

### API

<Anchor id="nemo_rl-models-generation-vllm-vllm_generation-VllmGeneration">

<CodeBlock links={{"nemo_rl.distributed.virtual_cluster.RayVirtualCluster":"/nemo-rl/nemo_rl/distributed/virtual_cluster#nemo_rl-distributed-virtual_cluster-RayVirtualCluster","nemo_rl.models.generation.vllm.config.VllmConfig":"/nemo-rl/nemo_rl/models/generation/vllm/config#nemo_rl-models-generation-vllm-config-VllmConfig"}} showLineNumbers={false} wordWrap={true}>

```python
class nemo_rl.models.generation.vllm.vllm_generation.VllmGeneration(
    cluster: nemo_rl.distributed.virtual_cluster.RayVirtualCluster,
    config: nemo_rl.models.generation.vllm.config.VllmConfig,
    name_prefix: str = 'vllm_policy',
    workers_per_node: typing.Optional[typing.Union[int, list[int]]] = None
)
```

</CodeBlock>
</Anchor>

<Indent>

**Bases:** [GenerationInterface](/nemo-rl/nemo_rl/models/generation/interfaces#nemo_rl-models-generation-interfaces-GenerationInterface)

<ParamField path="_step_metrics_snapshot" type="dict[str | tuple[str, int], float] | None = None">
</ParamField>

<ParamField path="current_generate_dp_shard_idx" type="= 0">
</ParamField>

<ParamField path="device_uuids" type="= self._report_device_id()">
</ParamField>

<ParamField path="dp_openai_server_base_urls" type="= self._report_dp_openai_server_base_urls()">
</ParamField>

<ParamField path="dp_size" type="= cluster.world_size() // self.model_parallel_size">
</ParamField>

<ParamField path="ep_size" type="= self.cfg['vllm_cfg']['expert_parallel_size']">
</ParamField>

<ParamField path="model_parallel_size" type="= self.tp_size * self.pp_size">
</ParamField>

<ParamField path="pp_size" type="= self.cfg['vllm_cfg']['pipeline_parallel_size']">
</ParamField>

<ParamField path="requires_kv_scale_sync" type="bool">
Check if KV cache scales should be synchronized during refit.

Returns True if kv_cache_dtype is fp8/fp8_e4m3.
</ParamField>

<ParamField path="sharding_annotations">
</ParamField>

<ParamField path="tp_size" type="= self.cfg['vllm_cfg']['tensor_parallel_size']">
</ParamField>

<ParamField path="vllm_dp_size" type="= self.ep_size // self.tp_size">
</ParamField>

<ParamField path="worker_group">
</ParamField>
<Anchor id="nemo_rl-models-generation-vllm-vllm_generation-VllmGeneration-__del__">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.models.generation.vllm.vllm_generation.VllmGeneration.__del__() -> None
```

</CodeBlock>
</Anchor>

<Indent>

Shuts down the worker groups when the object is deleted or is garbage collected.

This is an extra safety net in case the user forgets to call shutdown() and the pointer to
the object is lost due to leaving a function scope. It's always recommended that the
user calls shutdown().


</Indent>
<Anchor id="nemo_rl-models-generation-vllm-vllm_generation-VllmGeneration-_async_generate_base">

<CodeBlock links={{"nemo_rl.distributed.batched_data_dict.BatchedDataDict":"/nemo-rl/nemo_rl/distributed/batched_data_dict#nemo_rl-distributed-batched_data_dict-BatchedDataDict","nemo_rl.models.generation.interfaces.GenerationDatumSpec":"/nemo-rl/nemo_rl/models/generation/interfaces#nemo_rl-models-generation-interfaces-GenerationDatumSpec","nemo_rl.models.generation.interfaces.GenerationOutputSpec":"/nemo-rl/nemo_rl/models/generation/interfaces#nemo_rl-models-generation-interfaces-GenerationOutputSpec"}} showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.models.generation.vllm.vllm_generation.VllmGeneration._async_generate_base(
    data: nemo_rl.distributed.batched_data_dict.BatchedDataDict[nemo_rl.models.generation.interfaces.GenerationDatumSpec],
    method_name: str,
    data_validation_fn,
    greedy: bool = False
) -> typing.AsyncGenerator[tuple[int, nemo_rl.distributed.batched_data_dict.BatchedDataDict[nemo_rl.models.generation.interfaces.GenerationOutputSpec]], None]
```

</CodeBlock>
</Anchor>

<Indent>

<Badge>async</Badge>

Base async generation method that handles common worker management logic.

**Parameters:**

<ParamField path="data" type="BatchedDataDict[GenerationDatumSpec]">
Input data for generation
</ParamField>

<ParamField path="method_name" type="str">
Name of the worker method to call ('generate_async' or 'generate_text_async')
</ParamField>

<ParamField path="data_validation_fn">
Function to validate input data
</ParamField>

<ParamField path="greedy" type="bool" default="False">
Whether to use greedy decoding
</ParamField>


</Indent>
<Anchor id="nemo_rl-models-generation-vllm-vllm_generation-VllmGeneration-_get_raw_spec_counters">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.models.generation.vllm.vllm_generation.VllmGeneration._get_raw_spec_counters() -> dict[str | tuple[str, int], float]
```

</CodeBlock>
</Anchor>

<Indent>

Collect raw spec decode counters from workers.


</Indent>
<Anchor id="nemo_rl-models-generation-vllm-vllm_generation-VllmGeneration-_get_tied_worker_bundle_indices">

<CodeBlock links={{"nemo_rl.distributed.virtual_cluster.RayVirtualCluster":"/nemo-rl/nemo_rl/distributed/virtual_cluster#nemo_rl-distributed-virtual_cluster-RayVirtualCluster"}} showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.models.generation.vllm.vllm_generation.VllmGeneration._get_tied_worker_bundle_indices(
    cluster: nemo_rl.distributed.virtual_cluster.RayVirtualCluster
) -> list[tuple[int, list[int]]]
```

</CodeBlock>
</Anchor>

<Indent>

Calculate bundle indices for tensor and pipeline parallel workers.

Handles both unified placement groups (for cross-node model parallelism) and
per-node placement groups (for node-local model parallelism).


</Indent>
<Anchor id="nemo_rl-models-generation-vllm-vllm_generation-VllmGeneration-_post_init">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.models.generation.vllm.vllm_generation.VllmGeneration._post_init()
```

</CodeBlock>
</Anchor>

<Indent>


</Indent>
<Anchor id="nemo_rl-models-generation-vllm-vllm_generation-VllmGeneration-_report_device_id">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.models.generation.vllm.vllm_generation.VllmGeneration._report_device_id() -> list[list[str]]
```

</CodeBlock>
</Anchor>

<Indent>

Report the device ID of vllm workers.


</Indent>
<Anchor id="nemo_rl-models-generation-vllm-vllm_generation-VllmGeneration-_report_dp_openai_server_base_urls">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.models.generation.vllm.vllm_generation.VllmGeneration._report_dp_openai_server_base_urls() -> list[typing.Optional[str]]
```

</CodeBlock>
</Anchor>

<Indent>

Report the data parallel OpenAI server base URLs of vLLM workers, only populated if it is async vLLM engine and the HTTP server is active.


</Indent>
<Anchor id="nemo_rl-models-generation-vllm-vllm_generation-VllmGeneration-clear_logger_metrics">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.models.generation.vllm.vllm_generation.VllmGeneration.clear_logger_metrics() -> None
```

</CodeBlock>
</Anchor>

<Indent>

Clear logger metrics for performance reporting.


</Indent>
<Anchor id="nemo_rl-models-generation-vllm-vllm_generation-VllmGeneration-clear_vllm_logger_metrics">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.models.generation.vllm.vllm_generation.VllmGeneration.clear_vllm_logger_metrics() -> None
```

</CodeBlock>
</Anchor>

<Indent>


</Indent>
<Anchor id="nemo_rl-models-generation-vllm-vllm_generation-VllmGeneration-finish_generation">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.models.generation.vllm.vllm_generation.VllmGeneration.finish_generation(
    args: typing.Any = (),
    kwargs: typing.Any = {}
) -> bool
```

</CodeBlock>
</Anchor>

<Indent>

Sleep workers and reset prefix cache.


</Indent>
<Anchor id="nemo_rl-models-generation-vllm-vllm_generation-VllmGeneration-generate">

<CodeBlock links={{"nemo_rl.distributed.batched_data_dict.BatchedDataDict":"/nemo-rl/nemo_rl/distributed/batched_data_dict#nemo_rl-distributed-batched_data_dict-BatchedDataDict","nemo_rl.models.generation.interfaces.GenerationDatumSpec":"/nemo-rl/nemo_rl/models/generation/interfaces#nemo_rl-models-generation-interfaces-GenerationDatumSpec","nemo_rl.models.generation.interfaces.GenerationOutputSpec":"/nemo-rl/nemo_rl/models/generation/interfaces#nemo_rl-models-generation-interfaces-GenerationOutputSpec"}} showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.models.generation.vllm.vllm_generation.VllmGeneration.generate(
    data: nemo_rl.distributed.batched_data_dict.BatchedDataDict[nemo_rl.models.generation.interfaces.GenerationDatumSpec],
    greedy: bool = False
) -> nemo_rl.distributed.batched_data_dict.BatchedDataDict[nemo_rl.models.generation.interfaces.GenerationOutputSpec]
```

</CodeBlock>
</Anchor>

<Indent>

Generate a batch of data using vLLM.


</Indent>
<Anchor id="nemo_rl-models-generation-vllm-vllm_generation-VllmGeneration-generate_async">

<CodeBlock links={{"nemo_rl.distributed.batched_data_dict.BatchedDataDict":"/nemo-rl/nemo_rl/distributed/batched_data_dict#nemo_rl-distributed-batched_data_dict-BatchedDataDict","nemo_rl.models.generation.interfaces.GenerationDatumSpec":"/nemo-rl/nemo_rl/models/generation/interfaces#nemo_rl-models-generation-interfaces-GenerationDatumSpec","nemo_rl.models.generation.interfaces.GenerationOutputSpec":"/nemo-rl/nemo_rl/models/generation/interfaces#nemo_rl-models-generation-interfaces-GenerationOutputSpec"}} showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.models.generation.vllm.vllm_generation.VllmGeneration.generate_async(
    data: nemo_rl.distributed.batched_data_dict.BatchedDataDict[nemo_rl.models.generation.interfaces.GenerationDatumSpec],
    greedy: bool = False
) -> typing.AsyncGenerator[tuple[int, nemo_rl.distributed.batched_data_dict.BatchedDataDict[nemo_rl.models.generation.interfaces.GenerationOutputSpec]], None]
```

</CodeBlock>
</Anchor>

<Indent>

<Badge>async</Badge>

Generate responses asynchronously, yielding individual samples as they complete.

This method provides per-sample streaming across all workers, yielding each
sample result as soon as it's ready, regardless of which worker processed it.


</Indent>
<Anchor id="nemo_rl-models-generation-vllm-vllm_generation-VllmGeneration-generate_text">

<CodeBlock links={{"nemo_rl.distributed.batched_data_dict.BatchedDataDict":"/nemo-rl/nemo_rl/distributed/batched_data_dict#nemo_rl-distributed-batched_data_dict-BatchedDataDict","nemo_rl.models.generation.interfaces.GenerationDatumSpec":"/nemo-rl/nemo_rl/models/generation/interfaces#nemo_rl-models-generation-interfaces-GenerationDatumSpec","nemo_rl.models.generation.interfaces.GenerationOutputSpec":"/nemo-rl/nemo_rl/models/generation/interfaces#nemo_rl-models-generation-interfaces-GenerationOutputSpec"}} showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.models.generation.vllm.vllm_generation.VllmGeneration.generate_text(
    data: nemo_rl.distributed.batched_data_dict.BatchedDataDict[nemo_rl.models.generation.interfaces.GenerationDatumSpec],
    greedy: bool = False
) -> nemo_rl.distributed.batched_data_dict.BatchedDataDict[nemo_rl.models.generation.interfaces.GenerationOutputSpec]
```

</CodeBlock>
</Anchor>

<Indent>

Generate text responses using vLLM.


</Indent>
<Anchor id="nemo_rl-models-generation-vllm-vllm_generation-VllmGeneration-generate_text_async">

<CodeBlock links={{"nemo_rl.distributed.batched_data_dict.BatchedDataDict":"/nemo-rl/nemo_rl/distributed/batched_data_dict#nemo_rl-distributed-batched_data_dict-BatchedDataDict","nemo_rl.models.generation.interfaces.GenerationDatumSpec":"/nemo-rl/nemo_rl/models/generation/interfaces#nemo_rl-models-generation-interfaces-GenerationDatumSpec","nemo_rl.models.generation.interfaces.GenerationOutputSpec":"/nemo-rl/nemo_rl/models/generation/interfaces#nemo_rl-models-generation-interfaces-GenerationOutputSpec"}} showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.models.generation.vllm.vllm_generation.VllmGeneration.generate_text_async(
    data: nemo_rl.distributed.batched_data_dict.BatchedDataDict[nemo_rl.models.generation.interfaces.GenerationDatumSpec],
    greedy: bool = False
) -> typing.AsyncGenerator[tuple[int, nemo_rl.distributed.batched_data_dict.BatchedDataDict[nemo_rl.models.generation.interfaces.GenerationOutputSpec]], None]
```

</CodeBlock>
</Anchor>

<Indent>

<Badge>async</Badge>

Generate text responses asynchronously, yielding results as they are ready.

**Parameters:**

<ParamField path="data" type="BatchedDataDict[GenerationDatumSpec]">
BatchedDataDict containing prompts with text strings
</ParamField>

<ParamField path="greedy" type="bool" default="False">
Whether to use greedy decoding instead of sampling
</ParamField>


</Indent>
<Anchor id="nemo_rl-models-generation-vllm-vllm_generation-VllmGeneration-get_logger_metrics">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.models.generation.vllm.vllm_generation.VllmGeneration.get_logger_metrics() -> dict[str, typing.Any]
```

</CodeBlock>
</Anchor>

<Indent>

Get logger metrics for performance reporting.


</Indent>
<Anchor id="nemo_rl-models-generation-vllm-vllm_generation-VllmGeneration-get_step_metrics">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.models.generation.vllm.vllm_generation.VllmGeneration.get_step_metrics() -> dict[str, float]
```

</CodeBlock>
</Anchor>

<Indent>

Get speculative decoding metrics delta since snapshot_step_metrics().

**Returns:** `dict[str, float]`

Dictionary of delta metrics with 'vllm/' prefix.

**Raises:**

- `RuntimeWarning`: If called without snapshot_step_metrics() first.


</Indent>
<Anchor id="nemo_rl-models-generation-vllm-vllm_generation-VllmGeneration-get_vllm_logger_metrics">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.models.generation.vllm.vllm_generation.VllmGeneration.get_vllm_logger_metrics() -> dict[str, typing.Any]
```

</CodeBlock>
</Anchor>

<Indent>

Collect vLLM logger metrics from vLLM workers (model-owner actors only).


</Indent>
<Anchor id="nemo_rl-models-generation-vllm-vllm_generation-VllmGeneration-init_collective">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.models.generation.vllm.vllm_generation.VllmGeneration.init_collective(
    ip: str,
    port: int,
    world_size: int,
    train_world_size: int
) -> list[ray.ObjectRef]
```

</CodeBlock>
</Anchor>

<Indent>

Initialize the collective communication.


</Indent>
<Anchor id="nemo_rl-models-generation-vllm-vllm_generation-VllmGeneration-invalidate_kv_cache">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.models.generation.vllm.vllm_generation.VllmGeneration.invalidate_kv_cache() -> bool
```

</CodeBlock>
</Anchor>

<Indent>

Invalidate reusable caches in vLLM (e.g., prefix/KV cache) after weight updates.

For async_engine, calls reset_prefix_cache_async on workers. For sync, calls reset_prefix_cache.
Returns True if all workers report success.


</Indent>
<Anchor id="nemo_rl-models-generation-vllm-vllm_generation-VllmGeneration-prepare_for_generation">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.models.generation.vllm.vllm_generation.VllmGeneration.prepare_for_generation(
    args: typing.Any = (),
    kwargs: typing.Any = {}
) -> bool
```

</CodeBlock>
</Anchor>

<Indent>

Wake workers up for colocated inference.


</Indent>
<Anchor id="nemo_rl-models-generation-vllm-vllm_generation-VllmGeneration-prepare_refit_info">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.models.generation.vllm.vllm_generation.VllmGeneration.prepare_refit_info(
    state_dict_info: dict[str, typing.Any]
) -> None
```

</CodeBlock>
</Anchor>

<Indent>

Prepare the info for refit.


</Indent>
<Anchor id="nemo_rl-models-generation-vllm-vllm_generation-VllmGeneration-shutdown">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.models.generation.vllm.vllm_generation.VllmGeneration.shutdown() -> bool
```

</CodeBlock>
</Anchor>

<Indent>

Shut down all vLLM workers and clean up resources.


</Indent>
<Anchor id="nemo_rl-models-generation-vllm-vllm_generation-VllmGeneration-snapshot_step_metrics">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.models.generation.vllm.vllm_generation.VllmGeneration.snapshot_step_metrics() -> None
```

</CodeBlock>
</Anchor>

<Indent>

Snapshot current spec decode counters to begin tracking a training step.

Call this before generation to establish a baseline for metrics delta.

**Raises:**

- `RuntimeWarning`: If called twice without get_step_metrics() in between.


</Indent>
<Anchor id="nemo_rl-models-generation-vllm-vllm_generation-VllmGeneration-start_gpu_profiling">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.models.generation.vllm.vllm_generation.VllmGeneration.start_gpu_profiling() -> None
```

</CodeBlock>
</Anchor>

<Indent>

Start GPU profiling.


</Indent>
<Anchor id="nemo_rl-models-generation-vllm-vllm_generation-VllmGeneration-stop_gpu_profiling">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.models.generation.vllm.vllm_generation.VllmGeneration.stop_gpu_profiling() -> None
```

</CodeBlock>
</Anchor>

<Indent>

Stop GPU profiling.


</Indent>
<Anchor id="nemo_rl-models-generation-vllm-vllm_generation-VllmGeneration-update_weights_from_collective">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.models.generation.vllm.vllm_generation.VllmGeneration.update_weights_from_collective() -> list[ray.ObjectRef]
```

</CodeBlock>
</Anchor>

<Indent>

Update weights of the policy using collective communication.


</Indent>
<Anchor id="nemo_rl-models-generation-vllm-vllm_generation-VllmGeneration-update_weights_via_ipc_zmq">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.models.generation.vllm.vllm_generation.VllmGeneration.update_weights_via_ipc_zmq() -> list[ray.ObjectRef]
```

</CodeBlock>
</Anchor>

<Indent>

Update weights of the policy using IPC handles via ZMQ socket.


</Indent>
</Indent>

<Anchor id="nemo_rl-models-generation-vllm-vllm_generation-TOP_K_THRESHOLD">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.models.generation.vllm.vllm_generation.TOP_K_THRESHOLD = 8000
```

</CodeBlock>
</Anchor>


<Anchor id="nemo_rl-models-generation-vllm-vllm_generation-TOP_P_THRESHOLD">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.models.generation.vllm.vllm_generation.TOP_P_THRESHOLD = 0.99
```

</CodeBlock>
</Anchor>

