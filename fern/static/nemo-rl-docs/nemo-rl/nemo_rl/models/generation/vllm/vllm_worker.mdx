---
layout: overview
slug: nemo-rl/nemo_rl/models/generation/vllm/vllm_worker
title: nemo_rl.models.generation.vllm.vllm_worker
---

## Module Contents

### Classes

| Name | Description |
|------|-------------|
| [`BaseVllmGenerationWorker`](#nemo_rl-models-generation-vllm-vllm_worker-BaseVllmGenerationWorker) | - |
| [`VllmGenerationWorker`](#nemo_rl-models-generation-vllm-vllm_worker-VllmGenerationWorker) | - |

### API

<Anchor id="nemo_rl-models-generation-vllm-vllm_worker-BaseVllmGenerationWorker">

<CodeBlock links={{"nemo_rl.models.generation.vllm.config.VllmConfig":"/nemo-rl/nemo_rl/models/generation/vllm/config#nemo_rl-models-generation-vllm-config-VllmConfig"}} showLineNumbers={false} wordWrap={true}>

```python
class nemo_rl.models.generation.vllm.vllm_worker.BaseVllmGenerationWorker(
    config: nemo_rl.models.generation.vllm.config.VllmConfig,
    bundle_indices: typing.Optional[list[int]] = None,
    fraction_of_gpus: float = 1.0,
    seed: typing.Optional[int] = None
)
```

</CodeBlock>
</Anchor>

<Indent>

<ParamField path="SamplingParams" type="= vllm.SamplingParams">
</ParamField>

<ParamField path="enable_expert_parallel" type="= self.expert_parallel_size &gt; 1">
</ParamField>

<ParamField path="expert_parallel_size" type="= self.cfg['vllm_cfg']['expert_parallel_size']">
</ParamField>

<ParamField path="gpu_memory_utilization" type="= self.cfg['vllm_cfg']['gpu_memory_utilization']">
</ParamField>

<ParamField path="is_model_owner" type="= bundle_indices is not None">
</ParamField>

<ParamField path="model_name" type="= self.cfg['model_name']">
</ParamField>

<ParamField path="pipeline_parallel_size" type="= self.cfg['vllm_cfg']['pipeline_parallel_size']">
</ParamField>

<ParamField path="precision" type="= self.cfg['vllm_cfg']['precision']">
</ParamField>

<ParamField path="py_executable" type="= sys.executable">
</ParamField>

<ParamField path="rank" type="= 0">
</ParamField>

<ParamField path="tensor_parallel_size" type="= self.cfg['vllm_cfg']['tensor_parallel_size']">
</ParamField>

<ParamField path="world_size" type="= 1">
</ParamField>
<Anchor id="nemo_rl-models-generation-vllm-vllm_worker-BaseVllmGenerationWorker-__repr__">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.models.generation.vllm.vllm_worker.BaseVllmGenerationWorker.__repr__() -> str
```

</CodeBlock>
</Anchor>

<Indent>

Customizes the actor's prefix in the Ray logs.

This makes it easier to identify which worker is producing specific log messages.


</Indent>
<Anchor id="nemo_rl-models-generation-vllm-vllm_worker-BaseVllmGenerationWorker-_build_sampling_params">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.models.generation.vllm.vllm_worker.BaseVllmGenerationWorker._build_sampling_params(
    greedy: bool,
    stop_strings,
    max_new_tokens: typing.Optional[int] = None
)
```

</CodeBlock>
</Anchor>

<Indent>


</Indent>
<Anchor id="nemo_rl-models-generation-vllm-vllm_worker-BaseVllmGenerationWorker-_get_raw_spec_counters">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.models.generation.vllm.vllm_worker.BaseVllmGenerationWorker._get_raw_spec_counters() -> dict[str, float | list[float]]
```

</CodeBlock>
</Anchor>

<Indent>

Get speculative decoding metrics from the vLLM engine.

Collects spec decode counters including number of drafts,
draft tokens, and accepted tokens for monitoring acceptance rates.

**Returns:** `dict[str, float | list[float]]`

Dictionary mapping metric names to their values.

**Raises:**

- `AssertionError`: If called before vLLM engine is initialized.


</Indent>
<Anchor id="nemo_rl-models-generation-vllm-vllm_worker-BaseVllmGenerationWorker-_merge_stop_strings">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.models.generation.vllm.vllm_worker.BaseVllmGenerationWorker._merge_stop_strings(
    batch_stop_strings
)
```

</CodeBlock>
</Anchor>

<Indent>


</Indent>
<Anchor id="nemo_rl-models-generation-vllm-vllm_worker-BaseVllmGenerationWorker-configure_worker">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.models.generation.vllm.vllm_worker.BaseVllmGenerationWorker.configure_worker(
    num_gpus: int | float,
    bundle_indices: typing.Optional[tuple[int, list[int]]] = None
) -> tuple[dict[str, typing.Any], dict[str, str], dict[str, typing.Any]]
```

</CodeBlock>
</Anchor>

<Indent>

<Badge>staticmethod</Badge>

Provides complete worker configuration for vLLM tensor and pipeline parallelism.

This method configures the worker based on its role in tensor and pipeline parallelism,
which is determined directly from the bundle_indices parameter.

**Parameters:**

<ParamField path="num_gpus" type="int | float">
Original GPU allocation for this worker based on the placement group
</ParamField>

<ParamField path="bundle_indices" type="Optional[tuple[int, list[int]]]" default="None">
Tuple of (node_idx, local_bundle_indices) for parallelism (if applicable)
</ParamField>

**Returns:** `tuple[dict[str, Any], dict[str, str], dict[str, Any]]`

tuple with complete worker configuration:
- 'resources': Resource allocation (e.g., num_gpus)
- 'env_vars': Environment variables for this worker
- 'init_kwargs': Parameters to pass to __init__ of the worker


</Indent>
<Anchor id="nemo_rl-models-generation-vllm-vllm_worker-BaseVllmGenerationWorker-is_alive">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.models.generation.vllm.vllm_worker.BaseVllmGenerationWorker.is_alive()
```

</CodeBlock>
</Anchor>

<Indent>

Check if the worker is alive.


</Indent>
<Anchor id="nemo_rl-models-generation-vllm-vllm_worker-BaseVllmGenerationWorker-llm">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.models.generation.vllm.vllm_worker.BaseVllmGenerationWorker.llm()
```

</CodeBlock>
</Anchor>

<Indent>


</Indent>
<Anchor id="nemo_rl-models-generation-vllm-vllm_worker-BaseVllmGenerationWorker-start_gpu_profiling">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.models.generation.vllm.vllm_worker.BaseVllmGenerationWorker.start_gpu_profiling() -> None
```

</CodeBlock>
</Anchor>

<Indent>

Start GPU profiling.


</Indent>
<Anchor id="nemo_rl-models-generation-vllm-vllm_worker-BaseVllmGenerationWorker-stop_gpu_profiling">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.models.generation.vllm.vllm_worker.BaseVllmGenerationWorker.stop_gpu_profiling() -> None
```

</CodeBlock>
</Anchor>

<Indent>

Stop GPU profiling.


</Indent>
</Indent>

<Anchor id="nemo_rl-models-generation-vllm-vllm_worker-VllmGenerationWorker">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
class nemo_rl.models.generation.vllm.vllm_worker.VllmGenerationWorker()
```

</CodeBlock>
</Anchor>

<Indent>

**Bases:** [BaseVllmGenerationWorker](#nemo_rl-models-generation-vllm-vllm_worker-BaseVllmGenerationWorker)

<Anchor id="nemo_rl-models-generation-vllm-vllm_worker-VllmGenerationWorker-_create_engine">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.models.generation.vllm.vllm_worker.VllmGenerationWorker._create_engine(
    llm_kwargs: dict[str, typing.Any]
) -> None
```

</CodeBlock>
</Anchor>

<Indent>


</Indent>
<Anchor id="nemo_rl-models-generation-vllm-vllm_worker-VllmGenerationWorker-generate">

<CodeBlock links={{"nemo_rl.distributed.batched_data_dict.BatchedDataDict":"/nemo-rl/nemo_rl/distributed/batched_data_dict#nemo_rl-distributed-batched_data_dict-BatchedDataDict","nemo_rl.models.generation.interfaces.GenerationDatumSpec":"/nemo-rl/nemo_rl/models/generation/interfaces#nemo_rl-models-generation-interfaces-GenerationDatumSpec","nemo_rl.models.generation.interfaces.GenerationOutputSpec":"/nemo-rl/nemo_rl/models/generation/interfaces#nemo_rl-models-generation-interfaces-GenerationOutputSpec"}} showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.models.generation.vllm.vllm_worker.VllmGenerationWorker.generate(
    data: nemo_rl.distributed.batched_data_dict.BatchedDataDict[nemo_rl.models.generation.interfaces.GenerationDatumSpec],
    greedy: bool = False
) -> nemo_rl.distributed.batched_data_dict.BatchedDataDict[nemo_rl.models.generation.interfaces.GenerationOutputSpec]
```

</CodeBlock>
</Anchor>

<Indent>

Generate a batch of data using vLLM generation.

**Parameters:**

<ParamField path="data" type="BatchedDataDict[GenerationDatumSpec]">
BatchedDataDict containing input_ids and input_lengths tensors
</ParamField>

<ParamField path="greedy" type="bool" default="False">
Whether to use greedy decoding instead of sampling
</ParamField>

**Returns:** `BatchedDataDict[GenerationOutputSpec]`

BatchedDataDict conforming to GenerationOutputSpec:
- output_ids: input + generated token IDs with proper padding
- logprobs: Log probabilities for tokens
- generation_lengths: Lengths of each response
- unpadded_sequence_lengths: Lengths of each input + generated sequence


</Indent>
<Anchor id="nemo_rl-models-generation-vllm-vllm_worker-VllmGenerationWorker-generate_text">

<CodeBlock links={{"nemo_rl.distributed.batched_data_dict.BatchedDataDict":"/nemo-rl/nemo_rl/distributed/batched_data_dict#nemo_rl-distributed-batched_data_dict-BatchedDataDict","nemo_rl.models.generation.interfaces.GenerationDatumSpec":"/nemo-rl/nemo_rl/models/generation/interfaces#nemo_rl-models-generation-interfaces-GenerationDatumSpec","nemo_rl.models.generation.interfaces.GenerationOutputSpec":"/nemo-rl/nemo_rl/models/generation/interfaces#nemo_rl-models-generation-interfaces-GenerationOutputSpec"}} showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.models.generation.vllm.vllm_worker.VllmGenerationWorker.generate_text(
    data: nemo_rl.distributed.batched_data_dict.BatchedDataDict[nemo_rl.models.generation.interfaces.GenerationDatumSpec],
    greedy: bool = False
) -> nemo_rl.distributed.batched_data_dict.BatchedDataDict[nemo_rl.models.generation.interfaces.GenerationOutputSpec]
```

</CodeBlock>
</Anchor>

<Indent>

Generate text responses using vLLM generation.

**Parameters:**

<ParamField path="data" type="BatchedDataDict[GenerationDatumSpec]">
BatchedDataDict containing prompts with text strings
</ParamField>

<ParamField path="greedy" type="bool" default="False">
Whether to use greedy decoding instead of sampling
</ParamField>

**Returns:** `BatchedDataDict[GenerationOutputSpec]`

BatchedDataDict containing:
- texts: List of generated text responses


</Indent>
<Anchor id="nemo_rl-models-generation-vllm-vllm_worker-VllmGenerationWorker-init_collective">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.models.generation.vllm.vllm_worker.VllmGenerationWorker.init_collective(
    rank_prefix: int,
    ip: str,
    port: int,
    world_size: int,
    train_world_size: int
) -> None
```

</CodeBlock>
</Anchor>

<Indent>


</Indent>
<Anchor id="nemo_rl-models-generation-vllm-vllm_worker-VllmGenerationWorker-post_init">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.models.generation.vllm.vllm_worker.VllmGenerationWorker.post_init()
```

</CodeBlock>
</Anchor>

<Indent>


</Indent>
<Anchor id="nemo_rl-models-generation-vllm-vllm_worker-VllmGenerationWorker-prepare_refit_info">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.models.generation.vllm.vllm_worker.VllmGenerationWorker.prepare_refit_info(
    state_dict_info: dict[str, typing.Any]
) -> None
```

</CodeBlock>
</Anchor>

<Indent>

Prepare the info for refit.


</Indent>
<Anchor id="nemo_rl-models-generation-vllm-vllm_worker-VllmGenerationWorker-report_device_id">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.models.generation.vllm.vllm_worker.VllmGenerationWorker.report_device_id() -> list[str]
```

</CodeBlock>
</Anchor>

<Indent>

Report device ID from the vLLM worker.


</Indent>
<Anchor id="nemo_rl-models-generation-vllm-vllm_worker-VllmGenerationWorker-reset_prefix_cache">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.models.generation.vllm.vllm_worker.VllmGenerationWorker.reset_prefix_cache()
```

</CodeBlock>
</Anchor>

<Indent>

Reset the prefix cache of vLLM engine.


</Indent>
<Anchor id="nemo_rl-models-generation-vllm-vllm_worker-VllmGenerationWorker-shutdown">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.models.generation.vllm.vllm_worker.VllmGenerationWorker.shutdown() -> bool
```

</CodeBlock>
</Anchor>

<Indent>

Clean up vLLM resources.


</Indent>
<Anchor id="nemo_rl-models-generation-vllm-vllm_worker-VllmGenerationWorker-sleep">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.models.generation.vllm.vllm_worker.VllmGenerationWorker.sleep()
```

</CodeBlock>
</Anchor>

<Indent>

Put the vLLM engine to sleep.


</Indent>
<Anchor id="nemo_rl-models-generation-vllm-vllm_worker-VllmGenerationWorker-update_weights_from_collective">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.models.generation.vllm.vllm_worker.VllmGenerationWorker.update_weights_from_collective() -> bool
```

</CodeBlock>
</Anchor>

<Indent>

Update the model weights from collective communication.


</Indent>
<Anchor id="nemo_rl-models-generation-vllm-vllm_worker-VllmGenerationWorker-update_weights_via_ipc_zmq">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.models.generation.vllm.vllm_worker.VllmGenerationWorker.update_weights_via_ipc_zmq() -> bool
```

</CodeBlock>
</Anchor>

<Indent>

Update weights from IPC handles via ZMQ socket.


</Indent>
<Anchor id="nemo_rl-models-generation-vllm-vllm_worker-VllmGenerationWorker-wake_up">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.models.generation.vllm.vllm_worker.VllmGenerationWorker.wake_up(
    kwargs = {}
)
```

</CodeBlock>
</Anchor>

<Indent>

Wake up the vLLM engine.


</Indent>
</Indent>
