---
layout: overview
slug: nemo-rl/nemo_rl/models/automodel/data
title: nemo_rl.models.automodel.data
---

Data processing utilities for automodel training and inference.

## Module Contents

### Classes

| Name | Description |
|------|-------------|
| [`ProcessedInputs`](#nemo_rl-models-automodel-data-ProcessedInputs) | Processed microbatch inputs ready for model forward pass. |
| [`ProcessedMicrobatch`](#nemo_rl-models-automodel-data-ProcessedMicrobatch) | Container for a processed microbatch ready for model forward pass. |

### Functions

| Name | Description |
|------|-------------|
| [`check_sequence_dim`](#nemo_rl-models-automodel-data-check_sequence_dim) | Check and validate sequence dimension across all tensors. |
| [`get_microbatch_iterator`](#nemo_rl-models-automodel-data-get_microbatch_iterator) | Create processed microbatch iterator based on batching strategy. |
| [`make_processed_microbatch_iterator`](#nemo_rl-models-automodel-data-make_processed_microbatch_iterator) | Wrap a raw microbatch iterator to yield processed microbatches. |
| [`process_global_batch`](#nemo_rl-models-automodel-data-process_global_batch) | Process a global batch and compute normalization factors. |
| [`process_microbatch`](#nemo_rl-models-automodel-data-process_microbatch) | Process a microbatch and prepare inputs for model forward. |

### API

<Anchor id="nemo_rl-models-automodel-data-ProcessedInputs">

<CodeBlock showLineNumbers={false} wordWrap={true}>

```python
class nemo_rl.models.automodel.data.ProcessedInputs(
    input_ids: torch.Tensor,
    seq_len: int,
    attention_mask: typing.Optional[torch.Tensor] = None,
    position_ids: typing.Optional[torch.Tensor] = None,
    flash_attn_kwargs: dict[str, typing.Any] = dict(),
    vlm_kwargs: dict[str, typing.Any] = dict(),
    cp_buffers: list[torch.Tensor] = list(),
    seq_index: typing.Optional[torch.Tensor] = None
)
```

</CodeBlock>
</Anchor>

<Indent>

<Badge>Dataclass</Badge>

Processed microbatch inputs ready for model forward pass.

This structure contains all necessary tensors and metadata for a forward pass,
including context parallel buffers and flash attention configuration.


<ParamField path="attention_mask" type="Optional[Tensor] = None">
</ParamField>

<ParamField path="cp_buffers" type="list[Tensor] = field(default_factory=list)">
</ParamField>

<ParamField path="flash_attn_kwargs" type="dict[str, Any] = field(default_factory=dict)">
</ParamField>

<ParamField path="has_context_parallel" type="bool">
Check if context parallel is enabled.
</ParamField>

<ParamField path="has_flash_attention" type="bool">
Check if flash attention is configured.

Works for both empty dict &#123;&#125; and dataclass objects like FlashAttnKwargs.
</ParamField>

<ParamField path="input_ids" type="Tensor">
</ParamField>

<ParamField path="is_multimodal" type="bool">
Check if this is a multimodal input.
</ParamField>

<ParamField path="position_ids" type="Optional[Tensor] = None">
</ParamField>

<ParamField path="seq_index" type="Optional[Tensor] = None">
</ParamField>

<ParamField path="seq_len" type="int">
</ParamField>

<ParamField path="vlm_kwargs" type="dict[str, Any] = field(default_factory=dict)">
</ParamField>
</Indent>

<Anchor id="nemo_rl-models-automodel-data-ProcessedMicrobatch">

<CodeBlock links={{"nemo_rl.distributed.batched_data_dict.BatchedDataDict":"/nemo-rl/nemo_rl/distributed/batched_data_dict#nemo_rl-distributed-batched_data_dict-BatchedDataDict","nemo_rl.models.automodel.data.ProcessedInputs":"#nemo_rl-models-automodel-data-ProcessedInputs"}} showLineNumbers={false} wordWrap={true}>

```python
class nemo_rl.models.automodel.data.ProcessedMicrobatch(
    data_dict: nemo_rl.distributed.batched_data_dict.BatchedDataDict[typing.Any],
    processed_inputs: nemo_rl.models.automodel.data.ProcessedInputs,
    original_batch_size: int,
    original_seq_len: int
)
```

</CodeBlock>
</Anchor>

<Indent>

<Badge>Dataclass</Badge>

Container for a processed microbatch ready for model forward pass.

This dataclass holds both the original data dictionary and the processed
tensors needed for the automodel forward pass. It follows the same pattern
as nemo_rl/models/megatron/data.py ProcessedMicrobatch.


<ParamField path="data_dict" type="BatchedDataDict[Any]">
</ParamField>

<ParamField path="original_batch_size" type="int">
</ParamField>

<ParamField path="original_seq_len" type="int">
</ParamField>

<ParamField path="processed_inputs" type="ProcessedInputs">
</ParamField>
</Indent>

<Anchor id="nemo_rl-models-automodel-data-check_sequence_dim">

<CodeBlock links={{"nemo_rl.distributed.batched_data_dict.BatchedDataDict":"/nemo-rl/nemo_rl/distributed/batched_data_dict#nemo_rl-distributed-batched_data_dict-BatchedDataDict"}} showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.models.automodel.data.check_sequence_dim(
    data: nemo_rl.distributed.batched_data_dict.BatchedDataDict[typing.Any]
) -> typing.Tuple[int, int]
```

</CodeBlock>
</Anchor>

<Indent>

Check and validate sequence dimension across all tensors.

Verifies that dimension 1 is the sequence dimension for all tensors
in the data dictionary that have more than one dimension.

**Parameters:**

<ParamField path="data" type="BatchedDataDict[Any]">
BatchedDataDict to validate
</ParamField>

**Returns:** `Tuple[int, int]`

Tuple of (sequence_dim, seq_dim_size)

**Raises:**

- `AssertionError`: If any tensor has inconsistent sequence dimension


</Indent>

<Anchor id="nemo_rl-models-automodel-data-get_microbatch_iterator">

<CodeBlock links={{"nemo_rl.distributed.batched_data_dict.BatchedDataDict":"/nemo-rl/nemo_rl/distributed/batched_data_dict#nemo_rl-distributed-batched_data_dict-BatchedDataDict","nemo_rl.models.automodel.data.ProcessedMicrobatch":"#nemo_rl-models-automodel-data-ProcessedMicrobatch"}} showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.models.automodel.data.get_microbatch_iterator(
    data: nemo_rl.distributed.batched_data_dict.BatchedDataDict[typing.Any],
    cfg: dict[str, typing.Any],
    mbs: int,
    dp_mesh: typing.Any,
    tokenizer: transformers.AutoTokenizer,
    cp_size: int = 1
) -> tuple[typing.Iterator[nemo_rl.models.automodel.data.ProcessedMicrobatch], int]
```

</CodeBlock>
</Anchor>

<Indent>

Create processed microbatch iterator based on batching strategy.

**Parameters:**

<ParamField path="data" type="BatchedDataDict[Any]">
Full dataset to iterate over
</ParamField>

<ParamField path="cfg" type="dict[str, Any]">
Configuration dictionary (enable_seq_packing is inferred from cfg["sequence_packing"]["enabled"])
</ParamField>

<ParamField path="mbs" type="int">
Microbatch size
</ParamField>

<ParamField path="dp_mesh" type="Any">
Data parallel mesh
</ParamField>

<ParamField path="tokenizer" type="AutoTokenizer">
Tokenizer for processing
</ParamField>

<ParamField path="cp_size" type="int" default="1">
Context parallel size
</ParamField>

**Returns:** `tuple[Iterator[ProcessedMicrobatch], int]`

Tuple of (processed_microbatch_iterator, iterator_length)


</Indent>

<Anchor id="nemo_rl-models-automodel-data-make_processed_microbatch_iterator">

<CodeBlock links={{"nemo_rl.distributed.batched_data_dict.BatchedDataDict":"/nemo-rl/nemo_rl/distributed/batched_data_dict#nemo_rl-distributed-batched_data_dict-BatchedDataDict","nemo_rl.models.automodel.data.ProcessedMicrobatch":"#nemo_rl-models-automodel-data-ProcessedMicrobatch"}} showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.models.automodel.data.make_processed_microbatch_iterator(
    raw_iterator: typing.Iterator[nemo_rl.distributed.batched_data_dict.BatchedDataDict[typing.Any]],
    tokenizer: transformers.AutoTokenizer,
    cfg: dict[str, typing.Any],
    cp_size: int
) -> typing.Iterator[nemo_rl.models.automodel.data.ProcessedMicrobatch]
```

</CodeBlock>
</Anchor>

<Indent>

Wrap a raw microbatch iterator to yield processed microbatches.

This function takes a raw iterator that yields BatchedDataDict objects and
wraps it to yield ProcessedMicrobatch objects that contain both the original
data and the processed tensors ready for model forward pass.

**Parameters:**

<ParamField path="raw_iterator" type="Iterator[BatchedDataDict[Any]]">
Iterator yielding raw BatchedDataDict microbatches
</ParamField>

<ParamField path="tokenizer" type="AutoTokenizer">
Tokenizer for processing
</ParamField>

<ParamField path="cfg" type="dict[str, Any]">
Configuration dictionary (enable_seq_packing is inferred from cfg["sequence_packing"]["enabled"])
</ParamField>

<ParamField path="cp_size" type="int">
Context parallel size
</ParamField>


</Indent>

<Anchor id="nemo_rl-models-automodel-data-process_global_batch">

<CodeBlock links={{"nemo_rl.distributed.batched_data_dict.BatchedDataDict":"/nemo-rl/nemo_rl/distributed/batched_data_dict#nemo_rl-distributed-batched_data_dict-BatchedDataDict","nemo_rl.algorithms.interfaces.LossFunction":"/nemo-rl/nemo_rl/algorithms/interfaces#nemo_rl-algorithms-interfaces-LossFunction"}} showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.models.automodel.data.process_global_batch(
    data: nemo_rl.distributed.batched_data_dict.BatchedDataDict[typing.Any],
    loss_fn: nemo_rl.algorithms.interfaces.LossFunction,
    dp_group: torch.distributed.ProcessGroup,
    batch_idx: int,
    batch_size: int
) -> dict[str, typing.Any]
```

</CodeBlock>
</Anchor>

<Indent>

Process a global batch and compute normalization factors.

**Parameters:**

<ParamField path="data" type="BatchedDataDict[Any]">
Full dataset
</ParamField>

<ParamField path="loss_fn" type="LossFunction">
Loss function (used to check loss type)
</ParamField>

<ParamField path="dp_group" type="torch.distributed.ProcessGroup">
Data parallel process group (for consistency with Megatron naming)
</ParamField>

<ParamField path="batch_idx" type="int">
Index of batch to extract
</ParamField>

<ParamField path="batch_size" type="int">
Size of batch to extract
</ParamField>

**Returns:** `dict[str, Any]`

Dictionary containing:


</Indent>

<Anchor id="nemo_rl-models-automodel-data-process_microbatch">

<CodeBlock links={{"nemo_rl.distributed.batched_data_dict.BatchedDataDict":"/nemo-rl/nemo_rl/distributed/batched_data_dict#nemo_rl-distributed-batched_data_dict-BatchedDataDict","nemo_rl.models.automodel.data.ProcessedInputs":"#nemo_rl-models-automodel-data-ProcessedInputs"}} showLineNumbers={false} wordWrap={true}>

```python
nemo_rl.models.automodel.data.process_microbatch(
    mb: nemo_rl.distributed.batched_data_dict.BatchedDataDict[typing.Any],
    tokenizer: transformers.AutoTokenizer,
    enable_seq_packing: bool,
    cfg: dict[str, typing.Any],
    cp_size: int
) -> nemo_rl.models.automodel.data.ProcessedInputs
```

</CodeBlock>
</Anchor>

<Indent>

Process a microbatch and prepare inputs for model forward.

**Parameters:**

<ParamField path="mb" type="BatchedDataDict[Any]">
Microbatch data
</ParamField>

<ParamField path="tokenizer" type="AutoTokenizer">
Tokenizer for padding value
</ParamField>

<ParamField path="enable_seq_packing" type="bool">
Whether sequence packing is enabled
</ParamField>

<ParamField path="cfg" type="dict[str, Any]">
Configuration dictionary
</ParamField>

<ParamField path="cp_size" type="int">
Context parallel size
</ParamField>

**Returns:** `ProcessedInputs`

ProcessedInputs containing all tensors and metadata for forward pass


</Indent>
